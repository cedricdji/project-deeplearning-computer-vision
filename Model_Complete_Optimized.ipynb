{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL - IMAGE LOADING & NEURAL NETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Andrew\\anaconda3\\envs\\cancer_isic\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#Import libraries\n",
    "import gc\n",
    "import csv\n",
    "import os\n",
    "import io\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import keras_cv\n",
    "import random\n",
    "from collections.abc import Generator\n",
    "\n",
    "TF_ENABLE_ONEDNN_OPTS=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) HYPERPARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "train_frac_to_use = 0.05   #Reduce training data to this fraction\n",
    "val_frac_to_use = 0.05     #Reduce validation data to this fraction\n",
    "test_frac_to_use = 0.05     #Reduce validation data to this fraction\n",
    "save_val_in_memory = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) GENERAL FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#Function to show image\\ndef show_img(image):\\n    plt.imshow(image, interpolation=None)\\n    plt.grid(None)\\n    plt.show()\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "#Function to show image\n",
    "def show_img(image):\n",
    "    plt.imshow(image, interpolation=None)\n",
    "    plt.grid(None)\n",
    "    plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Image cropping\n",
    "def crop_image(images_list, nbPix = 100):\n",
    "    output_images = []\n",
    "    for image in images_list:\n",
    "        #Height adjustments\n",
    "        h = len(image)\n",
    "        adj = len(image) - nbPix\n",
    "        h1 = round(adj / 2) #Top\n",
    "        h2 = h - (adj - h1) #Bottom\n",
    "\n",
    "        #Width adjustments\n",
    "        w = len(image[0])\n",
    "        w_adj = w - nbPix\n",
    "        w1 = round(w_adj / 2) #Left\n",
    "        w2 = w - (w_adj - w1) #Right\n",
    "\n",
    "        img = image[h1:h2,w1:w2]\n",
    "        output_images.append(img)\n",
    "        \n",
    "    return np.array(output_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) IMPORT DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 - Declare file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parent directory for data files - FULL DATA\n",
    "dataPath = \"C:/Users/Andrew/Downloads/isic-2024-challenge/\"\n",
    "#Metadata file paths\n",
    "metaPath = dataPath + \"train-metadata.csv\"\n",
    "#Image file path\n",
    "hdf5_file = dataPath + \"train-image.hdf5\"\n",
    "\n",
    "#ALTERNATIVE 1 QUI MARCHE PARFOIS\n",
    "#base_path = \"C:/Users/admin/Documents/DSTI/DeepLearning/Project/Computer_Vision/isic-2024-challenge\"\n",
    "#hdf5_file = os.path.join(base_path, \"sampleclaire-image.hdf5\")\n",
    "#metadata = pd.read_csv(metaPath, sep=\",\")\n",
    "\n",
    "#ALTERNATIVE 2 QUI MARCHE PARFOIS\n",
    "#base_path = \"C:/Users/admin/Documents/DSTI/DeepLearning/Project/Computer_Vision/isic-2024-challenge2\"\n",
    "#metadata = pd.read_csv(os.path.join(base_path, \"sampleclaire50000-metadata.csv\"))\n",
    "#hdf5_file = os.path.join(base_path, \"sampleclaire50000-image.hdf5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 - Load metadata from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Andrew\\AppData\\Local\\Temp\\ipykernel_2232\\327758908.py:2: DtypeWarning: Columns (51,52) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  metadata = pd.read_csv(metaPath, sep=\",\")\n"
     ]
    }
   ],
   "source": [
    "#Import metadata from file\n",
    "metadata = pd.read_csv(metaPath, sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- X_meta NA counts --\n",
      "isic_id                           0\n",
      "age_approx                     2798\n",
      "target                            0\n",
      "clin_size_long_diam_mm            0\n",
      "tbp_lv_areaMM2                    0\n",
      "tbp_lv_area_perim_ratio           0\n",
      "tbp_lv_eccentricity               0\n",
      "tbp_lv_minorAxisMM                0\n",
      "tbp_lv_color_std_mean             0\n",
      "tbp_lv_deltaLBnorm                0\n",
      "tbp_lv_radial_color_std_max       0\n",
      "tbp_lv_location                   0\n",
      "dtype: int64\n",
      "Number of unknown for tbp_lv_location 5756\n"
     ]
    }
   ],
   "source": [
    "#METADATA: color and size features having no NAs\n",
    "metadata = metadata[[\"isic_id\",\n",
    "                     \"age_approx\",\n",
    "                     \"target\",\n",
    "                     \"clin_size_long_diam_mm\",\n",
    "                     \"tbp_lv_areaMM2\",\n",
    "                     \"tbp_lv_area_perim_ratio\",\n",
    "                     \"tbp_lv_eccentricity\",\n",
    "                     \"tbp_lv_minorAxisMM\",\n",
    "                     \"tbp_lv_color_std_mean\",\n",
    "                     \"tbp_lv_deltaLBnorm\",\n",
    "                     \"tbp_lv_radial_color_std_max\",\n",
    "                     \"tbp_lv_location\"]]\n",
    "\n",
    "\n",
    "\n",
    "#Verify that there are no NAs\n",
    "print(\"-- X_meta NA counts --\")\n",
    "print(metadata.isna().sum())\n",
    "\n",
    "#Check number of Unknoxn for tbp_lv_location\n",
    "loc_unknown=metadata[metadata[\"tbp_lv_location\"]==\"Unknown\"]\n",
    "print(\"Number of unknown for tbp_lv_location\", len(loc_unknown))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Activate for debugging of the predict function\n",
    "#metadata[\"target_cheat\"] = metadata[\"target\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 - Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unknown for tbp_lv_location 0\n"
     ]
    }
   ],
   "source": [
    "metadata=metadata[metadata[\"tbp_lv_location\"]!=\"Unknown\"]\n",
    "\n",
    "loc_unknown2=metadata[metadata[\"tbp_lv_location\"]==\"Unknown\"]\n",
    "print(\"Number of unknown for tbp_lv_location\", len(loc_unknown2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['isic_id', 'age_approx', 'target', 'clin_size_long_diam_mm',\n",
      "       'tbp_lv_areaMM2', 'tbp_lv_area_perim_ratio', 'tbp_lv_eccentricity',\n",
      "       'tbp_lv_minorAxisMM', 'tbp_lv_color_std_mean', 'tbp_lv_deltaLBnorm',\n",
      "       'tbp_lv_radial_color_std_max', 'category_Head & Neck',\n",
      "       'category_Left Arm', 'category_Left Arm - Lower',\n",
      "       'category_Left Arm - Upper', 'category_Left Leg',\n",
      "       'category_Left Leg - Lower', 'category_Left Leg - Upper',\n",
      "       'category_Right Arm', 'category_Right Arm - Lower',\n",
      "       'category_Right Arm - Upper', 'category_Right Leg',\n",
      "       'category_Right Leg - Lower', 'category_Right Leg - Upper',\n",
      "       'category_Torso Back', 'category_Torso Back Bottom Third',\n",
      "       'category_Torso Back Middle Third', 'category_Torso Back Top Third',\n",
      "       'category_Torso Front', 'category_Torso Front Bottom Half',\n",
      "       'category_Torso Front Top Half'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#Apply One-hot encoding for location\n",
    "location=pd.get_dummies(metadata[\"tbp_lv_location\"],prefix='category')\n",
    "location = location.astype(int)\n",
    "metadata = pd.concat([metadata, location], axis=1)\n",
    "metadata=metadata.drop(\"tbp_lv_location\",axis=1)\n",
    "print(metadata.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- X_meta NA counts --\n",
      "isic_id                             0\n",
      "age_approx                          0\n",
      "target                              0\n",
      "clin_size_long_diam_mm              0\n",
      "tbp_lv_areaMM2                      0\n",
      "tbp_lv_area_perim_ratio             0\n",
      "tbp_lv_eccentricity                 0\n",
      "tbp_lv_minorAxisMM                  0\n",
      "tbp_lv_color_std_mean               0\n",
      "tbp_lv_deltaLBnorm                  0\n",
      "tbp_lv_radial_color_std_max         0\n",
      "category_Head & Neck                0\n",
      "category_Left Arm                   0\n",
      "category_Left Arm - Lower           0\n",
      "category_Left Arm - Upper           0\n",
      "category_Left Leg                   0\n",
      "category_Left Leg - Lower           0\n",
      "category_Left Leg - Upper           0\n",
      "category_Right Arm                  0\n",
      "category_Right Arm - Lower          0\n",
      "category_Right Arm - Upper          0\n",
      "category_Right Leg                  0\n",
      "category_Right Leg - Lower          0\n",
      "category_Right Leg - Upper          0\n",
      "category_Torso Back                 0\n",
      "category_Torso Back Bottom Third    0\n",
      "category_Torso Back Middle Third    0\n",
      "category_Torso Back Top Third       0\n",
      "category_Torso Front                0\n",
      "category_Torso Front Bottom Half    0\n",
      "category_Torso Front Top Half       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Calculate the mean of age_approx for each target group\n",
    "mean_age_malign = metadata.loc[metadata[\"target\"] == 1, \"age_approx\"].mean()\n",
    "mean_age_benign = metadata.loc[metadata[\"target\"] == 0, \"age_approx\"].mean()\n",
    "\n",
    "# Define a function to fill NA based on the target value\n",
    "def fill_na_by_target(row):\n",
    "    if pd.isna(row['age_approx']):\n",
    "        if row['target'] == 1:\n",
    "            return mean_age_malign\n",
    "        elif row['target'] == 0:\n",
    "            return mean_age_benign\n",
    "    return row['age_approx']\n",
    "\n",
    "# Apply the function to the age_approx column\n",
    "metadata['age_approx'] = metadata.apply(fill_na_by_target, axis=1)\n",
    "\n",
    "#Verify that there are no NAs\n",
    "print(\"-- X_meta NA counts --\")\n",
    "print(metadata.isna().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n#from sklearn.preprocessing import StandardScaler\\nfrom sklearn.preprocessing import MinMaxScaler\\n#Normalization\\n#Select the column\\nfeature=metadata.drop(columns=['isic_id','target'])\\n\\n#scaler=StandardScaler() for standardization\\nscaler = MinMaxScaler()\\nfeature_standardized=scaler.fit_transform(feature)\\nfeature_standardized_df = pd.DataFrame(feature_standardized, columns=feature.columns)\\n\\nmetadata=pd.concat([metadata[['isic_id','target']].reset_index(drop=True), feature_standardized_df] , axis=1)\\nprint(len(metadata.columns))\\n\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "#from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "#Normalization\n",
    "#Select the column\n",
    "feature=metadata.drop(columns=['isic_id','target'])\n",
    "\n",
    "#scaler=StandardScaler() for standardization\n",
    "scaler = MinMaxScaler()\n",
    "feature_standardized=scaler.fit_transform(feature)\n",
    "feature_standardized_df = pd.DataFrame(feature_standardized, columns=feature.columns)\n",
    "\n",
    "metadata=pd.concat([metadata[['isic_id','target']].reset_index(drop=True), feature_standardized_df] , axis=1)\n",
    "print(len(metadata.columns))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n#from sklearn.preprocessing import StandardScaler\\nfrom sklearn.preprocessing import MinMaxScaler\\n#Normalization\\n#Select the column\\n\\nfeature=metadata.drop(columns=['isic_id','target', 'category_Head & Neck',\\n       'category_Left Arm', 'category_Left Arm - Lower',\\n       'category_Left Arm - Upper', 'category_Left Leg',\\n       'category_Left Leg - Lower', 'category_Left Leg - Upper',\\n       'category_Right Arm', 'category_Right Arm - Lower',\\n       'category_Right Arm - Upper', 'category_Right Leg',\\n       'category_Right Leg - Lower', 'category_Right Leg - Upper',\\n       'category_Torso Back Bottom Third', 'category_Torso Back Middle Third',\\n       'category_Torso Back Top Third','category_Torso Front',\\n       'category_Torso Front Bottom Half', 'category_Torso Front Top Half'])\\n'''\\n#Select the column\\nfeature=metadata.drop(columns=['isic_id','target', 'category_Head & Neck',\\n       'category_Left Arm', 'category_Left Arm - Lower',\\n       'category_Left Arm - Upper', 'category_Left Leg',\\n       'category_Left Leg - Lower', 'category_Left Leg - Upper',\\n       'category_Right Arm', 'category_Right Arm - Lower',\\n       'category_Right Arm - Upper', 'category_Right Leg',\\n       'category_Right Leg - Lower', 'category_Right Leg - Upper',\\n       'category_Torso Back Bottom Third', 'category_Torso Back Middle Third',\\n       'category_Torso Back Top Third',\\n       'category_Torso Front Bottom Half', 'category_Torso Front Top Half'])\\n'''\\n\\n#scaler=StandardScaler() for standardization\\nscaler = MinMaxScaler()\\nfeature_standardized=scaler.fit_transform(feature)\\nfeature_standardized_df = pd.DataFrame(feature_standardized, columns=feature.columns)\\n\\n\\nmetadata=pd.concat([metadata[['isic_id','target', 'category_Head & Neck',\\n       'category_Left Arm', 'category_Left Arm - Lower',\\n       'category_Left Arm - Upper', 'category_Left Leg',\\n       'category_Left Leg - Lower', 'category_Left Leg - Upper',\\n       'category_Right Arm', 'category_Right Arm - Lower',\\n       'category_Right Arm - Upper', 'category_Right Leg',\\n       'category_Right Leg - Lower', 'category_Right Leg - Upper',\\n       'category_Torso Back Bottom Third', 'category_Torso Back Middle Third',\\n       'category_Torso Back Top Third', 'category_Torso Front',\\n       'category_Torso Front Bottom Half', 'category_Torso Front Top Half']].reset_index(drop=True), feature_standardized_df] , axis=1)\\n'''\\n\\nmetadata=pd.concat([metadata[['isic_id','target', 'category_Head & Neck',\\n       'category_Left Arm', 'category_Left Arm - Lower',\\n       'category_Left Arm - Upper', 'category_Left Leg',\\n       'category_Left Leg - Lower', 'category_Left Leg - Upper',\\n       'category_Right Arm', 'category_Right Arm - Lower',\\n       'category_Right Arm - Upper', 'category_Right Leg',\\n       'category_Right Leg - Lower', 'category_Right Leg - Upper',\\n       'category_Torso Back Bottom Third', 'category_Torso Back Middle Third',\\n       'category_Torso Back Top Third',\\n       'category_Torso Front Bottom Half', 'category_Torso Front Top Half']].reset_index(drop=True), feature_standardized_df] , axis=1)\\n'''\\n#print(len(metadata.columns))\\n\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "#from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "#Normalization\n",
    "#Select the column\n",
    "\n",
    "feature=metadata.drop(columns=['isic_id','target', 'category_Head & Neck',\n",
    "       'category_Left Arm', 'category_Left Arm - Lower',\n",
    "       'category_Left Arm - Upper', 'category_Left Leg',\n",
    "       'category_Left Leg - Lower', 'category_Left Leg - Upper',\n",
    "       'category_Right Arm', 'category_Right Arm - Lower',\n",
    "       'category_Right Arm - Upper', 'category_Right Leg',\n",
    "       'category_Right Leg - Lower', 'category_Right Leg - Upper',\n",
    "       'category_Torso Back Bottom Third', 'category_Torso Back Middle Third',\n",
    "       'category_Torso Back Top Third','category_Torso Front',\n",
    "       'category_Torso Front Bottom Half', 'category_Torso Front Top Half'])\n",
    "'''\n",
    "#Select the column\n",
    "feature=metadata.drop(columns=['isic_id','target', 'category_Head & Neck',\n",
    "       'category_Left Arm', 'category_Left Arm - Lower',\n",
    "       'category_Left Arm - Upper', 'category_Left Leg',\n",
    "       'category_Left Leg - Lower', 'category_Left Leg - Upper',\n",
    "       'category_Right Arm', 'category_Right Arm - Lower',\n",
    "       'category_Right Arm - Upper', 'category_Right Leg',\n",
    "       'category_Right Leg - Lower', 'category_Right Leg - Upper',\n",
    "       'category_Torso Back Bottom Third', 'category_Torso Back Middle Third',\n",
    "       'category_Torso Back Top Third',\n",
    "       'category_Torso Front Bottom Half', 'category_Torso Front Top Half'])\n",
    "'''\n",
    "\n",
    "#scaler=StandardScaler() for standardization\n",
    "scaler = MinMaxScaler()\n",
    "feature_standardized=scaler.fit_transform(feature)\n",
    "feature_standardized_df = pd.DataFrame(feature_standardized, columns=feature.columns)\n",
    "\n",
    "\n",
    "metadata=pd.concat([metadata[['isic_id','target', 'category_Head & Neck',\n",
    "       'category_Left Arm', 'category_Left Arm - Lower',\n",
    "       'category_Left Arm - Upper', 'category_Left Leg',\n",
    "       'category_Left Leg - Lower', 'category_Left Leg - Upper',\n",
    "       'category_Right Arm', 'category_Right Arm - Lower',\n",
    "       'category_Right Arm - Upper', 'category_Right Leg',\n",
    "       'category_Right Leg - Lower', 'category_Right Leg - Upper',\n",
    "       'category_Torso Back Bottom Third', 'category_Torso Back Middle Third',\n",
    "       'category_Torso Back Top Third', 'category_Torso Front',\n",
    "       'category_Torso Front Bottom Half', 'category_Torso Front Top Half']].reset_index(drop=True), feature_standardized_df] , axis=1)\n",
    "'''\n",
    "\n",
    "metadata=pd.concat([metadata[['isic_id','target', 'category_Head & Neck',\n",
    "       'category_Left Arm', 'category_Left Arm - Lower',\n",
    "       'category_Left Arm - Upper', 'category_Left Leg',\n",
    "       'category_Left Leg - Lower', 'category_Left Leg - Upper',\n",
    "       'category_Right Arm', 'category_Right Arm - Lower',\n",
    "       'category_Right Arm - Upper', 'category_Right Leg',\n",
    "       'category_Right Leg - Lower', 'category_Right Leg - Upper',\n",
    "       'category_Torso Back Bottom Third', 'category_Torso Back Middle Third',\n",
    "       'category_Torso Back Top Third',\n",
    "       'category_Torso Front Bottom Half', 'category_Torso Front Top Half']].reset_index(drop=True), feature_standardized_df] , axis=1)\n",
    "'''\n",
    "#print(len(metadata.columns))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Train, Validate, Test Split + Preparation of Data Augmentation\n",
    "1. Make two separate lists of isic_ids for target=0 and target=1. Transform into tuples (isic_id, target, mod toggle). Base data has mod toggle = 0, meaning no adjustment will be made.\n",
    "2. Reserve 10% of target = 1 for validate\n",
    "3. Split into train-validate & test on both lists (0 and 1). Take the test lists (0 and 1), concatenate and shuffle them\n",
    "4. Create augmentation preparation function for target = 1: mod toggle = strictly positive integer (this adds more isic_ids to the list, with mod toggle non zero))\n",
    "5. Run augmentation preparation function on train-validate and the reserved validation data: mod toggle = strictly positive integer\n",
    "6. Split train-validate on both lists (0 and 1)\n",
    "7. Reduce the validation data on target = 0 by value specified in reduce_frac\n",
    "8. Concatenate and shuffle the train lists and validation lists\n",
    "9. Limit training and validation data to speed up training (take only fraction of prepared lists)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### FIXES TO MAKE\n",
    "- mode toggle for reserved data should be positive, not -1\n",
    "- change function name from augment to augment_prep (or something similar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "reserve_frac = 0.1        #Fraction of total original data of Target = 1 (reserved for use in validation data)\n",
    "test_frac = 0.2           #Fraction of total original data, excluding the reserved fraction, to use as the test data\n",
    "nb_of_augments = 100      #Number of augments to perform on Target = 1 images in train-validate sets\n",
    "val_frac = 0.33           #Fraction of augmented train-validate list to use as the validation data. The rest becomes the training data.\n",
    "nb_of_duplications = 15   #Number of duplications (simple augments) to perform on reserved validation fraction (Target = 1). Note: this is added to the validation data.\n",
    "reduce_frac = 0.8         #Fraction of Target = 0 samples to remove from the validation data (improves balance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 - Make two separate lists of isic_ids for target=0 and target=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FIXES TO MAKE: Make code shorter and more efficient if possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total ids with target = 0: 394910\n",
      "Total ids with target = 1: 393\n"
     ]
    }
   ],
   "source": [
    "#Make a list of isic_ids for each target value (0 and 1)\n",
    "isic_id_target_0 = metadata[metadata['target'] == 0]['isic_id'].tolist()\n",
    "isic_id_target_1 = metadata[metadata['target'] == 1]['isic_id'].tolist()\n",
    "\n",
    "#Retrieve dataframe with isic id and target\n",
    "temp_0 = metadata[metadata[\"isic_id\"].isin(isic_id_target_0)].loc[:,[\"isic_id\",\"target\"]]\n",
    "temp_1 = metadata[metadata[\"isic_id\"].isin(isic_id_target_1)].loc[:,[\"isic_id\",\"target\"]]\n",
    "\n",
    "#Convert into list of tuples... this makes it compatible with data augmentations\n",
    "#Form: (isic_id, target, mod toggle)\n",
    "isic_id_target_0 = list(zip(temp_0.iloc[:,0], temp_0.iloc[:,1], [0]*len(temp_0)))\n",
    "isic_id_target_1 = list(zip(temp_1.iloc[:,0], temp_1.iloc[:,1], [0]*len(temp_1)))\n",
    "\n",
    "#Delete temporary dataframes (the original metadata dataframe is untouched)\n",
    "del temp_0\n",
    "del temp_1\n",
    "\n",
    "#Count the number of occurrences for each target value\n",
    "print(\"Total ids with target = 0:\", len(isic_id_target_0))\n",
    "print(\"Total ids with target = 1:\", len(isic_id_target_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 - Reserve 10% of target = 1 for validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total ids with target = 0: 394910\n",
      "Total ids with target = 1: 353\n",
      "Total reserved target = 1: 40\n"
     ]
    }
   ],
   "source": [
    "#Keep 10% of isic_Id of target=1 without duplication\n",
    "isic_id_target_1, isic_id_target_1_reserved = train_test_split(isic_id_target_1, test_size = reserve_frac, random_state=88, shuffle=False)\n",
    "\n",
    "#Count the number of occurrences for each target value (AFTER RESERVATION)\n",
    "print(\"Total ids with target = 0:\", len(isic_id_target_0))\n",
    "print(\"Total ids with target = 1:\", len(isic_id_target_1))\n",
    "print(\"Total reserved target = 1:\", len(isic_id_target_1_reserved))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 - Split into train-validate & test on both lists (0 and 1). Take the test lists (0 and 1), concatenate and shuffle them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split out the test ids\n",
    "trainval_0, test_0 = train_test_split(isic_id_target_0, test_size = test_frac, random_state=88, shuffle=True)\n",
    "trainval_1, test_1 = train_test_split(isic_id_target_1, test_size = test_frac, random_state=88, shuffle=True)\n",
    "\n",
    "test_ids = test_0 + test_1\n",
    "np.random.shuffle(test_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 - Create augmentation preparation function for target = 1: mod toggle = strictly positive integer\n",
    "(this adds more isic_ids to the list, with mod toggle non zero))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### FIXES TO MAKE:\n",
    "- verify if extend or append is faster for a list\n",
    "- remove simple_dupl (performed with is_training toggle in dataset generation function where augments are performed)\n",
    "- make \"temp = tuple_list + temp\" faster. Append or extend is faster than adding. To check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate a list containing augmentation toggles. Apply only to training and validation sets.\n",
    "def augment(tuple_list, simple_dupl = False, nb_of_augments = 30, shuffle_seed=None):\n",
    "    temp = []\n",
    "    \n",
    "    \"\"\"\n",
    "    #If duplication is desired, then the mod toggle is -1\n",
    "    if simple_dupl:\n",
    "        for item in tuple_list:\n",
    "            a = item[0]\n",
    "            b = item[1]\n",
    "            temp.extend([(a, b, -1) for i in range(1, nb_of_augments + 1)])\n",
    "\n",
    "    \n",
    "    #If augmentation is desired, then the mod toggle is a strictly positive integer\n",
    "    else:\n",
    "    \"\"\"\n",
    "    for item in tuple_list:\n",
    "        a = item[0] #isic_id\n",
    "        b = item[1] #target\n",
    "        temp.extend([(a, b, i) for i in range(1, nb_of_augments + 1)])\n",
    "\n",
    "    #Shuffle the list\n",
    "    temp = tuple_list + temp\n",
    "    np.random.seed(shuffle_seed)\n",
    "    np.random.shuffle(temp)\n",
    "\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 - Run augmentation preparation function on train-validate and the reserved validation data: mod toggle = strictly positive integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Augment the training and validation list\n",
    "trainval_1 = augment(trainval_1, nb_of_augments=nb_of_augments, shuffle_seed=50)\n",
    "\n",
    "#Duplicate the reserved training data\n",
    "isic_id_target_1_reserved = augment(isic_id_target_1_reserved, nb_of_augments=nb_of_duplications, shuffle_seed=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 - Split train-validate on both lists (0 and 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split training and validation lists\n",
    "train_0, val_0 = train_test_split(trainval_0, test_size = val_frac, random_state=88, shuffle=True)\n",
    "train_1, val_1 = train_test_split(trainval_1, test_size = val_frac, random_state=88, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7 - Reduce the validation data on target = 0 by value specified in reduce_frac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reduce the validation data of type Target = 0\n",
    "nb_samples = int((1 - reduce_frac) * len(val_0))\n",
    "val_0 = random.sample(val_0, nb_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.8 - Concatenate and shuffle the train and validation lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### THINGS TO FIX:\n",
    "- Check if there is a more efficient way compared to adding lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concatenate\n",
    "train_ids = train_0 + train_1\n",
    "val_ids = val_0 + val_1 + isic_id_target_1_reserved\n",
    "\n",
    "#Shuffle\n",
    "np.random.seed(60)\n",
    "np.random.shuffle(train_ids)\n",
    "np.random.shuffle(val_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Validate/Test Counts: 230753 / 30891 / 79053\n",
      "Train/Validate/Test Fractions: 0.68 / 0.09 / 0.23\n",
      "Proportion of Target = 1 in training data: 0.08269448284529345\n",
      "Proportion of Target = 1 in validation data: 0.3250137580525072\n",
      "Proportion of Target = 1 in test data: 0.0008981316332081009\n"
     ]
    }
   ],
   "source": [
    "#Calculate the proportaion of Target=1 in each set (training, validation, test)\n",
    "def calc_frac_target1(ids):\n",
    "    return sum([item[1] for item in ids]) / len(ids)\n",
    "\n",
    "tot_samples = len(train_ids) + len(val_ids) + len(test_ids)\n",
    "\n",
    "print(\"Train/Validate/Test Counts:\", len(train_ids), \"/\", len(val_ids), \"/\", len(test_ids))\n",
    "print(\"Train/Validate/Test Fractions:\", round(len(train_ids)/tot_samples,2), \"/\", round(len(val_ids)/tot_samples,2), \"/\", round(len(test_ids)/tot_samples,2))\n",
    "print(\"Proportion of Target = 1 in training data:\", calc_frac_target1(train_ids))\n",
    "print(\"Proportion of Target = 1 in validation data:\", calc_frac_target1(val_ids))\n",
    "print(\"Proportion of Target = 1 in test data:\", calc_frac_target1(test_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.9 - Limit training and validation data to speed up training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train ids length before: 230753\n",
      "Train ids length after: 11537\n",
      "Validate ids length before: 30891\n",
      "Validate ids length after: 1544\n",
      "Test ids length before: 79053\n",
      "Test ids length after: 3952\n"
     ]
    }
   ],
   "source": [
    "#Choose a portion of TRAINING ids to load into memory\n",
    "def take_fewer_samples(ids, frac_to_use, seed):\n",
    "    if frac_to_use < 1:\n",
    "        random.seed(seed)\n",
    "        k = int(frac_to_use * len(ids))\n",
    "        ids_short = random.choices(ids, k=k)\n",
    "        return ids_short\n",
    "\n",
    "print(\"Train ids length before:\", len(train_ids))\n",
    "train_ids = take_fewer_samples(train_ids, train_frac_to_use, seed=12)\n",
    "print(\"Train ids length after:\", len(train_ids))\n",
    "\n",
    "print(\"Validate ids length before:\", len(val_ids))\n",
    "val_ids = take_fewer_samples(val_ids, val_frac_to_use, seed=12)\n",
    "print(\"Validate ids length after:\", len(val_ids))\n",
    "\n",
    "print(\"Test ids length before:\", len(test_ids))\n",
    "test_ids = take_fewer_samples(test_ids, test_frac_to_use, seed=12)\n",
    "print(\"Test ids length after:\", len(test_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Augmentation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### THINGS TO FIX:\n",
    "- Try compute_class_weights with inverse weights instead of ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hair_removal(image, crop_pixels=10):\n",
    "    height_pixels = len(image)  # Image rows\n",
    "    width_pixels = len(image[0])  # Image columns\n",
    "\n",
    "    # Image cropping\n",
    "    height = [crop_pixels, height_pixels - crop_pixels]\n",
    "    width = [crop_pixels, width_pixels - crop_pixels]\n",
    "    img = image[height[0]:height[1], width[0]:width[1]]\n",
    "\n",
    "    # Gray scale\n",
    "    grayScale = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    # Black hat filter\n",
    "    kernel = cv2.getStructuringElement(1, (9, 9))\n",
    "    blackhat = cv2.morphologyEx(grayScale, cv2.MORPH_BLACKHAT, kernel)\n",
    "    # Gaussian filter\n",
    "    bhg = cv2.GaussianBlur(blackhat, (3, 3), cv2.BORDER_DEFAULT)\n",
    "    # Binary thresholding (MASK)\n",
    "    ret, mask = cv2.threshold(bhg, 10, 255, cv2.THRESH_BINARY)\n",
    "    # Replace pixels of the mask\n",
    "    dst = cv2.inpaint(img, mask, 6, cv2.INPAINT_TELEA)\n",
    "\n",
    "    return dst\n",
    "\n",
    "#def resize_image(image, target_size=(100, 100)):\n",
    "#    resized_image = cv2.resize(image, target_size, interpolation=cv2.INTER_AREA)\n",
    "#    return resized_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the augmentation function\n",
    "def augment_image(image):\n",
    "    \"\"\"\n",
    "    Apply a series of augmentations to create diverse variations of the input image.\n",
    "    Includes random flips, rotations, brightness adjustments, and other transformations.\n",
    "    \"\"\"\n",
    "    # Apply various augmentations\n",
    "    '''\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    image = tf.image.random_flip_up_down(image)\n",
    "    image = tf.image.rot90(image, tf.random.uniform(shape=[], minval=0, maxval=4, dtype=tf.int32))\n",
    "    image = tf.image.random_brightness(image, max_delta=0.2)\n",
    "    image = tf.image.random_contrast(image, lower=0.8, upper=1.2)\n",
    "    image = tf.image.random_saturation(image, lower=0.8, upper=1.2)\n",
    "    '''\n",
    "\n",
    "    # RandomCutout initialization\n",
    "    cutout_layer = keras_cv.layers.RandomCutout(height_factor=(0.02, 0.06), width_factor=(0.02, 0.06))\n",
    "    \n",
    "    # List of augmentations\n",
    "    augmentations = [\n",
    "        tf.image.random_flip_left_right,  \n",
    "        tf.image.random_flip_up_down,   \n",
    "        lambda img: tf.image.rot90(img, tf.random.uniform(shape=[], minval=0, maxval=4, dtype=tf.int32)), \n",
    "        tf.image.random_brightness,      \n",
    "        tf.image.random_contrast,         \n",
    "        tf.image.random_saturation,       \n",
    "        lambda img: cutout_layer(img)    \n",
    "    ]\n",
    "    \n",
    "    # Shuffle and pick one augmentation\n",
    "    augmentation = augmentations[tf.random.uniform(shape=[], minval=0, maxval=len(augmentations), dtype=tf.int32)]\n",
    "    \n",
    "    # Apply augmentation with 99% probability\n",
    "    if tf.random.uniform([]) < 0.95:\n",
    "        # Apply augmentation \n",
    "        if augmentation in [tf.image.random_flip_left_right, tf.image.random_flip_up_down]:\n",
    "            image = augmentation(image) \n",
    "        elif augmentation == tf.image.random_brightness:\n",
    "            image = augmentation(image, max_delta=0.25)  \n",
    "        elif augmentation == tf.image.random_contrast:\n",
    "            image = augmentation(image, lower=0.7, upper=1.8)  \n",
    "        elif augmentation == tf.image.random_saturation:\n",
    "            image = augmentation(image, lower=0.7, upper=1.8)  \n",
    "        else:\n",
    "            image = augmentation(image)  \n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_class_weights(img_names):\n",
    "    # Initialize counters for target=0 and target=1\n",
    "    target_0_count = 0\n",
    "    target_1_count = 0\n",
    "\n",
    "    # Calculate total number of images\n",
    "    total = len(img_names)\n",
    "    # Calculate number of target = 1\n",
    "    target_1_count = sum([item[1] for item in img_names])\n",
    "    # Calculate number of target = 1\n",
    "    target_0_count = total - target_1_count\n",
    "\n",
    "    # Calculate class weights based on the counts, avoid division by zero\n",
    "    if target_0_count > 0:\n",
    "        weight_for_0 = total / (2 * target_0_count)\n",
    "    else:\n",
    "        weight_for_0 = 1\n",
    "\n",
    "    if target_1_count > 0:\n",
    "        weight_for_1 = total / (2 * target_1_count)\n",
    "    else:\n",
    "        weight_for_1 = 1\n",
    "\n",
    "    return weight_for_0, weight_for_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Dataset generation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### THINGS TO VERIFY:\n",
    "- Claire can use these approches in EDA (distribution of image sizes)? This is very efficient.\n",
    "- Is this correct? \"if self.i < self.stop\" OR is it \"if self.i <= self.stop\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a metadata dictionary for efficient lookup\n",
    "#Objective: train_ids -> list_of_all_metadata_as_tuple\n",
    "def make_meta_dict(metadata, isic_ids_tuple):\n",
    "    #Reindex. Holes in index numbering will not work.\n",
    "    metadata = metadata.reset_index(drop=True)\n",
    "\n",
    "    #Retrieve the column number for isic_id and target\n",
    "    col_num_id = metadata.columns.get_loc(\"isic_id\")\n",
    "    col_num_target = metadata.columns.get_loc(\"target\")\n",
    "\n",
    "    #Transform isic_ic into a dictionary of form (isic_id: index number)\n",
    "    #For each isic_id, we need to know what the row number is within the metadata dataframe\n",
    "    isic_id_index = metadata[\"isic_id\"].to_dict()\n",
    "    isic_id_index = dict((v, k) for k, v in isic_id_index.items())\n",
    "\n",
    "    #Create a dictionary of metadata\n",
    "    #Use list of augmented samples to create a list of same size now including the metadata\n",
    "    dict_of_meta = {}\n",
    "    for pos, tup in enumerate(isic_ids_tuple):\n",
    "        # Use the lookup table to directly find the index\n",
    "        index = isic_id_index.get(tup[0], -1)  # -1 if not found\n",
    "\n",
    "        if index != -1:\n",
    "            # Access the row directly without masking\n",
    "            #dict_of_meta.update({pos: np.array(metadata.iloc[index].drop([\"isic_id\", \"target\"]).values, dtype=float)})\n",
    "            dict_of_meta.update({pos: np.array(metadata.iloc[index].values)})\n",
    "            # Process the row as needed\n",
    "        else:\n",
    "            raise Exception(\"isic_id values are not all unique\")\n",
    "    return dict_of_meta, col_num_id, col_num_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the metadata dictionaries for train-validate-test\n",
    "train_meta_dict, train_pos_isic_id, train_pos_target = make_meta_dict(metadata, train_ids)\n",
    "val_meta_dict, val_pos_isic_id, val_pos_target = make_meta_dict(metadata, val_ids)\n",
    "test_meta_dict, test_pos_isic_id, test_pos_target = make_meta_dict(metadata, test_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMAGE generator in a class\n",
    "class hdf5_generator_all_included(Generator):\n",
    "    def __init__(self, file, meta_dict, dict_pos_isic_id, dict_pos_target, num_features, imgSize, is_training=False, shuffle_seed=None):\n",
    "        self.file = file\n",
    "        self.meta_dict = meta_dict\n",
    "        self.dict_pos_isic_id = dict_pos_isic_id\n",
    "        self.dict_pos_target = dict_pos_target\n",
    "        self.num_features = num_features\n",
    "        self.imgSize = imgSize\n",
    "        self.is_training = is_training\n",
    "        self.shuffle_seed = shuffle_seed\n",
    "        self.len = len(meta_dict)\n",
    "        self.start = 0\n",
    "        self.stop = self.len\n",
    "        self.i = self.start\n",
    "        self.error_check()\n",
    "        self.open_hdf5()\n",
    "        self.order_and_shuffle()\n",
    "        \n",
    "    def send(self, value):\n",
    "        if self.i < self.stop:\n",
    "            if self.i == self.start:\n",
    "                self.open_hdf5()\n",
    "\n",
    "            #Retrieve index of isic_id according to the shuffled order\n",
    "            index = self.order[self.i]\n",
    "\n",
    "            #Retrieve target\n",
    "            target = self.meta_dict[index][self.dict_pos_target]\n",
    "            target = np.reshape(target, (1,1))\n",
    "            target = tf.cast(target, dtype=tf.int32)\n",
    "\n",
    "            #Retrieve metadata\n",
    "            meta = np.delete(self.meta_dict[index], [self.dict_pos_isic_id, self.dict_pos_target], 0)\n",
    "            meta = meta.astype(dtype=float)\n",
    "            meta = tf.cast(meta, dtype=tf.float32)\n",
    "            meta = tf.reshape(meta, shape=(1, self.num_features))\n",
    "\n",
    "            try:\n",
    "                #Retrieve isic_id\n",
    "                img_name = self.meta_dict[index][self.dict_pos_isic_id]\n",
    "                # Load image data from HDF5\n",
    "                img = np.array(Image.open(io.BytesIO(self.h5file[img_name][()])))\n",
    "                #Augment image\n",
    "                    #add hair removal here (use toggle to activate?)\n",
    "                    #add augment function here\n",
    "                # Resize the image\n",
    "                img = cv2.resize(img, (self.imgSize, self.imgSize), interpolation= cv2.INTER_AREA)\n",
    "                # Standardize and return as TensorFlow constant\n",
    "                img = tf.constant(img / 255, dtype=tf.float32)\n",
    "                #Augment counter\n",
    "                self.i = self.i + 1\n",
    "                return (img, meta), target\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error loading image {img_name}: {e}\")\n",
    "                # log the error to a file for later analysis\n",
    "                with open('image_errors.log', 'a') as f:\n",
    "                    f.write(f\"Error loading image {img_name}: {e}\\n\")\n",
    "\n",
    "            if self.i == self.stop:\n",
    "                self.h5file.close()\n",
    "        raise StopIteration\n",
    "\n",
    "    def throw(self, typ, val=None, tb=None):\n",
    "        #Close HDF5 file and terminate generator\n",
    "        try:\n",
    "            self.h5file.close()\n",
    "            super().throw(typ, val, tb)\n",
    "        except:\n",
    "            super().throw(typ, val, tb)\n",
    "\n",
    "    def error_check(self):\n",
    "        #Seed type check\n",
    "        try:\n",
    "            int(self.shuffle_seed) == self.shuffle_seed\n",
    "        except:\n",
    "            if self.shuffle_seed != None:\n",
    "                raise Exception(\"Seed must either be an integer or None\")\n",
    "\n",
    "    def order_and_shuffle(self):\n",
    "        np.random.seed(self.shuffle_seed)\n",
    "        self.order = np.array(list(self.meta_dict.keys()), dtype=int)\n",
    "        if self.shuffle_seed != None:\n",
    "            np.random.shuffle(self.order)\n",
    "\n",
    "    def open_hdf5(self):\n",
    "        self.h5file = h5py.File(self.file, 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(hdf5_file, meta_dict, dict_pos_isic_id, dict_pos_target, imgSize=100, batch_size=32, is_training=False, shuffle_seed = None):\n",
    "    num_features = len(val_meta_dict[0]) - 2 #Subtract isic_id and target\n",
    "\n",
    "    combined_generator = hdf5_generator_all_included(hdf5_file, meta_dict, dict_pos_isic_id, dict_pos_target, num_features, imgSize, is_training, shuffle_seed)\n",
    "\n",
    "    # Generate image dataset\n",
    "    element_spec = ((tf.TensorSpec(shape=(imgSize, imgSize, 3), dtype=tf.float32),\n",
    "                    tf.TensorSpec(shape=(1, num_features), dtype=tf.float32)),\n",
    "                    tf.TensorSpec(shape=(1, 1), dtype=tf.int32))\n",
    "\n",
    "    img_dataset = tf.data.Dataset.from_generator(\n",
    "        lambda: combined_generator,\n",
    "        output_signature=element_spec\n",
    "    )\n",
    "\n",
    "    return img_dataset.batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#THESE MUST BE MOVED TO THE CORRECT PLACE! TRAIN-TEST WITHIN FIT & TEST AFTER FIT\n",
    "#Initialize the datasets\n",
    "train_dataset = make_dataset(hdf5_file, train_meta_dict, train_pos_isic_id, train_pos_target)\n",
    "val_dataset = make_dataset(hdf5_file, val_meta_dict, val_pos_isic_id, val_pos_target, batch_size = 1)\n",
    "test_dataset = make_dataset(hdf5_file, test_meta_dict, test_pos_isic_id, test_pos_target, batch_size = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#Test how long it takes to cycle through all the training batches\\ncount = 0\\nfor batch in train_dataset:\\n    count += 1\\nprint(\"Number of batches =\",count)\\nbatch\\n'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "#Test how long it takes to cycle through all the training batches\n",
    "count = 0\n",
    "for batch in train_dataset:\n",
    "    count += 1\n",
    "print(\"Number of batches =\",count)\n",
    "batch\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Take 1 batch from the dataset and check its content\\nfor batch in train_dataset.take(1):\\n    (img_batch, meta_batch), target_batch = batch\\n    \\n    # Print the shapes of the individual components\\n    print(f\"Image batch shape: {img_batch.shape}\")\\n    print(f\"Metadata batch shape: {meta_batch.shape}\")\\n    print(f\"Target batch shape: {target_batch.shape}\")\\n\\n# To count the total number of batches\\nbatch_count = 0\\nfor _ in train_dataset:\\n    batch_count += 1\\n\\nprint(f\"Total number of batches in the dataset: {batch_count}\")\\n'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Take 1 batch from the dataset and check its content\n",
    "for batch in train_dataset.take(1):\n",
    "    (img_batch, meta_batch), target_batch = batch\n",
    "    \n",
    "    # Print the shapes of the individual components\n",
    "    print(f\"Image batch shape: {img_batch.shape}\")\n",
    "    print(f\"Metadata batch shape: {meta_batch.shape}\")\n",
    "    print(f\"Target batch shape: {target_batch.shape}\")\n",
    "\n",
    "# To count the total number of batches\n",
    "batch_count = 0\n",
    "for _ in train_dataset:\n",
    "    batch_count += 1\n",
    "\n",
    "print(f\"Total number of batches in the dataset: {batch_count}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) Alternative dataset function to load all in memory (not a generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### FIXES TO MAKE:\n",
    "- This is really slow. Improve it by taking concepts from the generator function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hdf5_reader(file, img_names, imgSize, is_training=False):\n",
    "    tensors = []\n",
    "    with h5py.File(file, 'r') as h5file:\n",
    "        for i, img_name_tuple in enumerate(img_names):\n",
    "            img_name, targ, mod = img_name_tuple\n",
    "            try:\n",
    "                # Load image data from HDF5\n",
    "                img = np.array(Image.open(io.BytesIO(h5file[img_name][()])))\n",
    "                \n",
    "                # Clean image\n",
    "                img = hair_removal(img)\n",
    "                \"\"\"\n",
    "                #if mod != 0:\n",
    "                if is_training==True:\n",
    "                    # Data Augmentation \n",
    "                    if mod != 0 and mod != -1:\n",
    "                        img= augment_image(img)\n",
    "                else:\n",
    "                    img = tf.image.random_flip_left_right(img)\n",
    "                    img = tf.image.random_flip_up_down(img)\n",
    "                    img = tf.image.random_brightness(img, max_delta=0.2)\n",
    "                    img = tf.image.random_contrast(img, lower=0.85, upper=1.15)\n",
    "                    img = tf.image.random_saturation(img, lower=0.85, upper=1.15)\n",
    "                \"\"\"    \n",
    "                    \n",
    "                # Resize the image\n",
    "                img = tf.image.resize(img, [imgSize, imgSize])\n",
    "\n",
    "                # Standardize and return as TensorFlow constant\n",
    "                img = tf.constant(img / 255, dtype=tf.float32)  # Standardize here\n",
    "\n",
    "                \"\"\"\n",
    "                #Build up the tensor\n",
    "                if i == 0:\n",
    "                    tensors = tf.reshape(img, [1, imgSize, imgSize, 3])\n",
    "                else:\n",
    "                    temp = tf.reshape(img, [1, 100, 100, 3])\n",
    "                    tensors = tf.concat([tensors, temp], 0)\n",
    "                \"\"\"\n",
    "\n",
    "                #Build up the tensor\n",
    "                tensors.append(tf.reshape(img, [imgSize, imgSize, 3]))\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error loading image {img_name}: {e}\")\n",
    "                # log the error to a file for later analysis\n",
    "                with open('image_errors.log', 'a') as f:\n",
    "                    f.write(f\"Error loading image {img_name}: {e}\\n\")\n",
    "                continue\n",
    "    #return tf.concat(tensors, 0)\n",
    "    return tf.data.Dataset.from_tensor_slices(tensors, name = \"image\")\n",
    "\n",
    "#Generate the dataset with batch size and prefetching\n",
    "def make_dataset_in_memory(hdf5_file, metadata, img_names, imgSize=100, batch_size = 32, is_training=False):\n",
    "    # Get the number of metadata features (isic_id and target are present, so subtract)\n",
    "    num_features = metadata.shape[-1] - 2\n",
    "    \n",
    "    # Generate image dataset\n",
    "    img_dataset = hdf5_reader(hdf5_file, img_names, imgSize=100, is_training=False)\n",
    "    #img_dataset = tf.data.Dataset.from_tensor_slices(img_dataset, name = \"image\")\n",
    "\n",
    "    #Extract isic_ids and target values\n",
    "    ids = [item[0] for item in img_names]\n",
    "    targ = [item[1] for item in img_names]\n",
    "\n",
    "    #Generate metadata for each element in the list\n",
    "    colnames = metadata.columns.tolist()\n",
    "    values = []\n",
    "    for id in ids:\n",
    "        values += [metadata[metadata[\"isic_id\"]==id].values.flatten().tolist()]\n",
    "    values = pd.DataFrame(data=values, columns = colnames)\n",
    "    values.drop(labels=[\"isic_id\", \"target\"], axis=1, inplace=True)\n",
    "\n",
    "    #Convert metadata to tensors\n",
    "    num_features = values.shape[-1]\n",
    "    meta = tf.cast(values, dtype=tf.float32)\n",
    "    meta = tf.reshape(meta, shape=(len(img_names),1,num_features))\n",
    "    meta = tf.data.Dataset.from_tensor_slices(meta, name = \"metadata\")\n",
    "\n",
    "    #Convert target to tensors\n",
    "    target = [np.reshape(element, (1,1)) for element in targ]\n",
    "    target = tf.cast(target, dtype=tf.int32)\n",
    "    target = tf.data.Dataset.from_tensor_slices(target, name = \"target\")\n",
    "\n",
    "    #Combine datasets into one\n",
    "    dataset = tf.data.Dataset.zip((img_dataset, meta))\n",
    "    dataset = tf.data.Dataset.zip((dataset, target))\n",
    "\n",
    "    return dataset.batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load validation dataset into memory to speed up the model val_loss calc\n",
    "#val_in_memory = make_dataset_in_memory(hdf5_file, metadata, val_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8) CNN MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 - Model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simple CNN model using only images and target\n",
    "class CNN_model(tf.keras.Model):\n",
    "    def __init__(self, neurons = 8, activ = 'leaky_relu', img_size = 100, img_channels=3):\n",
    "        #Run the constructor of the parent class\n",
    "        super().__init__()\n",
    "\n",
    "        #Weight and bias initializers\n",
    "        kernel_initializer = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.05, seed=None)\n",
    "        bias_initializer = tf.keras.initializers.RandomUniform(minval=-0.05, maxval=0.05, seed=None)\n",
    "        \n",
    "        #Image size declaration\n",
    "        self.img_size = img_size\n",
    "        self.img_channels = img_channels\n",
    "\n",
    "        #Layers\n",
    "        self.conv1 = tf.keras.layers.Conv2D(filters=16, kernel_size=5, strides=(1, 1), activation='relu', padding='same', input_shape=(img_size, img_size, img_channels),\n",
    "                                            kernel_initializer=kernel_initializer, bias_initializer=bias_initializer)\n",
    "        self.pool1 = tf.keras.layers.MaxPool2D(pool_size=(2,2))\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.dense1 = tf.keras.layers.Dense(neurons, activation = activ, kernel_initializer=kernel_initializer, bias_initializer=bias_initializer)\n",
    "        self.dense2 = tf.keras.layers.Dense(1, activation='sigmoid', kernel_initializer=kernel_initializer, bias_initializer=bias_initializer)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x_image, x_meta = inputs\n",
    "\n",
    "        # Convolutions\n",
    "        x1 = self.conv1(x_image)\n",
    "        x1 = self.pool1(x1)\n",
    "\n",
    "        # Flattening of images for input layer\n",
    "        x1 = self.flatten(x1)\n",
    "\n",
    "        # Hidden layers of neural network\n",
    "        x1 = self.dense1(x1)\n",
    "\n",
    "        # Output layer of neural network\n",
    "        output = self.dense2(x1)\n",
    "\n",
    "        return output\n",
    "\n",
    "#Metadata Neural Network\n",
    "class Meta_model(tf.keras.Model):\n",
    "    def __init__(self, neurons = 8, activ = 'tanh'):\n",
    "        #Run the constructor of the parent class\n",
    "        super().__init__()\n",
    "\n",
    "        #Weight and bias initializers\n",
    "        kernel_initializer = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.05, seed=None)\n",
    "        bias_initializer = tf.keras.initializers.RandomUniform(minval=-0.05, maxval=0.05, seed=None)\n",
    "\n",
    "        #Layers\n",
    "        self.dense1 = tf.keras.layers.Dense(neurons, activation = activ, kernel_initializer=kernel_initializer, bias_initializer=bias_initializer)\n",
    "        self.dense2 = tf.keras.layers.Dense(neurons, activation = activ, kernel_initializer=kernel_initializer, bias_initializer=bias_initializer)\n",
    "        self.dense3 = tf.keras.layers.Dense(1, activation='sigmoid', kernel_initializer=kernel_initializer, bias_initializer=bias_initializer)\n",
    "        self.dropout = tf.keras.layers.Dropout(0.25)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        x_image, x_meta = inputs\n",
    "        x_all = tf.reshape(x_meta, (tf.shape(x_meta)[0], x_meta.shape[-1]))\n",
    "        # Neural Network\n",
    "        x_all = self.dense1(x_all)\n",
    "        x_all = self.dense2(x_all)\n",
    "        if training:\n",
    "            x_all = self.dropout(x_all, training=training)\n",
    "        output = self.dense3(x_all)\n",
    "        return output\n",
    "\n",
    "#Hybrid CNN model taking metadata\n",
    "class Hybrid_model(tf.keras.Model):\n",
    "    def __init__(self, neurons = 8, activ = 'leaky_relu', img_size = 100, img_channels = 3):\n",
    "        #Run the constructor of the parent class\n",
    "        super().__init__()\n",
    "\n",
    "        #Weight and bias initializers\n",
    "        kernel_initializer = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.05, seed=None)\n",
    "        bias_initializer = tf.keras.initializers.RandomUniform(minval=-0.05, maxval=0.05, seed=None)\n",
    "\n",
    "        #Image size declaration\n",
    "        self.img_size = img_size\n",
    "        self.img_channels = img_channels\n",
    "\n",
    "        #Layers\n",
    "        self.conv1 = tf.keras.layers.Conv2D(filters=32, kernel_size=5, strides=(1, 1), activation='relu', padding='same', input_shape=(img_size, img_size, img_channels),\n",
    "                                            kernel_initializer=kernel_initializer, bias_initializer=bias_initializer)\n",
    "        self.conv2 = tf.keras.layers.Conv2D(64, 5, activation='relu', kernel_initializer=kernel_initializer, bias_initializer=bias_initializer)\n",
    "        self.pool = tf.keras.layers.MaxPool2D(pool_size=(2,2))\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.dense1 = tf.keras.layers.Dense(neurons, activation = activ, kernel_initializer=kernel_initializer, bias_initializer=bias_initializer)\n",
    "        self.dropout1 = tf.keras.layers.Dropout(0.10)\n",
    "        self.dense2 = tf.keras.layers.Dense(neurons, activation = activ, kernel_initializer=kernel_initializer, bias_initializer=bias_initializer)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(0.10)\n",
    "        self.dense3 = tf.keras.layers.Dense(1, activation='sigmoid', kernel_initializer=kernel_initializer, bias_initializer=bias_initializer)\n",
    "        self.concatenate = keras.layers.Concatenate(axis=1)\n",
    "        \n",
    "    def call(self, inputs, training=False):\n",
    "        x_image, x_meta = inputs\n",
    "        # Convolutions\n",
    "        x = self.conv1(x_image)\n",
    "        x = self.pool(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.pool(x)\n",
    "        # Flattening of images and concatenation with other data\n",
    "        x = self.flatten(x)\n",
    "        # Reshape metadata to match dimensions\n",
    "        x_meta = tf.reshape(x_meta, (tf.shape(x_meta)[0], x_meta.shape[-1]))\n",
    "        x_all = self.concatenate([x, x_meta])\n",
    "        # Neural Network\n",
    "        x_all = self.dense1(x_all)\n",
    "        if training:\n",
    "            x_all = self.dropout1(x_all, training=training)\n",
    "        x_all = self.dense2(x_all)\n",
    "        if training:\n",
    "            x_all = self.dropout2(x_all, training=training)\n",
    "        output = self.dense3(x_all)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 - Model compiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Andrew\\anaconda3\\envs\\cancer_isic\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "#Set seed\n",
    "tf.random.set_seed(71)\n",
    "\n",
    "#Initialize model\n",
    "#model = CNN_model(neurons=8, activ='tanh')\n",
    "model = Hybrid_model(neurons=36, activ='leaky_relu')\n",
    "#model = Meta_model(neurons=18, activ='tanh')\n",
    "\n",
    "#Define optimizer and loss function\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "loss = tf.keras.losses.BinaryCrossentropy(from_logits=False,\n",
    "                                          label_smoothing=0.0,\n",
    "                                          axis=-1,\n",
    "                                          reduction='sum_over_batch_size',\n",
    "                                          name='binary_crossentropy')\n",
    "\n",
    "#Compile the model with loss, optimizer, and metrics\n",
    "model.compile(loss = loss,\n",
    "              optimizer = optimizer,\n",
    "              metrics = [\n",
    "                  tf.keras.metrics.BinaryAccuracy(),\n",
    "                  tf.keras.metrics.FalseNegatives(),\n",
    "                  tf.keras.metrics.FalsePositives(),\n",
    "                  tf.keras.metrics.TrueNegatives(),\n",
    "                  tf.keras.metrics.TruePositives()\n",
    "                  ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3 - Model fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clear the memory leak in Keras\n",
    "class CustomCallback(tf.keras.callbacks.Callback):\n",
    "  def on_epoch_end(self, epoch, logs=None):\n",
    "    gc.collect()\n",
    "    #print(f\"Epoch {epoch+1} finished. Validation loss: {logs['val_loss']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training batches in dataset: 360\n",
      "Total validate batches in dataset: 1544\n",
      "Total test batches in dataset: 3952\n"
     ]
    }
   ],
   "source": [
    "#Set batch sizes\n",
    "train_batch_size = 32\n",
    "val_batch_size = 1\n",
    "test_batch_size = 1\n",
    "\n",
    "#Determine the number of batches\n",
    "nb_training_batches = int(len(train_ids)//train_batch_size)\n",
    "nb_validate_batches = int(len(val_ids)//val_batch_size)\n",
    "nb_test_batches = int(len(test_ids)//test_batch_size)\n",
    "\n",
    "#Print results\n",
    "print(\"Total training batches in dataset:\", nb_training_batches)\n",
    "print(\"Total validate batches in dataset:\", nb_validate_batches)\n",
    "print(\"Total test batches in dataset:\", nb_test_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get weights for training\n",
    "weight_for_0, weight_for_1 = compute_class_weights(train_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1\n",
      "First training ID: ISIC_6321919\n",
      "\u001b[1m360/360\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 285ms/step - binary_accuracy: 0.6456 - false_negatives: 240.2549 - false_positives: 1685.5568 - loss: 0.7253 - true_negatives: 3613.0830 - true_positives: 253.0166 - val_binary_accuracy: 0.7429 - val_false_negatives: 333.0000 - val_false_positives: 64.0000 - val_loss: 0.5884 - val_true_negatives: 941.0000 - val_true_positives: 206.0000\n",
      "WARNING:tensorflow:From c:\\Users\\Andrew\\anaconda3\\envs\\cancer_isic\\Lib\\site-packages\\keras\\src\\backend\\common\\global_state.py:82: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Run the model through epochs\n",
    "nb_epochs = 1\n",
    "early_break = False #End early in case of increasing validation loss\n",
    "\n",
    "for epoch in range(1, nb_epochs + 1):\n",
    "    #Make datasets\n",
    "    print(\"EPOCH\", epoch)\n",
    "    print(\"First training ID:\", train_meta_dict[0][0])\n",
    "    shuffle_seed = 8 + epoch #Next initialization of datasets will have a different shuffle\n",
    "    train_dataset = make_dataset(hdf5_file, train_meta_dict, train_pos_isic_id, train_pos_target, batch_size = train_batch_size, is_training=True, shuffle_seed=shuffle_seed)\n",
    "    val_dataset = make_dataset(hdf5_file, val_meta_dict, val_pos_isic_id, val_pos_target, batch_size = val_batch_size, is_training=False, shuffle_seed=shuffle_seed)\n",
    "    \n",
    "    if save_val_in_memory:\n",
    "        mod = model.fit(train_dataset, epochs=1, steps_per_epoch = nb_training_batches, validation_data = val_in_memory, callbacks = [CustomCallback()],\n",
    "                        class_weight={0: weight_for_0, 1: weight_for_1})\n",
    "    else:\n",
    "        mod = model.fit(train_dataset, epochs=1, steps_per_epoch = nb_training_batches, validation_data = val_dataset, validation_steps = nb_validate_batches, callbacks = [CustomCallback()],\n",
    "                        class_weight={0: weight_for_0, 1: weight_for_1})\n",
    "    \n",
    "    #Save results\n",
    "    if epoch == 1:\n",
    "        results = mod.history\n",
    "    else:\n",
    "        for key in mod.history:   \n",
    "            results[key] += mod.history[key]\n",
    "\n",
    "    #Clean memory after use\n",
    "    del mod\n",
    "    del train_dataset\n",
    "    #If save_val_in_memory is not true, we have a generator. In this case, we want to delete the generator.\n",
    "    if save_val_in_memory != True:\n",
    "        del val_dataset\n",
    "    tf.keras.backend.clear_session()\n",
    "    gc.collect()\n",
    "\n",
    "    #Early termination (check after 15 epochs)\n",
    "    if epoch >= 15 and early_break == True:\n",
    "        #Calculate previous three changes, if positive, then loss is increasing\n",
    "        change1 = results[\"val_loss\"][-1] - results[\"val_loss\"][-2]\n",
    "        change2 = results[\"val_loss\"][-2] - results[\"val_loss\"][-3]\n",
    "        change3 = results[\"val_loss\"][-3] - results[\"val_loss\"][-4]\n",
    "\n",
    "        #Three consecutive increases in validation loss will stop the model\n",
    "        if change1 > 0 and change2 > 0 and change3 > 0:\n",
    "            break\n",
    "\n",
    "    #Save occasionally\n",
    "    #if (epoch % 25 == 0):\n",
    "    #    model.save(f\"XXXX\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<KerasVariable shape=(5, 5, 3, 32), dtype=float32, path=hybrid_model/conv2d/kernel>,\n",
       " <KerasVariable shape=(32,), dtype=float32, path=hybrid_model/conv2d/bias>,\n",
       " <KerasVariable shape=(5, 5, 32, 64), dtype=float32, path=hybrid_model/conv2d_1/kernel>,\n",
       " <KerasVariable shape=(64,), dtype=float32, path=hybrid_model/conv2d_1/bias>,\n",
       " <KerasVariable shape=(33885, 36), dtype=float32, path=hybrid_model/dense/kernel>,\n",
       " <KerasVariable shape=(36,), dtype=float32, path=hybrid_model/dense/bias>,\n",
       " <KerasVariable shape=(36, 36), dtype=float32, path=hybrid_model/dense_1/kernel>,\n",
       " <KerasVariable shape=(36,), dtype=float32, path=hybrid_model/dense_1/bias>,\n",
       " <KerasVariable shape=(36, 1), dtype=float32, path=hybrid_model/dense_2/kernel>,\n",
       " <KerasVariable shape=(1,), dtype=float32, path=hybrid_model/dense_2/bias>]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 1.0)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plot the training and validation losses\n",
    "\n",
    "#Convert loss results into a datafram\n",
    "result_preproc = pd.DataFrame({\n",
    "    'Epoch': [i+1 for i in range(len(results[\"loss\"]))], \n",
    "    'Train': results[\"loss\"],\n",
    "    'Validate': results[\"val_loss\"]\n",
    "    })\n",
    "\n",
    "# Convert dataframe from wide to long format\n",
    "df = pd.melt(result_preproc, ['Epoch'])\n",
    "\n",
    "#Make plot\n",
    "g = sns.lineplot(data=df, x='Epoch', y='value', hue='variable')\n",
    "g.set_title(\"Loss Curves\")\n",
    "g.legend_.set_title(\"Loss\")\n",
    "g.set_ylabel('Loss')\n",
    "g.set_ylim(0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BATCHES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Iterate through all batches in the dataset and print their shapes\\nfor i, batch in enumerate(train_dataset):\\n    (img_batch, meta_batch), target_batch = batch\\n    \\n    # Print the shapes of the current batch\\n    print(f\"Batch {i+1}:\")\\n    print(\"  Image Batch Shape:\", img_batch.shape)\\n    print(\"  Metadata Batch Shape:\", meta_batch.shape)\\n    print(\"  Target Batch Shape:\", target_batch.shape)\\n'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Iterate through all batches in the dataset and print their shapes\n",
    "for i, batch in enumerate(train_dataset):\n",
    "    (img_batch, meta_batch), target_batch = batch\n",
    "    \n",
    "    # Print the shapes of the current batch\n",
    "    print(f\"Batch {i+1}:\")\n",
    "    print(\"  Image Batch Shape:\", img_batch.shape)\n",
    "    print(\"  Metadata Batch Shape:\", meta_batch.shape)\n",
    "    print(\"  Target Batch Shape:\", target_batch.shape)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9) Predict Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TEST BATCHES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor i, batch in enumerate(test_dataset):\\n    (img_batch, meta_batch), target_batch = batch\\n    \\n    # Print the shapes of the current batch\\n    print(f\"Batch {i+1}:\")\\n    print(\"  Image Batch Shape:\", img_batch.shape)\\n    print(\"  Metadata Batch Shape:\", meta_batch.shape)\\n    print(\"  Target Batch Shape:\", target_batch.shape)\\n'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "for i, batch in enumerate(test_dataset):\n",
    "    (img_batch, meta_batch), target_batch = batch\n",
    "    \n",
    "    # Print the shapes of the current batch\n",
    "    print(f\"Batch {i+1}:\")\n",
    "    print(\"  Image Batch Shape:\", img_batch.shape)\n",
    "    print(\"  Metadata Batch Shape:\", meta_batch.shape)\n",
    "    print(\"  Target Batch Shape:\", target_batch.shape)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PREDICTIONS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3952/3952\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 10ms/step\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "need at least one array to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[47], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m predictions \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(test_dataset, steps \u001b[38;5;241m=\u001b[39m nb_test_batches)\n\u001b[0;32m      3\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;28mround\u001b[39m(i) \u001b[38;5;28;01mfor\u001b[39;00m i  \u001b[38;5;129;01min\u001b[39;00m predictions\u001b[38;5;241m.\u001b[39mflatten()])\n\u001b[1;32m----> 4\u001b[0m y_test \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape of prediction data:\u001b[39m\u001b[38;5;124m\"\u001b[39m, predictions\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[1;31mValueError\u001b[0m: need at least one array to concatenate"
     ]
    }
   ],
   "source": [
    "#Retrieve test predictions and real test values\n",
    "predictions = model.predict(test_dataset, steps = nb_test_batches)\n",
    "y_pred = np.array([round(i) for i  in predictions.flatten()])\n",
    "y_test = np.concatenate([y for x, y in test_dataset], axis=0).flatten()\n",
    "print(\"Shape of prediction data:\", predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_dataset = make_dataset(hdf5_file, test_meta_dict, test_pos_isic_id, test_pos_target, batch_size = 1)\n",
    "test_dataset.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the loss\n",
    "loss = sum(abs(y_test - y_pred))/len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Determine true/false positives and negatives\n",
    "pos_indices = y_test == 1\n",
    "neg_indices = y_test == 0\n",
    "\n",
    "#True positives\n",
    "true_pos = sum(abs(y_test[pos_indices] == y_pred[pos_indices]))\n",
    "\n",
    "#False negatives\n",
    "false_neg = sum(abs(y_test[pos_indices] != y_pred[pos_indices]))\n",
    "\n",
    "#True negatives\n",
    "true_neg = sum(abs(y_test[neg_indices] == y_pred[neg_indices]))\n",
    "\n",
    "#False positives\n",
    "false_pos = sum(abs(y_test[neg_indices] != y_pred[neg_indices]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"---TEST RESULTS---\")\n",
    "print(\"Loss on test data:\", loss)\n",
    "print(\"True positives:\", true_pos)\n",
    "print(\"False positives:\", false_pos)\n",
    "print(\"True negatives:\", true_neg)\n",
    "print(\"False negatives:\", false_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

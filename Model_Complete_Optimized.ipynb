{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL - IMAGE LOADING & NEURAL NETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "import gc\n",
    "import csv\n",
    "import os\n",
    "import io\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import keras_cv\n",
    "import random\n",
    "from collections.abc import Generator\n",
    "\n",
    "\n",
    "TF_ENABLE_ONEDNN_OPTS=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) HYPERPARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "train_frac_to_use = 0.25   #Reduce training data to this fraction\n",
    "val_frac_to_use = 0.25     #Reduce validation data to this fraction\n",
    "test_frac_to_use = 0.25     #Reduce validation data to this fraction\n",
    "save_val_in_memory = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) GENERAL FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#Function to show image\\ndef show_img(image):\\n    plt.imshow(image, interpolation=None)\\n    plt.grid(None)\\n    plt.show()\\n'"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "#Function to show image\n",
    "def show_img(image):\n",
    "    plt.imshow(image, interpolation=None)\n",
    "    plt.grid(None)\n",
    "    plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Image cropping\n",
    "def crop_image(images_list, nbPix = 100):\n",
    "    output_images = []\n",
    "    for image in images_list:\n",
    "        #Height adjustments\n",
    "        h = len(image)\n",
    "        adj = len(image) - nbPix\n",
    "        h1 = round(adj / 2) #Top\n",
    "        h2 = h - (adj - h1) #Bottom\n",
    "\n",
    "        #Width adjustments\n",
    "        w = len(image[0])\n",
    "        w_adj = w - nbPix\n",
    "        w1 = round(w_adj / 2) #Left\n",
    "        w2 = w - (w_adj - w1) #Right\n",
    "\n",
    "        img = image[h1:h2,w1:w2]\n",
    "        output_images.append(img)\n",
    "        \n",
    "    return np.array(output_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) IMPORT DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 - Declare file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_12992\\1307597089.py:16: DtypeWarning: Columns (51,52) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  metadata = pd.read_csv(os.path.join(base_path, \"train-metadata.csv\"))\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "#Parent directory for data files - FULL DATA\n",
    "dataPath = \"C:/Users/Andrew/Downloads/isic-2024-challenge/\"\n",
    "#Metadata file paths\n",
    "metaPath = dataPath + \"train-metadata.csv\"\n",
    "#Image file path\n",
    "hdf5_file = dataPath + \"train-image.hdf5\"\n",
    "'''\n",
    "#ALTERNATIVE 1 QUI MARCHE PARFOIS\n",
    "#base_path = \"C:/Users/admin/Documents/DSTI/DeepLearning/Project/Computer_Vision/isic-2024-challenge\"\n",
    "#hdf5_file = os.path.join(base_path, \"sampleclaire-image.hdf5\")\n",
    "#metadata = pd.read_csv(metaPath, sep=\",\")\n",
    "\n",
    "#ALTERNATIVE 2 QUI MARCHE PARFOIS\n",
    "base_path = \"C:/Users/admin/Documents/DSTI/DeepLearning/Project/Computer_Vision/isic-2024-challenge\"\n",
    "metadata = pd.read_csv(os.path.join(base_path, \"train-metadata.csv\"))\n",
    "hdf5_file = os.path.join(base_path, \"train-image.hdf5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 - Load metadata from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'metaPath' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[193], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#Import metadata from file\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m metadata \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[43mmetaPath\u001b[49m, sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'metaPath' is not defined"
     ]
    }
   ],
   "source": [
    "#Import metadata from file\n",
    "metadata = pd.read_csv(metaPath, sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- X_meta NA counts --\n",
      "isic_id                           0\n",
      "age_approx                     2798\n",
      "target                            0\n",
      "clin_size_long_diam_mm            0\n",
      "tbp_lv_areaMM2                    0\n",
      "tbp_lv_area_perim_ratio           0\n",
      "tbp_lv_eccentricity               0\n",
      "tbp_lv_minorAxisMM                0\n",
      "tbp_lv_color_std_mean             0\n",
      "tbp_lv_deltaLBnorm                0\n",
      "tbp_lv_radial_color_std_max       0\n",
      "tbp_lv_location                   0\n",
      "dtype: int64\n",
      "Number of unknown for tbp_lv_location 5756\n"
     ]
    }
   ],
   "source": [
    "#METADATA: color and size features having no NAs\n",
    "metadata = metadata[[\"isic_id\",\n",
    "                     \"age_approx\",\n",
    "                     \"target\",\n",
    "                     \"clin_size_long_diam_mm\",\n",
    "                     \"tbp_lv_areaMM2\",\n",
    "                     \"tbp_lv_area_perim_ratio\",\n",
    "                     \"tbp_lv_eccentricity\",\n",
    "                     \"tbp_lv_minorAxisMM\",\n",
    "                     \"tbp_lv_color_std_mean\",\n",
    "                     \"tbp_lv_deltaLBnorm\",\n",
    "                     \"tbp_lv_radial_color_std_max\",\n",
    "                     \"tbp_lv_location\"]]\n",
    "\n",
    "#Verify that there are no NAs\n",
    "print(\"-- X_meta NA counts --\")\n",
    "print(metadata.isna().sum())\n",
    "\n",
    "#Check number of Unknoxn for tbp_lv_location\n",
    "loc_unknown=metadata[metadata[\"tbp_lv_location\"]==\"Unknown\"]\n",
    "print(\"Number of unknown for tbp_lv_location\", len(loc_unknown))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Activate for debugging of the predict function\n",
    "#metadata[\"target_cheat\"] = metadata[\"target\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 - Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unknown for tbp_lv_location 0\n"
     ]
    }
   ],
   "source": [
    "metadata=metadata[metadata[\"tbp_lv_location\"]!=\"Unknown\"]\n",
    "\n",
    "loc_unknown2=metadata[metadata[\"tbp_lv_location\"]==\"Unknown\"]\n",
    "print(\"Number of unknown for tbp_lv_location\", len(loc_unknown2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['isic_id', 'age_approx', 'target', 'clin_size_long_diam_mm',\n",
      "       'tbp_lv_areaMM2', 'tbp_lv_area_perim_ratio', 'tbp_lv_eccentricity',\n",
      "       'tbp_lv_minorAxisMM', 'tbp_lv_color_std_mean', 'tbp_lv_deltaLBnorm',\n",
      "       'tbp_lv_radial_color_std_max', 'category_Head & Neck',\n",
      "       'category_Left Arm', 'category_Left Arm - Lower',\n",
      "       'category_Left Arm - Upper', 'category_Left Leg',\n",
      "       'category_Left Leg - Lower', 'category_Left Leg - Upper',\n",
      "       'category_Right Arm', 'category_Right Arm - Lower',\n",
      "       'category_Right Arm - Upper', 'category_Right Leg',\n",
      "       'category_Right Leg - Lower', 'category_Right Leg - Upper',\n",
      "       'category_Torso Back', 'category_Torso Back Bottom Third',\n",
      "       'category_Torso Back Middle Third', 'category_Torso Back Top Third',\n",
      "       'category_Torso Front', 'category_Torso Front Bottom Half',\n",
      "       'category_Torso Front Top Half'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#Apply One-hot encoding for location\n",
    "location=pd.get_dummies(metadata[\"tbp_lv_location\"],prefix='category')\n",
    "location = location.astype(int)\n",
    "metadata = pd.concat([metadata, location], axis=1)\n",
    "metadata=metadata.drop(\"tbp_lv_location\",axis=1)\n",
    "print(metadata.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- X_meta NA counts --\n",
      "isic_id                             0\n",
      "age_approx                          0\n",
      "target                              0\n",
      "clin_size_long_diam_mm              0\n",
      "tbp_lv_areaMM2                      0\n",
      "tbp_lv_area_perim_ratio             0\n",
      "tbp_lv_eccentricity                 0\n",
      "tbp_lv_minorAxisMM                  0\n",
      "tbp_lv_color_std_mean               0\n",
      "tbp_lv_deltaLBnorm                  0\n",
      "tbp_lv_radial_color_std_max         0\n",
      "category_Head & Neck                0\n",
      "category_Left Arm                   0\n",
      "category_Left Arm - Lower           0\n",
      "category_Left Arm - Upper           0\n",
      "category_Left Leg                   0\n",
      "category_Left Leg - Lower           0\n",
      "category_Left Leg - Upper           0\n",
      "category_Right Arm                  0\n",
      "category_Right Arm - Lower          0\n",
      "category_Right Arm - Upper          0\n",
      "category_Right Leg                  0\n",
      "category_Right Leg - Lower          0\n",
      "category_Right Leg - Upper          0\n",
      "category_Torso Back                 0\n",
      "category_Torso Back Bottom Third    0\n",
      "category_Torso Back Middle Third    0\n",
      "category_Torso Back Top Third       0\n",
      "category_Torso Front                0\n",
      "category_Torso Front Bottom Half    0\n",
      "category_Torso Front Top Half       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Calculate the mean of age_approx for each target group\n",
    "mean_age_malign = metadata.loc[metadata[\"target\"] == 1, \"age_approx\"].mean()\n",
    "mean_age_benign = metadata.loc[metadata[\"target\"] == 0, \"age_approx\"].mean()\n",
    "\n",
    "# Define a function to fill NA based on the target value\n",
    "def fill_na_by_target(row):\n",
    "    if pd.isna(row['age_approx']):\n",
    "        if row['target'] == 1:\n",
    "            return mean_age_malign\n",
    "        elif row['target'] == 0:\n",
    "            return mean_age_benign\n",
    "    return row['age_approx']\n",
    "\n",
    "# Apply the function to the age_approx column\n",
    "metadata['age_approx'] = metadata.apply(fill_na_by_target, axis=1)\n",
    "\n",
    "#Verify that there are no NAs\n",
    "print(\"-- X_meta NA counts --\")\n",
    "print(metadata.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n#from sklearn.preprocessing import StandardScaler\\nfrom sklearn.preprocessing import MinMaxScaler\\n#Normalization\\n#Select the column\\nfeature=metadata.drop(columns=['isic_id','target'])\\n\\n#scaler=StandardScaler() for standardization\\nscaler = MinMaxScaler()\\nfeature_standardized=scaler.fit_transform(feature)\\nfeature_standardized_df = pd.DataFrame(feature_standardized, columns=feature.columns)\\n\\nmetadata=pd.concat([metadata[['isic_id','target']].reset_index(drop=True), feature_standardized_df] , axis=1)\\nprint(len(metadata.columns))\\n\""
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "#from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "#Normalization\n",
    "#Select the column\n",
    "feature=metadata.drop(columns=['isic_id','target'])\n",
    "\n",
    "#scaler=StandardScaler() for standardization\n",
    "scaler = MinMaxScaler()\n",
    "feature_standardized=scaler.fit_transform(feature)\n",
    "feature_standardized_df = pd.DataFrame(feature_standardized, columns=feature.columns)\n",
    "\n",
    "metadata=pd.concat([metadata[['isic_id','target']].reset_index(drop=True), feature_standardized_df] , axis=1)\n",
    "print(len(metadata.columns))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n#from sklearn.preprocessing import StandardScaler\\nfrom sklearn.preprocessing import MinMaxScaler\\n#Normalization\\n#Select the column\\n\\nfeature=metadata.drop(columns=['isic_id','target', 'category_Head & Neck',\\n       'category_Left Arm', 'category_Left Arm - Lower',\\n       'category_Left Arm - Upper', 'category_Left Leg',\\n       'category_Left Leg - Lower', 'category_Left Leg - Upper',\\n       'category_Right Arm', 'category_Right Arm - Lower',\\n       'category_Right Arm - Upper', 'category_Right Leg',\\n       'category_Right Leg - Lower', 'category_Right Leg - Upper',\\n       'category_Torso Back Bottom Third', 'category_Torso Back Middle Third',\\n       'category_Torso Back Top Third','category_Torso Front',\\n       'category_Torso Front Bottom Half', 'category_Torso Front Top Half'])\\n'''\\n#Select the column\\nfeature=metadata.drop(columns=['isic_id','target', 'category_Head & Neck',\\n       'category_Left Arm', 'category_Left Arm - Lower',\\n       'category_Left Arm - Upper', 'category_Left Leg',\\n       'category_Left Leg - Lower', 'category_Left Leg - Upper',\\n       'category_Right Arm', 'category_Right Arm - Lower',\\n       'category_Right Arm - Upper', 'category_Right Leg',\\n       'category_Right Leg - Lower', 'category_Right Leg - Upper',\\n       'category_Torso Back Bottom Third', 'category_Torso Back Middle Third',\\n       'category_Torso Back Top Third',\\n       'category_Torso Front Bottom Half', 'category_Torso Front Top Half'])\\n'''\\n\\n#scaler=StandardScaler() for standardization\\nscaler = MinMaxScaler()\\nfeature_standardized=scaler.fit_transform(feature)\\nfeature_standardized_df = pd.DataFrame(feature_standardized, columns=feature.columns)\\n\\n\\nmetadata=pd.concat([metadata[['isic_id','target', 'category_Head & Neck',\\n       'category_Left Arm', 'category_Left Arm - Lower',\\n       'category_Left Arm - Upper', 'category_Left Leg',\\n       'category_Left Leg - Lower', 'category_Left Leg - Upper',\\n       'category_Right Arm', 'category_Right Arm - Lower',\\n       'category_Right Arm - Upper', 'category_Right Leg',\\n       'category_Right Leg - Lower', 'category_Right Leg - Upper',\\n       'category_Torso Back Bottom Third', 'category_Torso Back Middle Third',\\n       'category_Torso Back Top Third', 'category_Torso Front',\\n       'category_Torso Front Bottom Half', 'category_Torso Front Top Half']].reset_index(drop=True), feature_standardized_df] , axis=1)\\n'''\\n\\nmetadata=pd.concat([metadata[['isic_id','target', 'category_Head & Neck',\\n       'category_Left Arm', 'category_Left Arm - Lower',\\n       'category_Left Arm - Upper', 'category_Left Leg',\\n       'category_Left Leg - Lower', 'category_Left Leg - Upper',\\n       'category_Right Arm', 'category_Right Arm - Lower',\\n       'category_Right Arm - Upper', 'category_Right Leg',\\n       'category_Right Leg - Lower', 'category_Right Leg - Upper',\\n       'category_Torso Back Bottom Third', 'category_Torso Back Middle Third',\\n       'category_Torso Back Top Third',\\n       'category_Torso Front Bottom Half', 'category_Torso Front Top Half']].reset_index(drop=True), feature_standardized_df] , axis=1)\\n'''\\n#print(len(metadata.columns))\\n\""
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "#from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "#Normalization\n",
    "#Select the column\n",
    "\n",
    "feature=metadata.drop(columns=['isic_id','target', 'category_Head & Neck',\n",
    "       'category_Left Arm', 'category_Left Arm - Lower',\n",
    "       'category_Left Arm - Upper', 'category_Left Leg',\n",
    "       'category_Left Leg - Lower', 'category_Left Leg - Upper',\n",
    "       'category_Right Arm', 'category_Right Arm - Lower',\n",
    "       'category_Right Arm - Upper', 'category_Right Leg',\n",
    "       'category_Right Leg - Lower', 'category_Right Leg - Upper',\n",
    "       'category_Torso Back Bottom Third', 'category_Torso Back Middle Third',\n",
    "       'category_Torso Back Top Third','category_Torso Front',\n",
    "       'category_Torso Front Bottom Half', 'category_Torso Front Top Half'])\n",
    "'''\n",
    "#Select the column\n",
    "feature=metadata.drop(columns=['isic_id','target', 'category_Head & Neck',\n",
    "       'category_Left Arm', 'category_Left Arm - Lower',\n",
    "       'category_Left Arm - Upper', 'category_Left Leg',\n",
    "       'category_Left Leg - Lower', 'category_Left Leg - Upper',\n",
    "       'category_Right Arm', 'category_Right Arm - Lower',\n",
    "       'category_Right Arm - Upper', 'category_Right Leg',\n",
    "       'category_Right Leg - Lower', 'category_Right Leg - Upper',\n",
    "       'category_Torso Back Bottom Third', 'category_Torso Back Middle Third',\n",
    "       'category_Torso Back Top Third',\n",
    "       'category_Torso Front Bottom Half', 'category_Torso Front Top Half'])\n",
    "'''\n",
    "\n",
    "#scaler=StandardScaler() for standardization\n",
    "scaler = MinMaxScaler()\n",
    "feature_standardized=scaler.fit_transform(feature)\n",
    "feature_standardized_df = pd.DataFrame(feature_standardized, columns=feature.columns)\n",
    "\n",
    "\n",
    "metadata=pd.concat([metadata[['isic_id','target', 'category_Head & Neck',\n",
    "       'category_Left Arm', 'category_Left Arm - Lower',\n",
    "       'category_Left Arm - Upper', 'category_Left Leg',\n",
    "       'category_Left Leg - Lower', 'category_Left Leg - Upper',\n",
    "       'category_Right Arm', 'category_Right Arm - Lower',\n",
    "       'category_Right Arm - Upper', 'category_Right Leg',\n",
    "       'category_Right Leg - Lower', 'category_Right Leg - Upper',\n",
    "       'category_Torso Back Bottom Third', 'category_Torso Back Middle Third',\n",
    "       'category_Torso Back Top Third', 'category_Torso Front',\n",
    "       'category_Torso Front Bottom Half', 'category_Torso Front Top Half']].reset_index(drop=True), feature_standardized_df] , axis=1)\n",
    "'''\n",
    "\n",
    "metadata=pd.concat([metadata[['isic_id','target', 'category_Head & Neck',\n",
    "       'category_Left Arm', 'category_Left Arm - Lower',\n",
    "       'category_Left Arm - Upper', 'category_Left Leg',\n",
    "       'category_Left Leg - Lower', 'category_Left Leg - Upper',\n",
    "       'category_Right Arm', 'category_Right Arm - Lower',\n",
    "       'category_Right Arm - Upper', 'category_Right Leg',\n",
    "       'category_Right Leg - Lower', 'category_Right Leg - Upper',\n",
    "       'category_Torso Back Bottom Third', 'category_Torso Back Middle Third',\n",
    "       'category_Torso Back Top Third',\n",
    "       'category_Torso Front Bottom Half', 'category_Torso Front Top Half']].reset_index(drop=True), feature_standardized_df] , axis=1)\n",
    "'''\n",
    "#print(len(metadata.columns))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Train, Validate, Test Split + Preparation of Data Augmentation\n",
    "1. Make two separate lists of isic_ids for target=0 and target=1. Transform into tuples (isic_id, target, mod toggle). Base data has mod toggle = 0, meaning no adjustment will be made.\n",
    "2. Reserve 10% of target = 1 for validate\n",
    "3. Split into train-validate & test on both lists (0 and 1). Take the test lists (0 and 1), concatenate and shuffle them\n",
    "4. Create augmentation preparation function for target = 1: mod toggle = strictly positive integer (this adds more isic_ids to the list, with mod toggle non zero))\n",
    "5. Run augmentation preparation function on train-validate and the reserved validation data: mod toggle = strictly positive integer\n",
    "6. Split train-validate on both lists (0 and 1)\n",
    "7. Reduce the validation data on target = 0 by value specified in reduce_frac\n",
    "8. Concatenate and shuffle the train lists and validation lists\n",
    "9. Limit training and validation data to speed up training (take only fraction of prepared lists)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### FIXES TO MAKE\n",
    "- mode toggle for reserved data should be positive, not -1\n",
    "- change function name from augment to augment_prep (or something similar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "reserve_frac = 0.1        #Fraction of total original data of Target = 1 (reserved for use in validation data)\n",
    "test_frac = 0.2           #Fraction of total original data, excluding the reserved fraction, to use as the test data\n",
    "nb_of_augments = 100      #Number of augments to perform on Target = 1 images in train-validate sets\n",
    "val_frac = 0.33           #Fraction of augmented train-validate list to use as the validation data. The rest becomes the training data.\n",
    "nb_of_duplications = 15   #Number of duplications (simple augments) to perform on reserved validation fraction (Target = 1). Note: this is added to the validation data.\n",
    "reduce_frac = 0.8         #Fraction of Target = 0 samples to remove from the validation data (improves balance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 - Make two separate lists of isic_ids for target=0 and target=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FIXES TO MAKE: Make code shorter and more efficient if possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total ids with target = 0: 394910\n",
      "Total ids with target = 1: 393\n"
     ]
    }
   ],
   "source": [
    "#Make a list of isic_ids for each target value (0 and 1)\n",
    "isic_id_target_0 = metadata[metadata['target'] == 0]['isic_id'].tolist()\n",
    "isic_id_target_1 = metadata[metadata['target'] == 1]['isic_id'].tolist()\n",
    "\n",
    "#Retrieve dataframe with isic id and target\n",
    "temp_0 = metadata[metadata[\"isic_id\"].isin(isic_id_target_0)].loc[:,[\"isic_id\",\"target\"]]\n",
    "temp_1 = metadata[metadata[\"isic_id\"].isin(isic_id_target_1)].loc[:,[\"isic_id\",\"target\"]]\n",
    "\n",
    "#Convert into list of tuples... this makes it compatible with data augmentations\n",
    "#Form: (isic_id, target, mod toggle)\n",
    "isic_id_target_0 = list(zip(temp_0.iloc[:,0], temp_0.iloc[:,1], [0]*len(temp_0)))\n",
    "isic_id_target_1 = list(zip(temp_1.iloc[:,0], temp_1.iloc[:,1], [0]*len(temp_1)))\n",
    "\n",
    "#Delete temporary dataframes (the original metadata dataframe is untouched)\n",
    "del temp_0\n",
    "del temp_1\n",
    "\n",
    "#Count the number of occurrences for each target value\n",
    "print(\"Total ids with target = 0:\", len(isic_id_target_0))\n",
    "print(\"Total ids with target = 1:\", len(isic_id_target_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 - Reserve 10% of target = 1 for validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total ids with target = 0: 394910\n",
      "Total ids with target = 1: 353\n",
      "Total reserved target = 1: 40\n"
     ]
    }
   ],
   "source": [
    "#Keep 10% of isic_Id of target=1 without duplication\n",
    "isic_id_target_1, isic_id_target_1_reserved = train_test_split(isic_id_target_1, test_size = reserve_frac, random_state=88, shuffle=False)\n",
    "\n",
    "#Count the number of occurrences for each target value (AFTER RESERVATION)\n",
    "print(\"Total ids with target = 0:\", len(isic_id_target_0))\n",
    "print(\"Total ids with target = 1:\", len(isic_id_target_1))\n",
    "print(\"Total reserved target = 1:\", len(isic_id_target_1_reserved))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 - Split into train-validate & test on both lists (0 and 1). Take the test lists (0 and 1), concatenate and shuffle them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split out the test ids\n",
    "trainval_0, test_0 = train_test_split(isic_id_target_0, test_size = test_frac, random_state=88, shuffle=True)\n",
    "trainval_1, test_1 = train_test_split(isic_id_target_1, test_size = test_frac, random_state=88, shuffle=True)\n",
    "\n",
    "test_ids = test_0 + test_1\n",
    "np.random.shuffle(test_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 - Create augmentation preparation function for target = 1: mod toggle = strictly positive integer\n",
    "(this adds more isic_ids to the list, with mod toggle non zero))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### FIXES TO MAKE:\n",
    "- verify if extend or append is faster for a list : extend faster + list of comprehension faster than the loop\n",
    "- remove simple_dupl (performed with is_training toggle in dataset generation function where augments are performed)\n",
    "- make \"temp = tuple_list + temp\" faster. Append or extend is faster than adding. To check.: use of temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate a list containing augmentation toggles. Apply only to training and validation sets.\n",
    "def augment(tuple_list, simple_dupl = False, nb_of_augments = 30, shuffle_seed=None):\n",
    "    temp = []\n",
    "    \n",
    "    \"\"\"\n",
    "    #If duplication is desired, then the mod toggle is -1\n",
    "    if simple_dupl:\n",
    "        for item in tuple_list:\n",
    "            a = item[0]\n",
    "            b = item[1]\n",
    "            temp.extend([(a, b, -1) for i in range(1, nb_of_augments + 1)])\n",
    "\n",
    "    \n",
    "    #If augmentation is desired, then the mod toggle is a strictly positive integer\n",
    "    else:\n",
    "    \"\"\"\n",
    "    temp = [(item[0], item[1], i) for item in tuple_list for i in range(1, nb_of_augments + 1)]\n",
    "    \n",
    "    '''\n",
    "    for item in tuple_list:\n",
    "        a = item[0] #isic_id\n",
    "        b = item[1] #target\n",
    "        temp.extend([(a, b, i) for i in range(1, nb_of_augments + 1)])\n",
    "'''\n",
    "    \n",
    "    #Shuffle the list\n",
    "    temp.extend(tuple_list)\n",
    "    #temp = tuple_list + temp\n",
    "    np.random.seed(shuffle_seed)\n",
    "    np.random.shuffle(temp)\n",
    "\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 - Run augmentation preparation function on train-validate and the reserved validation data: mod toggle = strictly positive integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Augment the training and validation list\n",
    "trainval_1 = augment(trainval_1, nb_of_augments=nb_of_augments, shuffle_seed=50)\n",
    "\n",
    "#Duplicate the reserved training data\n",
    "isic_id_target_1_reserved = augment(isic_id_target_1_reserved, nb_of_augments=nb_of_duplications, shuffle_seed=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 - Split train-validate on both lists (0 and 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split training and validation lists\n",
    "train_0, val_0 = train_test_split(trainval_0, test_size = val_frac, random_state=88, shuffle=True)\n",
    "train_1, val_1 = train_test_split(trainval_1, test_size = val_frac, random_state=88, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7 - Reduce the validation data on target = 0 by value specified in reduce_frac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reduce the validation data of type Target = 0\n",
    "nb_samples = int((1 - reduce_frac) * len(val_0))\n",
    "val_0 = random.sample(val_0, nb_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.8 - Concatenate and shuffle the train and validation lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### THINGS TO FIX:\n",
    "- Check if there is a more efficient way compared to adding lists : extend for train_ids = train_0 + train_1 and list(itertools.chain() for val_0 + val_1 + isic_id_target_1_reserved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import itertools\n",
    "#Concatenate\n",
    "train_ids=train_0.copy()\n",
    "train_ids.extend(train_1)\n",
    "val_ids=list(itertools.chain(val_0,val_1,isic_id_target_1_reserved))\n",
    "\n",
    "\n",
    "'''\n",
    "#Concatenate\n",
    "train_ids = train_0 + train_1\n",
    "val_ids = val_0 + val_1 + isic_id_target_1_reserved\n",
    "'''\n",
    "\n",
    "#Shuffle\n",
    "np.random.seed(60)\n",
    "np.random.shuffle(train_ids)\n",
    "np.random.shuffle(val_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Validate/Test Counts: 230753 / 30891 / 79053\n",
      "Train/Validate/Test Fractions: 0.68 / 0.09 / 0.23\n",
      "Proportion of Target = 1 in training data: 0.08269448284529345\n",
      "Proportion of Target = 1 in validation data: 0.3250137580525072\n",
      "Proportion of Target = 1 in test data: 0.0008981316332081009\n"
     ]
    }
   ],
   "source": [
    "#Calculate the proportaion of Target=1 in each set (training, validation, test)\n",
    "def calc_frac_target1(ids):\n",
    "    return sum([item[1] for item in ids]) / len(ids)\n",
    "\n",
    "tot_samples = len(train_ids) + len(val_ids) + len(test_ids)\n",
    "\n",
    "print(\"Train/Validate/Test Counts:\", len(train_ids), \"/\", len(val_ids), \"/\", len(test_ids))\n",
    "print(\"Train/Validate/Test Fractions:\", round(len(train_ids)/tot_samples,2), \"/\", round(len(val_ids)/tot_samples,2), \"/\", round(len(test_ids)/tot_samples,2))\n",
    "print(\"Proportion of Target = 1 in training data:\", calc_frac_target1(train_ids))\n",
    "print(\"Proportion of Target = 1 in validation data:\", calc_frac_target1(val_ids))\n",
    "print(\"Proportion of Target = 1 in test data:\", calc_frac_target1(test_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.9 - Limit training and validation data to speed up training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train ids length before: 230753\n",
      "Train ids length after: 57688\n",
      "Validate ids length before: 30891\n",
      "Validate ids length after: 7722\n",
      "Test ids length before: 79053\n",
      "Test ids length after: 19763\n"
     ]
    }
   ],
   "source": [
    "#Choose a portion of TRAINING ids to load into memory\n",
    "def take_fewer_samples(ids, frac_to_use, seed):\n",
    "    if frac_to_use < 1:\n",
    "        random.seed(seed)\n",
    "        k = int(frac_to_use * len(ids))\n",
    "        ids_short = random.choices(ids, k=k)\n",
    "        return ids_short\n",
    "\n",
    "print(\"Train ids length before:\", len(train_ids))\n",
    "train_ids = take_fewer_samples(train_ids, train_frac_to_use, seed=12)\n",
    "print(\"Train ids length after:\", len(train_ids))\n",
    "\n",
    "print(\"Validate ids length before:\", len(val_ids))\n",
    "val_ids = take_fewer_samples(val_ids, val_frac_to_use, seed=12)\n",
    "print(\"Validate ids length after:\", len(val_ids))\n",
    "\n",
    "print(\"Test ids length before:\", len(test_ids))\n",
    "test_ids = take_fewer_samples(test_ids, test_frac_to_use, seed=12)\n",
    "print(\"Test ids length after:\", len(test_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Augmentation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### THINGS TO FIX:\n",
    "- Try compute_class_weights with inverse weights instead of ratios: OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hair_removal(image, crop_pixels=10):\n",
    "    height_pixels = len(image)  # Image rows\n",
    "    width_pixels = len(image[0])  # Image columns\n",
    "\n",
    "    # Image cropping\n",
    "    height = [crop_pixels, height_pixels - crop_pixels]\n",
    "    width = [crop_pixels, width_pixels - crop_pixels]\n",
    "    img = image[height[0]:height[1], width[0]:width[1]]\n",
    "\n",
    "    # Gray scale\n",
    "    grayScale = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    # Black hat filter\n",
    "    kernel = cv2.getStructuringElement(1, (9, 9))\n",
    "    blackhat = cv2.morphologyEx(grayScale, cv2.MORPH_BLACKHAT, kernel)\n",
    "    # Gaussian filter\n",
    "    bhg = cv2.GaussianBlur(blackhat, (3, 3), cv2.BORDER_DEFAULT)\n",
    "    # Binary thresholding (MASK)\n",
    "    ret, mask = cv2.threshold(bhg, 10, 255, cv2.THRESH_BINARY)\n",
    "    # Replace pixels of the mask\n",
    "    dst = cv2.inpaint(img, mask, 6, cv2.INPAINT_TELEA)\n",
    "\n",
    "    return dst\n",
    "\n",
    "#def resize_image(image, target_size=(100, 100)):\n",
    "#    resized_image = cv2.resize(image, target_size, interpolation=cv2.INTER_AREA)\n",
    "#    return resized_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the augmentation function\n",
    "def augment_image(image,is_training=False):\n",
    "    \"\"\"\n",
    "    Apply a series of augmentations to create diverse variations of the input image.\n",
    "    Includes random flips, rotations, brightness adjustments, and other transformations.\n",
    "    \"\"\"\n",
    "    # Apply various augmentations\n",
    "    '''\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    image = tf.image.random_flip_up_down(image)\n",
    "    image = tf.image.rot90(image, tf.random.uniform(shape=[], minval=0, maxval=4, dtype=tf.int32))\n",
    "    image = tf.image.random_brightness(image, max_delta=0.2)\n",
    "    image = tf.image.random_contrast(image, lower=0.8, upper=1.2)\n",
    "    image = tf.image.random_saturation(image, lower=0.8, upper=1.2)\n",
    "    '''\n",
    "    # parameter\n",
    "    if is_training==True:\n",
    "        height_factor_cut=(0.02, 0.06),\n",
    "        width_factor_cut=(0.02, 0.06),\n",
    "        max_delta_brigth=0.25,\n",
    "        lower_sat=0.7, \n",
    "        upper_sat=1.8,\n",
    "        lower_cont=0.7, \n",
    "        upper_cont=1.8,\n",
    "        minval_rot=0,\n",
    "        maxvalue_rot=4\n",
    "    else:\n",
    "        height_factor_cut=(0, 0),\n",
    "        width_factor_cut=(0, 0),\n",
    "        max_delta_brigth=0.15,\n",
    "        lower_sat=0.8, \n",
    "        upper_sat=1.2,\n",
    "        lower_cont=0.8, \n",
    "        upper_cont=1.2,\n",
    "        minval_rot=0,\n",
    "        maxvalue_rot=0\n",
    "        \n",
    "    # RandomCutout initialization\n",
    "    cutout_layer = keras_cv.layers.RandomCutout(height_factor=height_factor_cut, width_factor=width_factor_cut)\n",
    "    \n",
    "    # List of augmentations\n",
    "    augmentations = [\n",
    "        tf.image.random_flip_left_right,  \n",
    "        tf.image.random_flip_up_down,   \n",
    "        lambda img: tf.image.rot90(img, tf.random.uniform(shape=[], minval=minval_rot, maxval=maxvalue_rot, dtype=tf.int32)), \n",
    "        tf.image.random_brightness,      \n",
    "        tf.image.random_contrast,         \n",
    "        tf.image.random_saturation,       \n",
    "        lambda img: cutout_layer(img)    \n",
    "    ]\n",
    "    \n",
    "    # Shuffle and pick one augmentation\n",
    "    augmentation = augmentations[tf.random.uniform(shape=[], minval=0, maxval=len(augmentations), dtype=tf.int32)]\n",
    "    \n",
    "    # Apply augmentation with 99% probability\n",
    "    if tf.random.uniform([]) < 0.95:\n",
    "        # Apply augmentation \n",
    "        if augmentation in [tf.image.random_flip_left_right, tf.image.random_flip_up_down]:\n",
    "            image = augmentation(image) \n",
    "        elif augmentation == tf.image.random_brightness:\n",
    "            image = augmentation(image, max_delta=max_delta_brigth)  \n",
    "        elif augmentation == tf.image.random_contrast:\n",
    "            image = augmentation(image, lower_cont=0.7, upper_cont=1.8)  \n",
    "        elif augmentation == tf.image.random_saturation:\n",
    "            image = augmentation(image, lower_sat=0.7, upper_sat=1.8)  \n",
    "        else:\n",
    "            image = augmentation(image)  \n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_class_weights(img_names):\n",
    "    # Initialize counters for target=0 and target=1\n",
    "    target_0_count = 0\n",
    "    target_1_count = 0\n",
    "\n",
    "    # Calculate total number of images\n",
    "    total = len(img_names)\n",
    "    # Calculate number of target = 1\n",
    "    target_1_count = sum([item[1] for item in img_names])\n",
    "    # Calculate number of target = 1\n",
    "    target_0_count = total - target_1_count\n",
    "\n",
    "    # Calculate class weights based on the counts, avoid division by zero\n",
    "    if target_1_count > 0 :\n",
    "        if target_1_count < target_0_count:\n",
    "            weight_for_0 = 1\n",
    "            weight_for_1 = target_0_count/target_1_count\n",
    "        elif target_0_count > 0:\n",
    "            weight_for_0 = target_1_count/target_0_count\n",
    "            weight_for_1 = 1\n",
    "        else:\n",
    "            weight_for_0=0\n",
    "            weigth_for_1=target_count_1\n",
    "    else:\n",
    "        weight_for_0=target_count_0\n",
    "        weight_for_1=0\n",
    "        \n",
    "\n",
    "    return weight_for_0, weight_for_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef compute_class_weights(img_names):\\n    # Initialize counters for target=0 and target=1\\n    target_0_count = 0\\n    target_1_count = 0\\n\\n    # Calculate total number of images\\n    total = len(img_names)\\n    # Calculate number of target = 1\\n    target_1_count = sum([item[1] for item in img_names])\\n    # Calculate number of target = 1\\n    target_0_count = total - target_1_count\\n\\n    # Calculate class weights based on the counts, avoid division by zero\\n    if target_0_count > 0:\\n        weight_for_0 = total / (2 * target_0_count)\\n    else:\\n        weight_for_0 = 1\\n\\n    if target_1_count > 0:\\n        weight_for_1 = total / (2 * target_1_count)\\n    else:\\n        weight_for_1 = 1\\n\\n    return weight_for_0, weight_for_1\\n    '"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def compute_class_weights(img_names):\n",
    "    # Initialize counters for target=0 and target=1\n",
    "    target_0_count = 0\n",
    "    target_1_count = 0\n",
    "\n",
    "    # Calculate total number of images\n",
    "    total = len(img_names)\n",
    "    # Calculate number of target = 1\n",
    "    target_1_count = sum([item[1] for item in img_names])\n",
    "    # Calculate number of target = 1\n",
    "    target_0_count = total - target_1_count\n",
    "\n",
    "    # Calculate class weights based on the counts, avoid division by zero\n",
    "    if target_0_count > 0:\n",
    "        weight_for_0 = total / (2 * target_0_count)\n",
    "    else:\n",
    "        weight_for_0 = 1\n",
    "\n",
    "    if target_1_count > 0:\n",
    "        weight_for_1 = total / (2 * target_1_count)\n",
    "    else:\n",
    "        weight_for_1 = 1\n",
    "\n",
    "    return weight_for_0, weight_for_1\n",
    "    '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Dataset generation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### THINGS TO VERIFY:\n",
    "- Claire can use these approches in EDA (distribution of image sizes)? This is very efficient.\n",
    "- Is this correct? \"if self.i < self.stop\" OR is it \"if self.i <= self.stop\" -> I think so : self.start=0 \n",
    "- Remove the initialization of train_dataset, val_dataset, and train_dataset. The first two are already done at each epoch. Test is done in the test part of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' (OLD VERSION WITHOUT THE MOD TOGGLE)\\n#Create a metadata dictionary for efficient lookup\\n\\n#Objective: We need \"train_ids\" to efficiently retrieve and augment images from the HDF5 file. This already exists.\\n#           We need to accesss metadata through a dictionary to improve speed. A common reference is needed for both.\\n\\n#Idea:      Since train_ids is a LIST of tuples (isic_id, target, mod toggle), it is accessed via indices (ex. train_ids[10]).\\n#           We need to make a dictionary that, for each train_ids index, lists all the metadata associated to the isic_id.\\n#           Thus, when train_ids[10] is called, we call the dictionary and request key=10 to get the metadata.\\n#           Result: very fast data retrieval\\n\\ndef make_meta_dict_old(metadata, isic_ids_tuple):\\n    #Reindex. The metadata must be contiguously indexed. Holes in index numbering will not work.\\n    metadata = metadata.reset_index(drop=True)\\n\\n    #Get the column number for \"isic_id\" and \"target\" in the metadata dataframe.\\n    #This allows us to know where these items are located in the metadata retrieved for each item.\\n    #This is used later to filter out these items from the metadata.\\n    col_num_id = metadata.columns.get_loc(\"isic_id\")\\n    col_num_target = metadata.columns.get_loc(\"target\")\\n\\n    #The metadata contains all unique isic_ids. Since the dataframe is reindexed, it is possible to make\\n    #a dictionary of (isic_id: row number) for fast retrieval of all metadata associated to an isic_id.\\n\\n    #Take the metadata dataframe and create a dictionary that stores (isic_id, row number).\\n    metadata_index_dict = metadata[\"isic_id\"].to_dict()\\n    metadata_index_dict = dict((v, k) for k, v in metadata_index_dict.items())\\n\\n    #Make a dictionary of (index: metadata), where index is the position of a sample in train_ids and metadata is the metadata\\n    #associated to the sample\\'s isic_id. We thus create a dict_of_meta that is a mirror image of the \"isic_ids_tuple\" list.\\n    #We must be careful to not shuffle \"isic_ids_tuple\".\\n    dict_of_meta = {}\\n    for pos, tup in enumerate(isic_ids_tuple):\\n        # Use the lookup table to directly find the index\\n        index = metadata_index_dict.get(tup[0], -1)  # -1 if not found\\n\\n        if index != -1:\\n            # Access the row directly without masking\\n            #dict_of_meta.update({pos: np.array(metadata.iloc[index].drop([\"isic_id\", \"target\"]).values, dtype=float)})\\n            dict_of_meta.update({pos: np.array(metadata.iloc[index].values)})\\n            # Process the row as needed\\n        else:\\n            raise Exception(\"isic_id values are not all unique\")\\n    return dict_of_meta, col_num_id, col_num_target\\n'"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" (OLD VERSION WITHOUT THE MOD TOGGLE)\n",
    "#Create a metadata dictionary for efficient lookup\n",
    "\n",
    "#Objective: We need \"train_ids\" to efficiently retrieve and augment images from the HDF5 file. This already exists.\n",
    "#           We need to accesss metadata through a dictionary to improve speed. A common reference is needed for both.\n",
    "\n",
    "#Idea:      Since train_ids is a LIST of tuples (isic_id, target, mod toggle), it is accessed via indices (ex. train_ids[10]).\n",
    "#           We need to make a dictionary that, for each train_ids index, lists all the metadata associated to the isic_id.\n",
    "#           Thus, when train_ids[10] is called, we call the dictionary and request key=10 to get the metadata.\n",
    "#           Result: very fast data retrieval\n",
    "\n",
    "def make_meta_dict_old(metadata, isic_ids_tuple):\n",
    "    #Reindex. The metadata must be contiguously indexed. Holes in index numbering will not work.\n",
    "    metadata = metadata.reset_index(drop=True)\n",
    "\n",
    "    #Get the column number for \"isic_id\" and \"target\" in the metadata dataframe.\n",
    "    #This allows us to know where these items are located in the metadata retrieved for each item.\n",
    "    #This is used later to filter out these items from the metadata.\n",
    "    col_num_id = metadata.columns.get_loc(\"isic_id\")\n",
    "    col_num_target = metadata.columns.get_loc(\"target\")\n",
    "\n",
    "    #The metadata contains all unique isic_ids. Since the dataframe is reindexed, it is possible to make\n",
    "    #a dictionary of (isic_id: row number) for fast retrieval of all metadata associated to an isic_id.\n",
    "\n",
    "    #Take the metadata dataframe and create a dictionary that stores (isic_id, row number).\n",
    "    metadata_index_dict = metadata[\"isic_id\"].to_dict()\n",
    "    metadata_index_dict = dict((v, k) for k, v in metadata_index_dict.items())\n",
    "\n",
    "    #Make a dictionary of (index: metadata), where index is the position of a sample in train_ids and metadata is the metadata\n",
    "    #associated to the sample's isic_id. We thus create a dict_of_meta that is a mirror image of the \"isic_ids_tuple\" list.\n",
    "    #We must be careful to not shuffle \"isic_ids_tuple\".\n",
    "    dict_of_meta = {}\n",
    "    for pos, tup in enumerate(isic_ids_tuple):\n",
    "        # Use the lookup table to directly find the index\n",
    "        index = metadata_index_dict.get(tup[0], -1)  # -1 if not found\n",
    "\n",
    "        if index != -1:\n",
    "            # Access the row directly without masking\n",
    "            #dict_of_meta.update({pos: np.array(metadata.iloc[index].drop([\"isic_id\", \"target\"]).values, dtype=float)})\n",
    "            dict_of_meta.update({pos: np.array(metadata.iloc[index].values)})\n",
    "            # Process the row as needed\n",
    "        else:\n",
    "            raise Exception(\"isic_id values are not all unique\")\n",
    "    return dict_of_meta, col_num_id, col_num_target\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a metadata dictionary for efficient lookup\n",
    "\n",
    "#Objective: We need \"train_ids\" to efficiently retrieve and augment images from the HDF5 file. This already exists.\n",
    "#           We need to accesss metadata through a dictionary to improve speed. A common reference is needed for both.\n",
    "\n",
    "#Idea:      Since train_ids is a LIST of tuples (isic_id, target, mod toggle), it is accessed via indices (ex. train_ids[10]).\n",
    "#           We need to make a dictionary that, for each train_ids index, lists all the metadata associated to the isic_id.\n",
    "#           Thus, when train_ids[10] is called, we call the dictionary and request key=10 to get the metadata.\n",
    "#           Result: very fast data retrieval\n",
    "\n",
    "def make_meta_dict(metadata, isic_ids_tuple):\n",
    "    #Reindex. The metadata must be contiguously indexed. Holes in index numbering will not work.\n",
    "    metadata = metadata.reset_index(drop=True)\n",
    "\n",
    "    #Get the column number for \"isic_id\" and \"target\" in the metadata dataframe.\n",
    "    #This allows us to know where these items are located in the metadata retrieved for each item.\n",
    "    #This is used later to filter out these items from the metadata.\n",
    "    col_num_id = metadata.columns.get_loc(\"isic_id\")\n",
    "    col_num_target = metadata.columns.get_loc(\"target\")\n",
    "\n",
    "    #The metadata contains all unique isic_ids. Since the dataframe is reindexed, it is possible to make\n",
    "    #a dictionary of (isic_id: row number) for fast retrieval of all metadata associated to an isic_id.\n",
    "\n",
    "    #Take the metadata dataframe and create a dictionary that stores (isic_id, row number).\n",
    "    metadata_index_dict = metadata[\"isic_id\"].to_dict()\n",
    "    metadata_index_dict = dict((v, k) for k, v in metadata_index_dict.items())\n",
    "\n",
    "    #Make a dictionary of (index: (mod toggle, metadata)), where index is the position of a sample in train_ids and metadata is the metadata\n",
    "    #associated to the sample's isic_id. We thus create a dict_of_meta that is a mirror image of the \"isic_ids_tuple\" list.\n",
    "    #We must be careful to not shuffle \"isic_ids_tuple\".\n",
    "    dict_of_meta = {}\n",
    "    for pos, tup in enumerate(isic_ids_tuple):\n",
    "        # Use the lookup table to directly find the index\n",
    "        index = metadata_index_dict.get(tup[0], -1)  # -1 if not found\n",
    "\n",
    "        if index != -1:\n",
    "            # Access the row directly without masking\n",
    "            #dict_of_meta.update({pos: np.array(metadata.iloc[index].drop([\"isic_id\", \"target\"]).values, dtype=float)})\n",
    "            dict_of_meta.update({pos: (tup[2], np.array(metadata.iloc[index].values))})\n",
    "            # Process the row as needed\n",
    "        else:\n",
    "            raise Exception(\"isic_id values are not all unique\")\n",
    "    return dict_of_meta, col_num_id, col_num_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Create the metadata dictionaries for train-validate-test\n",
    "train_meta_dict, train_pos_isic_id, train_pos_target = make_meta_dict(metadata, train_ids)\n",
    "val_meta_dict, val_pos_isic_id, val_pos_target = make_meta_dict(metadata, val_ids)\n",
    "test_meta_dict, test_pos_isic_id, test_pos_target = make_meta_dict(metadata, test_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ISIC_6321919'"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_meta_dict[0][1][0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAMPLE OF ITEMS FROM VALIDATION META DICTIONARY\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: (0,\n",
       "  array(['ISIC_3195665', 50.0, 0, 3.15, 3.509101, 26.59917, 0.7898227,\n",
       "         2.054795, 0.3056555, 6.432599, 0.4260001, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0], dtype=object)),\n",
       " 1: (0,\n",
       "  array(['ISIC_4247741', 60.0, 0, 3.05, 3.959467, 17.84113, 0.8248685,\n",
       "         1.797849, 0.3743128, 5.595377, 0.4519306, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=object)),\n",
       " 2: (0,\n",
       "  array(['ISIC_3355923', 40.0, 0, 2.7, 3.884406, 15.30396, 0.8146706,\n",
       "         1.643836, 0.9441455, 9.421089, 0.6529271, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], dtype=object))}"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Look some dictionary elements to understand the structure {index, (mod_toggle, metadata)}\n",
    "print(\"SAMPLE OF ITEMS FROM VALIDATION META DICTIONARY\")\n",
    "dict(filter(lambda item: item[0] in {0, 1, 2}, val_meta_dict.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMAGE generator in a class\n",
    "class hdf5_generator_all_included(Generator):\n",
    "    def __init__(self, file, meta_dict, dict_pos_isic_id, dict_pos_target, num_features, imgSize, is_training=False, shuffle_seed=None, hair_removal=False):\n",
    "        self.file = file\n",
    "        self.meta_dict = meta_dict\n",
    "        self.dict_pos_isic_id = dict_pos_isic_id\n",
    "        self.dict_pos_target = dict_pos_target\n",
    "        self.num_features = num_features\n",
    "        self.imgSize = imgSize\n",
    "        self.is_training = is_training\n",
    "        self.shuffle_seed = shuffle_seed\n",
    "        self.len = len(meta_dict)\n",
    "        self.start = 0\n",
    "        self.stop = self.len\n",
    "        self.i = self.start\n",
    "        self.error_check()\n",
    "        self.open_hdf5()\n",
    "        self.order_and_shuffle()\n",
    "        \n",
    "    def send(self, value):\n",
    "        if self.i < self.stop:\n",
    "            if self.i == self.start:\n",
    "                self.open_hdf5()\n",
    "\n",
    "            #Retrieve index of isic_id according to the shuffled order\n",
    "            index = self.order[self.i]\n",
    "\n",
    "            #Retrieve target... remember that each item of the the meta_dict is a tuple of (mod toggle, metadata)\n",
    "            target = self.meta_dict[index][1][self.dict_pos_target]\n",
    "            target = np.reshape(target, (1,1))\n",
    "            target = tf.cast(target, dtype=tf.int32)\n",
    "\n",
    "            #Retrieve metadata... remember that each item of the the meta_dict is a tuple of (mod toggle, metadata)\n",
    "            meta = np.delete(self.meta_dict[index][1], [self.dict_pos_isic_id, self.dict_pos_target], 0)\n",
    "            meta = meta.astype(dtype=float)\n",
    "            meta = tf.cast(meta, dtype=tf.float32)\n",
    "            meta = tf.reshape(meta, shape=(1, self.num_features))\n",
    "\n",
    "            try:\n",
    "                #Retrieve isic_id\n",
    "                img_name = self.meta_dict[index][1][self.dict_pos_isic_id]\n",
    "                mod_toggle = self.meta_dict[index][0]\n",
    "                # Load image data from HDF5\n",
    "                img = np.array(Image.open(io.BytesIO(self.h5file[img_name][()])))\n",
    "\n",
    "                #Add hair removal here (use toggle to activate?)\n",
    "                #if hair_removal:\n",
    "                    #add hair removal function here\n",
    "\n",
    "                #Augment image\n",
    "                #if mod_toggle > 0:\n",
    "                    #add augment function here\n",
    "                    \n",
    "                # Resize the image\n",
    "                img = cv2.resize(img, (self.imgSize, self.imgSize), interpolation= cv2.INTER_AREA)\n",
    "                # Standardize and return as TensorFlow constant\n",
    "                img = tf.constant(img / 255, dtype=tf.float32)\n",
    "                #Augment counter\n",
    "                self.i = self.i + 1\n",
    "                return (img, meta), target\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error loading image {img_name}: {e}\")\n",
    "                # log the error to a file for later analysis\n",
    "                with open('image_errors.log', 'a') as f:\n",
    "                    f.write(f\"Error loading image {img_name}: {e}\\n\")\n",
    "\n",
    "            if self.i == self.stop:\n",
    "                self.h5file.close()\n",
    "        raise StopIteration\n",
    "\n",
    "    def throw(self, typ, val=None, tb=None):\n",
    "        #Close HDF5 file and terminate generator\n",
    "        try:\n",
    "            self.h5file.close()\n",
    "            super().throw(typ, val, tb)\n",
    "        except:\n",
    "            super().throw(typ, val, tb)\n",
    "\n",
    "    def error_check(self):\n",
    "        #Seed type check\n",
    "        try:\n",
    "            int(self.shuffle_seed) == self.shuffle_seed\n",
    "        except:\n",
    "            if self.shuffle_seed != None:\n",
    "                raise Exception(\"Seed must either be an integer or None\")\n",
    "\n",
    "    def order_and_shuffle(self):\n",
    "        np.random.seed(self.shuffle_seed)\n",
    "        self.order = np.array(list(self.meta_dict.keys()), dtype=int)\n",
    "        if self.shuffle_seed != None:\n",
    "            np.random.shuffle(self.order)\n",
    "\n",
    "    def open_hdf5(self):\n",
    "        self.h5file = h5py.File(self.file, 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(hdf5_file, meta_dict, dict_pos_isic_id, dict_pos_target, imgSize=100, batch_size=32, is_training=False, shuffle_seed = None):\n",
    "    num_features = len(val_meta_dict[0][1]) - 2 #Subtract isic_id and target\n",
    "\n",
    "    combined_generator = hdf5_generator_all_included(hdf5_file, meta_dict, dict_pos_isic_id, dict_pos_target, num_features, imgSize, is_training, shuffle_seed)\n",
    "\n",
    "    # Generate image dataset\n",
    "    element_spec = ((tf.TensorSpec(shape=(imgSize, imgSize, 3), dtype=tf.float32),\n",
    "                    tf.TensorSpec(shape=(1, num_features), dtype=tf.float32)),\n",
    "                    tf.TensorSpec(shape=(1, 1), dtype=tf.int32))\n",
    "\n",
    "    img_dataset = tf.data.Dataset.from_generator(\n",
    "        lambda: combined_generator,\n",
    "        output_signature=element_spec\n",
    "    )\n",
    "\n",
    "    return img_dataset.batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#THESE SERVE NO PURPOSE HERE. THEY ARE INITIALIZED LATER, EXACTLY WHERE THE NEED TO BE INITIALIZED.\\n#Initialize the datasets\\ntrain_dataset = make_dataset(hdf5_file, train_meta_dict, train_pos_isic_id, train_pos_target)\\nval_dataset = make_dataset(hdf5_file, val_meta_dict, val_pos_isic_id, val_pos_target, batch_size = 1)\\ntest_dataset = make_dataset(hdf5_file, test_meta_dict, test_pos_isic_id, test_pos_target, batch_size = 1)\\n'"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''#THESE SERVE NO PURPOSE HERE. THEY ARE INITIALIZED LATER, EXACTLY WHERE THE NEED TO BE INITIALIZED.\n",
    "#Initialize the datasets\n",
    "train_dataset = make_dataset(hdf5_file, train_meta_dict, train_pos_isic_id, train_pos_target)\n",
    "val_dataset = make_dataset(hdf5_file, val_meta_dict, val_pos_isic_id, val_pos_target, batch_size = 1)\n",
    "test_dataset = make_dataset(hdf5_file, test_meta_dict, test_pos_isic_id, test_pos_target, batch_size = 1)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#Test how long it takes to cycle through all the training batches\\ncount = 0\\nfor batch in train_dataset:\\n    count += 1\\nprint(\"Number of batches =\",count)\\nbatch\\n'"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "#Test how long it takes to cycle through all the training batches\n",
    "count = 0\n",
    "for batch in train_dataset:\n",
    "    count += 1\n",
    "print(\"Number of batches =\",count)\n",
    "batch\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Take 1 batch from the dataset and check its content\\nfor batch in train_dataset.take(1):\\n    (img_batch, meta_batch), target_batch = batch\\n    \\n    # Print the shapes of the individual components\\n    print(f\"Image batch shape: {img_batch.shape}\")\\n    print(f\"Metadata batch shape: {meta_batch.shape}\")\\n    print(f\"Target batch shape: {target_batch.shape}\")\\n\\n# To count the total number of batches\\nbatch_count = 0\\nfor _ in train_dataset:\\n    batch_count += 1\\n\\nprint(f\"Total number of batches in the dataset: {batch_count}\")\\n'"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Take 1 batch from the dataset and check its content\n",
    "for batch in train_dataset.take(1):\n",
    "    (img_batch, meta_batch), target_batch = batch\n",
    "    \n",
    "    # Print the shapes of the individual components\n",
    "    print(f\"Image batch shape: {img_batch.shape}\")\n",
    "    print(f\"Metadata batch shape: {meta_batch.shape}\")\n",
    "    print(f\"Target batch shape: {target_batch.shape}\")\n",
    "\n",
    "# To count the total number of batches\n",
    "batch_count = 0\n",
    "for _ in train_dataset:\n",
    "    batch_count += 1\n",
    "\n",
    "print(f\"Total number of batches in the dataset: {batch_count}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) Alternative dataset function to load all in memory (not a generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### FIXES TO MAKE:\n",
    "- This is really slow. Improve it by taking concepts from the generator function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_in_memory_dataset(hdf5_file, meta_dict, dict_pos_isic_id, dict_pos_target, imgSize=100, batch_size=32, is_training=False, shuffle_seed=None):\n",
    "    num_features = len(meta_dict[0]) - 2  # Subtract isic_id and target columns\n",
    "\n",
    "    # Initialize lists to hold images, metadata, and targets\n",
    "    images = []\n",
    "    metas = []\n",
    "    targets = []\n",
    "\n",
    "    np.random.seed(shuffle_seed)\n",
    "    order = np.array(list(meta_dict.keys()), dtype=int)\n",
    "    if shuffle_seed is not None:\n",
    "        np.random.shuffle(order)\n",
    "\n",
    "    with h5py.File(hdf5_file, 'r') as h5file:\n",
    "        for i in range(len(meta_dict)):\n",
    "            index = order[i]\n",
    "            \n",
    "            # Retrieve target\n",
    "            target = meta_dict[index][dict_pos_target]\n",
    "            target = np.reshape(target, (1, 1))\n",
    "            target = tf.cast(target, dtype=tf.int32)\n",
    "\n",
    "            # Retrieve metadata\n",
    "            meta = np.delete(meta_dict[index], [dict_pos_isic_id, dict_pos_target], 0)\n",
    "            meta = meta.astype(dtype=float)\n",
    "            meta = tf.cast(meta, dtype=tf.float32)\n",
    "            meta = tf.reshape(meta, shape=(1, num_features))\n",
    "\n",
    "            try:\n",
    "                # Retrieve isic_id and load image\n",
    "                img_name = meta_dict[index][dict_pos_isic_id]\n",
    "                img = np.array(Image.open(io.BytesIO(h5file[img_name][()])))\n",
    "\n",
    "                # Apply augmentations if needed\n",
    "                img = hair_removal(img)\n",
    "\n",
    "               #Add hair removal here (use toggle to activate?)\n",
    "                if hair_removal:\n",
    "                    img = hair_removal(img)\n",
    "\n",
    "                #Augment image\n",
    "                #if mod_toggle > 0:\n",
    "                    img = augment_image(img, is_training)\n",
    "\n",
    "                # Resize the image\n",
    "                img = cv2.resize(img, (imgSize, imgSize), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "                # Normalize the image\n",
    "                img = tf.constant(img / 255, dtype=tf.float32)\n",
    "\n",
    "                # Add processed data to lists\n",
    "                images.append(img)\n",
    "                metas.append(meta)\n",
    "                targets.append(target)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading image {img_name}: {e}\")\n",
    "                with open('image_errors.log', 'a') as f:\n",
    "                    f.write(f\"Error loading image {img_name}: {e}\\n\")\n",
    "                continue\n",
    "\n",
    "    # Convert lists to tensors\n",
    "    images_tensor = tf.stack(images, axis=0)\n",
    "    metas_tensor = tf.stack(metas, axis=0)\n",
    "    targets_tensor = tf.stack(targets, axis=0)\n",
    "\n",
    "    # Combine the images, metadata, and targets into a tf.data.Dataset\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(((images_tensor, metas_tensor), targets_tensor))\n",
    "\n",
    "    # Batch and prefetch the dataset\n",
    "    return dataset.batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load validation dataset into memory to speed up the model val_loss calc\n",
    "val_in_memory = make_dataset_in_memory(hdf5_file, val_meta_dict, val_pos_isic_id, val_pos_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8) CNN MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 - Model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simple CNN model using only images and target\n",
    "class CNN_model(tf.keras.Model):\n",
    "    def __init__(self, neurons = 8, activ = 'leaky_relu', img_size = 100, img_channels=3):\n",
    "        #Run the constructor of the parent class\n",
    "        super().__init__()\n",
    "\n",
    "        #Weight and bias initializers\n",
    "        kernel_initializer = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.05, seed=None)\n",
    "        bias_initializer = tf.keras.initializers.RandomUniform(minval=-0.05, maxval=0.05, seed=None)\n",
    "        \n",
    "        #Image size declaration\n",
    "        self.img_size = img_size\n",
    "        self.img_channels = img_channels\n",
    "\n",
    "        #Layers\n",
    "        self.conv1 = tf.keras.layers.Conv2D(filters=16, kernel_size=5, strides=(1, 1), activation='relu', padding='same', input_shape=(img_size, img_size, img_channels),\n",
    "                                            kernel_initializer=kernel_initializer, bias_initializer=bias_initializer)\n",
    "        self.pool1 = tf.keras.layers.MaxPool2D(pool_size=(2,2))\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.dense1 = tf.keras.layers.Dense(neurons, activation = activ, kernel_initializer=kernel_initializer, bias_initializer=bias_initializer)\n",
    "        self.dense2 = tf.keras.layers.Dense(1, activation='sigmoid', kernel_initializer=kernel_initializer, bias_initializer=bias_initializer)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x_image, x_meta = inputs\n",
    "\n",
    "        # Convolutions\n",
    "        x1 = self.conv1(x_image)\n",
    "        x1 = self.pool1(x1)\n",
    "\n",
    "        # Flattening of images for input layer\n",
    "        x1 = self.flatten(x1)\n",
    "\n",
    "        # Hidden layers of neural network\n",
    "        x1 = self.dense1(x1)\n",
    "\n",
    "        # Output layer of neural network\n",
    "        output = self.dense2(x1)\n",
    "\n",
    "        return output\n",
    "\n",
    "#Metadata Neural Network\n",
    "class Meta_model(tf.keras.Model):\n",
    "    def __init__(self, neurons = 8, activ = 'tanh'):\n",
    "        #Run the constructor of the parent class\n",
    "        super().__init__()\n",
    "\n",
    "        #Weight and bias initializers\n",
    "        kernel_initializer = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.05, seed=None)\n",
    "        bias_initializer = tf.keras.initializers.RandomUniform(minval=-0.05, maxval=0.05, seed=None)\n",
    "\n",
    "        #Layers\n",
    "        self.dense1 = tf.keras.layers.Dense(neurons, activation = activ, kernel_initializer=kernel_initializer, bias_initializer=bias_initializer)\n",
    "        self.dense2 = tf.keras.layers.Dense(neurons, activation = activ, kernel_initializer=kernel_initializer, bias_initializer=bias_initializer)\n",
    "        self.dense3 = tf.keras.layers.Dense(1, activation='sigmoid', kernel_initializer=kernel_initializer, bias_initializer=bias_initializer)\n",
    "        self.dropout = tf.keras.layers.Dropout(0.25)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        x_image, x_meta = inputs\n",
    "        x_all = tf.reshape(x_meta, (tf.shape(x_meta)[0], x_meta.shape[-1]))\n",
    "        # Neural Network\n",
    "        x_all = self.dense1(x_all)\n",
    "        x_all = self.dense2(x_all)\n",
    "        if training:\n",
    "            x_all = self.dropout(x_all, training=training)\n",
    "        output = self.dense3(x_all)\n",
    "        return output\n",
    "\n",
    "#Hybrid CNN model taking metadata\n",
    "class Hybrid_model(tf.keras.Model):\n",
    "    def __init__(self, neurons = 8, activ = 'leaky_relu', img_size = 100, img_channels = 3):\n",
    "        #Run the constructor of the parent class\n",
    "        super().__init__()\n",
    "\n",
    "        #Weight and bias initializers\n",
    "        kernel_initializer = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.05, seed=None)\n",
    "        bias_initializer = tf.keras.initializers.RandomUniform(minval=-0.05, maxval=0.05, seed=None)\n",
    "\n",
    "        #Image size declaration\n",
    "        self.img_size = img_size\n",
    "        self.img_channels = img_channels\n",
    "\n",
    "        #Layers\n",
    "        self.conv1 = tf.keras.layers.Conv2D(filters=32, kernel_size=5, strides=(1, 1), activation='relu', padding='same', input_shape=(img_size, img_size, img_channels),\n",
    "                                            kernel_initializer=kernel_initializer, bias_initializer=bias_initializer)\n",
    "        self.conv2 = tf.keras.layers.Conv2D(64, 5, activation='relu', kernel_initializer=kernel_initializer, bias_initializer=bias_initializer)\n",
    "        self.pool = tf.keras.layers.MaxPool2D(pool_size=(2,2))\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.dense1 = tf.keras.layers.Dense(neurons, activation = activ, kernel_initializer=kernel_initializer, bias_initializer=bias_initializer)\n",
    "        self.dropout1 = tf.keras.layers.Dropout(0.10)\n",
    "        self.dense2 = tf.keras.layers.Dense(neurons, activation = activ, kernel_initializer=kernel_initializer, bias_initializer=bias_initializer)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(0.10)\n",
    "        self.dense3 = tf.keras.layers.Dense(1, activation='sigmoid', kernel_initializer=kernel_initializer, bias_initializer=bias_initializer)\n",
    "        self.concatenate = keras.layers.Concatenate(axis=1)\n",
    "        \n",
    "    def call(self, inputs, training=False):\n",
    "        x_image, x_meta = inputs\n",
    "        # Convolutions\n",
    "        x = self.conv1(x_image)\n",
    "        x = self.pool(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.pool(x)\n",
    "        # Flattening of images and concatenation with other data\n",
    "        x = self.flatten(x)\n",
    "        # Reshape metadata to match dimensions\n",
    "        x_meta = tf.reshape(x_meta, (tf.shape(x_meta)[0], x_meta.shape[-1]))\n",
    "        x_all = self.concatenate([x, x_meta])\n",
    "        # Neural Network\n",
    "        x_all = self.dense1(x_all)\n",
    "        if training:\n",
    "            x_all = self.dropout1(x_all, training=training)\n",
    "        x_all = self.dense2(x_all)\n",
    "        if training:\n",
    "            x_all = self.dropout2(x_all, training=training)\n",
    "        output = self.dense3(x_all)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 - Model compiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\anaconda3\\envs\\env_DL_python3119\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "#Set seed\n",
    "tf.random.set_seed(71)\n",
    "\n",
    "#Initialize model\n",
    "#model = CNN_model(neurons=8, activ='tanh')\n",
    "model = Hybrid_model(neurons=36, activ='leaky_relu')\n",
    "#model = Meta_model(neurons=18, activ='tanh')\n",
    "\n",
    "#Define optimizer and loss function\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "loss = tf.keras.losses.BinaryCrossentropy(from_logits=False,\n",
    "                                          label_smoothing=0.0,\n",
    "                                          axis=-1,\n",
    "                                          reduction='sum_over_batch_size',\n",
    "                                          name='binary_crossentropy')\n",
    "\n",
    "#Compile the model with loss, optimizer, and metrics\n",
    "model.compile(loss = loss,\n",
    "              optimizer = optimizer,\n",
    "              metrics = [\n",
    "                  tf.keras.metrics.BinaryAccuracy(),\n",
    "                  tf.keras.metrics.FalseNegatives(),\n",
    "                  tf.keras.metrics.FalsePositives(),\n",
    "                  tf.keras.metrics.TrueNegatives(),\n",
    "                  tf.keras.metrics.TruePositives()\n",
    "                  ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3 - Model fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### FIXES TO MAKE:\n",
    "- With the weights function, maybe we should stop using the lists of tuples (train_ids, val_ids, test_ids)... The meta dictionaries are now the principal source of data (containing everything)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clear the memory leak in Keras\n",
    "class CustomCallback(tf.keras.callbacks.Callback):\n",
    "  def on_epoch_end(self, epoch, logs=None):\n",
    "    gc.collect()\n",
    "    #print(f\"Epoch {epoch+1} finished. Validation loss: {logs['val_loss']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training batches in dataset: 1803\n",
      "Total validate batches in dataset: 242\n"
     ]
    }
   ],
   "source": [
    "#Set batch sizes\n",
    "train_batch_size = 32\n",
    "val_batch_size = 32\n",
    "#Note: test batch size is declared in the test section (at the end) \n",
    "\n",
    "#Determine the number of batches (includes last incomplete batch)\n",
    "nb_training_batches = int(np.ceil(len(train_meta_dict)/train_batch_size))\n",
    "nb_validate_batches = int(np.ceil(len(val_meta_dict)/val_batch_size))\n",
    "\n",
    "#Print results\n",
    "print(\"Total training batches in dataset:\", nb_training_batches)\n",
    "print(\"Total validate batches in dataset:\", nb_validate_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57688"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_meta_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get weights for training\n",
    "weight_for_0, weight_for_1 = compute_class_weights(train_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1\n",
      "First training ID: ISIC_6321919\n",
      "\u001b[1m1803/1803\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m507s\u001b[0m 280ms/step - binary_accuracy: 0.7093 - false_negatives: 789.8182 - false_positives: 6422.7539 - loss: 1.1651 - true_negatives: 20045.2578 - true_positives: 1622.1436 - val_binary_accuracy: 0.8135 - val_false_negatives: 424.0000 - val_false_positives: 1016.0000 - val_loss: 0.4730 - val_true_negatives: 4109.0000 - val_true_positives: 2173.0000\n",
      "EPOCH 2\n",
      "First training ID: ISIC_6321919\n",
      "\u001b[1m1803/1803\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m492s\u001b[0m 272ms/step - binary_accuracy: 0.8146 - false_negatives: 562.2079 - false_positives: 4686.9482 - loss: 0.8806 - true_negatives: 21791.9824 - true_positives: 1838.8342 - val_binary_accuracy: 0.8362 - val_false_negatives: 795.0000 - val_false_positives: 470.0000 - val_loss: 0.4070 - val_true_negatives: 4655.0000 - val_true_positives: 1802.0000\n",
      "EPOCH 3\n",
      "First training ID: ISIC_6321919\n",
      "\u001b[1m1803/1803\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m465s\u001b[0m 257ms/step - binary_accuracy: 0.8280 - false_negatives: 553.8088 - false_positives: 4417.2280 - loss: 0.8553 - true_negatives: 22054.6211 - true_positives: 1854.3154 - val_binary_accuracy: 0.8315 - val_false_negatives: 435.0000 - val_false_positives: 866.0000 - val_loss: 0.4374 - val_true_negatives: 4259.0000 - val_true_positives: 2162.0000\n",
      "EPOCH 4\n",
      "First training ID: ISIC_6321919\n",
      "\u001b[1m1803/1803\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m465s\u001b[0m 257ms/step - binary_accuracy: 0.8330 - false_negatives: 537.7871 - false_positives: 4276.7646 - loss: 0.8370 - true_negatives: 22256.4316 - true_positives: 1808.9900 - val_binary_accuracy: 0.8500 - val_false_negatives: 630.0000 - val_false_positives: 528.0000 - val_loss: 0.3735 - val_true_negatives: 4597.0000 - val_true_positives: 1967.0000\n",
      "EPOCH 5\n",
      "First training ID: ISIC_6321919\n",
      "\u001b[1m1803/1803\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m476s\u001b[0m 264ms/step - binary_accuracy: 0.8229 - false_negatives: 545.9811 - false_positives: 4584.1797 - loss: 0.8176 - true_negatives: 21922.7285 - true_positives: 1827.0837 - val_binary_accuracy: 0.8451 - val_false_negatives: 621.0000 - val_false_positives: 575.0000 - val_loss: 0.3968 - val_true_negatives: 4550.0000 - val_true_positives: 1976.0000\n",
      "EPOCH 6\n",
      "First training ID: ISIC_6321919\n",
      "\u001b[1m1803/1803\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m471s\u001b[0m 261ms/step - binary_accuracy: 0.8286 - false_negatives: 517.8348 - false_positives: 4391.3657 - loss: 0.8051 - true_negatives: 22117.7793 - true_positives: 1852.9928 - val_binary_accuracy: 0.8530 - val_false_negatives: 581.0000 - val_false_positives: 554.0000 - val_loss: 0.3866 - val_true_negatives: 4571.0000 - val_true_positives: 2016.0000\n",
      "EPOCH 7\n",
      "First training ID: ISIC_6321919\n",
      "\u001b[1m1803/1803\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m471s\u001b[0m 260ms/step - binary_accuracy: 0.8289 - false_negatives: 518.6375 - false_positives: 4440.2544 - loss: 0.8064 - true_negatives: 22043.5781 - true_positives: 1877.5033 - val_binary_accuracy: 0.8081 - val_false_negatives: 366.0000 - val_false_positives: 1116.0000 - val_loss: 0.4540 - val_true_negatives: 4009.0000 - val_true_positives: 2231.0000\n",
      "EPOCH 8\n",
      "First training ID: ISIC_6321919\n",
      "\u001b[1m1803/1803\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m472s\u001b[0m 261ms/step - binary_accuracy: 0.8362 - false_negatives: 475.3625 - false_positives: 4312.6470 - loss: 0.7748 - true_negatives: 22189.3535 - true_positives: 1902.6104 - val_binary_accuracy: 0.8095 - val_false_negatives: 327.0000 - val_false_positives: 1144.0000 - val_loss: 0.4367 - val_true_negatives: 3981.0000 - val_true_positives: 2270.0000\n",
      "EPOCH 9\n",
      "First training ID: ISIC_6321919\n",
      "\u001b[1m1803/1803\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m488s\u001b[0m 271ms/step - binary_accuracy: 0.8303 - false_negatives: 478.0759 - false_positives: 4448.7310 - loss: 0.7892 - true_negatives: 22042.2441 - true_positives: 1910.9230 - val_binary_accuracy: 0.8328 - val_false_negatives: 355.0000 - val_false_positives: 936.0000 - val_loss: 0.4239 - val_true_negatives: 4189.0000 - val_true_positives: 2242.0000\n",
      "EPOCH 10\n",
      "First training ID: ISIC_6321919\n",
      "\u001b[1m1803/1803\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m473s\u001b[0m 262ms/step - binary_accuracy: 0.8332 - false_negatives: 444.2822 - false_positives: 4413.9097 - loss: 0.7609 - true_negatives: 22037.6348 - true_positives: 1984.1464 - val_binary_accuracy: 0.8302 - val_false_negatives: 338.0000 - val_false_positives: 973.0000 - val_loss: 0.4203 - val_true_negatives: 4152.0000 - val_true_positives: 2259.0000\n"
     ]
    }
   ],
   "source": [
    "#Run the model through epochs\n",
    "nb_epochs = 10\n",
    "early_break = False #End early in case of increasing validation loss\n",
    "\n",
    "for epoch in range(1, nb_epochs + 1):\n",
    "    #Make datasets\n",
    "    print(\"EPOCH\", epoch)\n",
    "    print(\"First training ID:\", train_meta_dict[0][1][0]) # correction [0][0]->[0][1][0]\n",
    "    shuffle_seed = 8 + epoch #Next initialization of datasets will have a different shuffle\n",
    "    train_dataset = make_dataset(hdf5_file, train_meta_dict, train_pos_isic_id, train_pos_target, batch_size = train_batch_size, is_training=True, shuffle_seed=shuffle_seed)\n",
    "    val_dataset = make_dataset(hdf5_file, val_meta_dict, val_pos_isic_id, val_pos_target, batch_size = val_batch_size, is_training=False, shuffle_seed=shuffle_seed)\n",
    "    \n",
    "    if save_val_in_memory:\n",
    "        mod = model.fit(train_dataset, epochs=1, steps_per_epoch = nb_training_batches, validation_data = val_in_memory, callbacks = [CustomCallback()],\n",
    "                        class_weight={0: weight_for_0, 1: weight_for_1})\n",
    "    else:\n",
    "        mod = model.fit(train_dataset, epochs=1, steps_per_epoch = nb_training_batches, validation_data = val_dataset, validation_steps = nb_validate_batches, callbacks = [CustomCallback()],\n",
    "                        class_weight={0: weight_for_0, 1: weight_for_1})\n",
    "    \n",
    "    #Save results\n",
    "    if epoch == 1:\n",
    "        results = mod.history\n",
    "    else:\n",
    "        for key in mod.history:   \n",
    "            results[key] += mod.history[key]\n",
    "\n",
    "    #Clean memory after use\n",
    "    del mod\n",
    "    del train_dataset\n",
    "    #If save_val_in_memory is not true, we have a generator. In this case, we want to delete the generator.\n",
    "    if save_val_in_memory != True:\n",
    "        del val_dataset\n",
    "    tf.keras.backend.clear_session()\n",
    "    gc.collect()\n",
    "\n",
    "    #Early termination (check after 15 epochs)\n",
    "    if epoch >= 15 and early_break == True:\n",
    "        #Calculate previous three changes, if positive, then loss is increasing\n",
    "        change1 = results[\"val_loss\"][-1] - results[\"val_loss\"][-2]\n",
    "        change2 = results[\"val_loss\"][-2] - results[\"val_loss\"][-3]\n",
    "        change3 = results[\"val_loss\"][-3] - results[\"val_loss\"][-4]\n",
    "\n",
    "        #Three consecutive increases in validation loss will stop the model\n",
    "        if change1 > 0 and change2 > 0 and change3 > 0:\n",
    "            break\n",
    "\n",
    "    #Save occasionally\n",
    "    #if (epoch % 25 == 0):\n",
    "    #    model.save(f\"XXXX\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<KerasVariable shape=(5, 5, 3, 32), dtype=float32, path=hybrid_model/conv2d/kernel>,\n",
       " <KerasVariable shape=(32,), dtype=float32, path=hybrid_model/conv2d/bias>,\n",
       " <KerasVariable shape=(5, 5, 32, 64), dtype=float32, path=hybrid_model/conv2d_1/kernel>,\n",
       " <KerasVariable shape=(64,), dtype=float32, path=hybrid_model/conv2d_1/bias>,\n",
       " <KerasVariable shape=(33885, 36), dtype=float32, path=hybrid_model/dense/kernel>,\n",
       " <KerasVariable shape=(36,), dtype=float32, path=hybrid_model/dense/bias>,\n",
       " <KerasVariable shape=(36, 36), dtype=float32, path=hybrid_model/dense_1/kernel>,\n",
       " <KerasVariable shape=(36,), dtype=float32, path=hybrid_model/dense_1/bias>,\n",
       " <KerasVariable shape=(36, 1), dtype=float32, path=hybrid_model/dense_2/kernel>,\n",
       " <KerasVariable shape=(1,), dtype=float32, path=hybrid_model/dense_2/bias>]"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 1.0)"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plot the training and validation losses\n",
    "\n",
    "#Convert loss results into a datafram\n",
    "result_preproc = pd.DataFrame({\n",
    "    'Epoch': [i+1 for i in range(len(results[\"loss\"]))], \n",
    "    'Train': results[\"loss\"],\n",
    "    'Validate': results[\"val_loss\"]\n",
    "    })\n",
    "\n",
    "# Convert dataframe from wide to long format\n",
    "df = pd.melt(result_preproc, ['Epoch'])\n",
    "\n",
    "#Make plot\n",
    "g = sns.lineplot(data=df, x='Epoch', y='value', hue='variable')\n",
    "g.set_title(\"Loss Curves\")\n",
    "g.legend_.set_title(\"Loss\")\n",
    "g.set_ylabel('Loss')\n",
    "g.set_ylim(0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9) Predict Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simple simple function to ititialize the test dataset using global variables\n",
    "def init_test_dataset():\n",
    "    return make_dataset(hdf5_file, test_meta_dict, test_pos_isic_id, test_pos_target, batch_size = test_batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TEST BATCHES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntest_dataset = init_test_dataset() #Initialize the dataset\\n\\nfor i, batch in enumerate(test_dataset):\\n    (img_batch, meta_batch), target_batch = batch\\n    \\n    # Print the shapes of the current batch\\n    print(f\"Batch {i+1}:\")\\n    print(\"  Image Batch Shape:\", img_batch.shape)\\n    print(\"  Metadata Batch Shape:\", meta_batch.shape)\\n    print(\"  Target Batch Shape:\", target_batch.shape)\\n'"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "test_dataset = init_test_dataset() #Initialize the dataset\n",
    "\n",
    "for i, batch in enumerate(test_dataset):\n",
    "    (img_batch, meta_batch), target_batch = batch\n",
    "    \n",
    "    # Print the shapes of the current batch\n",
    "    print(f\"Batch {i+1}:\")\n",
    "    print(\"  Image Batch Shape:\", img_batch.shape)\n",
    "    print(\"  Metadata Batch Shape:\", meta_batch.shape)\n",
    "    print(\"  Target Batch Shape:\", target_batch.shape)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total test batches in dataset: 618\n"
     ]
    }
   ],
   "source": [
    "#Test dataset basic size information: nb of samples and batch size\n",
    "test_batch_size = 32\n",
    "nb_test_batches = int(np.ceil(len(test_meta_dict)/test_batch_size))\n",
    "print(\"Total test batches in dataset:\", nb_test_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retrieve real values for the target in the test dataset\n",
    "y_test = []\n",
    "test_dataset = init_test_dataset()\n",
    "for item in test_dataset.take(nb_test_batches):\n",
    "    img_meta, targ = item\n",
    "    y_test.extend(targ.numpy().flatten())\n",
    "#Convert to numpy array (mathematical operations are faster)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PREDICTIONS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 15 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000028DC48B4220> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m618/618\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 169ms/step\n",
      "Shape of prediction data: (19763, 1)\n"
     ]
    }
   ],
   "source": [
    "#Reinitialize the test dataset (necessary to start at beginning)\n",
    "test_dataset = init_test_dataset()\n",
    "#Retrieve predictions\n",
    "predictions = model.predict(test_dataset, steps = nb_test_batches)\n",
    "#Put predictionsin a numpy array\n",
    "y_pred = np.array([round(i) for i  in predictions.flatten()])\n",
    "print(\"Shape of prediction data:\", predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the loss\n",
    "loss = sum(abs(y_test - y_pred))/len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Determine true/false positives and negatives\n",
    "pos_indices = y_test == 1\n",
    "neg_indices = y_test == 0\n",
    "\n",
    "#True positives\n",
    "true_pos = sum(abs(y_test[pos_indices] == y_pred[pos_indices]))\n",
    "\n",
    "#False negatives\n",
    "false_neg = sum(abs(y_test[pos_indices] != y_pred[pos_indices]))\n",
    "\n",
    "#True negatives\n",
    "true_neg = sum(abs(y_test[neg_indices] == y_pred[neg_indices]))\n",
    "\n",
    "#False positives\n",
    "false_pos = sum(abs(y_test[neg_indices] != y_pred[neg_indices]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---TEST RESULTS---\n",
      "Loss on test data: 0.19723726154935992\n",
      "True positives: 19\n",
      "False positives: 3897\n",
      "True negatives: 15846\n",
      "False negatives: 1\n"
     ]
    }
   ],
   "source": [
    "print(\"---TEST RESULTS---\")\n",
    "print(\"Loss on test data:\", loss)\n",
    "print(\"True positives:\", true_pos)\n",
    "print(\"False positives:\", false_pos)\n",
    "print(\"True negatives:\", true_neg)\n",
    "print(\"False negatives:\", false_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

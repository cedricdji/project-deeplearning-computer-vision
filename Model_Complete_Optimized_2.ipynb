{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL - IMAGE LOADING & NEURAL NETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#REQUIRED FOR AWS SAGEMAKER INSTANCE\n",
    "#pip install setuptools==58.0.4\n",
    "#pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: C:\\ProgramData\\sagemaker\\sagemaker\\config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: C:\\Users\\Andrew\\AppData\\Local\\sagemaker\\sagemaker\\config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Andrew\\anaconda3\\envs\\cancer_test2\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#Import libraries\n",
    "import boto3\n",
    "import sagemaker\n",
    "import fsspec\n",
    "import gc\n",
    "import csv\n",
    "import os\n",
    "import io\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_recall_curve, roc_auc_score, auc, make_scorer, roc_curve\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import keras_cv\n",
    "import random\n",
    "from collections.abc import Generator\n",
    "import itertools\n",
    "import datetime\n",
    "\n",
    "TF_ENABLE_ONEDNN_OPTS=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) HYPERPARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AWS SageMaker with S3 bucket\n",
    "use_sagemaker=True                    #Sagemake and S3 bucket configurations activated if this is set to True\n",
    "\n",
    "#Fraction of data to use (CEDRIC can reduce to speed up tests... go to 0.02!)\n",
    "train_frac_to_use = 1           #Reduce training data to this fraction. Set to \"1\" to use all data.\n",
    "val_frac_to_use = 1             #Reduce validation data to this fraction. Set to \"1\" to use all data.\n",
    "test_frac_to_use = 1            #Reduce validation data to this fraction. Set to \"1\" to use all data.\n",
    "\n",
    "#Memory management (Speeds up model. Need 16GB of RAM for validation data and likely 64GB of RAM for training data)\n",
    "save_train_in_memory = False    #Place all training data directly in memory to accelerate the validation step in the model\n",
    "save_val_in_memory = False      #Place all validation data directly in memory to accelerate the validation step in the model\n",
    "\n",
    "#Saves management (weights only, all other saves are performed by default)\n",
    "wt_save_freq = 1                #Save model weights every X epochs. Set to 0 for no saving during epochs.\n",
    "final_wt_save = True            #Save weights after all epochs have run\n",
    "\n",
    "#Splitting of train-validate-test\n",
    "split_seed = 88                 #Seed to use for all train-validate-test splits, including the split of reserved Target=1 data\n",
    "reserve_frac = 0.1              #Fraction of total original data of Target = 1 (reserved for use in validation data)\n",
    "test_frac = 0.2                 #Fraction of total original data, excluding the reserved fraction, to use as the test data\n",
    "nb_of_augments = 100            #Number of augments to perform on Target = 1 images in train-validate sets\n",
    "val_frac = 0.33                 #Fraction of augmented train-validate list to use as the validation data. The rest becomes the training data.\n",
    "nb_of_augments_reserved = 15    #Number of augmentations to perform on reserved validation fraction (Target = 1). Note: this is added to the validation data.\n",
    "reduce_frac = 0.8               #Fraction of Target = 0 samples to remove from the validation data (improves balance)\n",
    "\n",
    "#Image resizing: all images are adjusted to this size so the the CNN receives same number of data points each time\n",
    "imgSize = 100                   #Resize all images to this standard size (all images sent to CNN must have same number of pixels)\n",
    "\n",
    "#Hair removal (applies to all images)\n",
    "apply_hair_removal = False      #Apply the hair removing algorithm to all the photos\n",
    "\n",
    "#Batch sizes\n",
    "train_batch_size = 32           #Batch size for training dataset\n",
    "val_batch_size = 32             #Batch size for validation dataset\n",
    "test_batch_size = 32            #Batch size for test dataset\n",
    "\n",
    "#Choice of model\n",
    "#0 = in series (convolutions on images followed by concatenation with metadata followed by deep NN layers, followed by final layer)\n",
    "#1 = in parallel (metadata run separately in deep NN layers, then concatenated with results from image convolutions, followed by final layer)\n",
    "model_choice = 0\n",
    "\n",
    "#Neural Network Parameters\n",
    "nb_neurons_hidden_layers = 36   #Number of neurons in each hidden layer\n",
    "dropout = 0.1                   #Fraction of neurons to drop\n",
    "\n",
    "#Optimizer Parameters\n",
    "learning_rate = 0.005           #Initial learning rate for the Adam optimizer\n",
    "\n",
    "#Epoch management (CEDRIC... reduce number of epochs if needed)\n",
    "nb_epochs = 20                  #Number of epoch to cycle through with the CNN\n",
    "early_break = False             #End early in case of increasing validation loss\n",
    "\n",
    "#Debugging\n",
    "cheat = False                   #Add the target to the metadata so the model can precisely learn the correct response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) IMPORT DATA & DECLARE SAVES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 - Declare file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA PATH: C:/Users/Andrew/Downloads/isic-2024-challenge/\n",
      "SAVE PATH: C:/Users/Andrew/Projects/deep_learning_saves/\n",
      "HDF5 PATH: C:/Users/Andrew/Downloads/isic-2024-challenge/train-image.hdf5\n"
     ]
    }
   ],
   "source": [
    "#IMPORT FILES\n",
    "#Directory for data files - FULL DATA\n",
    "if use_sagemaker:\n",
    "    dataPath = \"s3://images-projet-deep-learning-01/\" #on sagemaker\n",
    "else:\n",
    "    dataPath = \"C:/Users/Andrew/Downloads/isic-2024-challenge/\" #on personal machine\n",
    "#Metadata file paths\n",
    "metaPath = dataPath + \"cleaned_metadata.csv\"\n",
    "#Image file path\n",
    "hdf5_key = 'train-image.hdf5'\n",
    "hdf5_file = dataPath + hdf5_key\n",
    "\n",
    "\n",
    "#SAVE FILES\n",
    "#Directory for saved files\n",
    "if use_sagemaker:\n",
    "    folder = '/results/' #create it in the instance\n",
    "    savePath = os.getcwd() + folder # on sagemaker\n",
    "else:\n",
    "    savePath = \"C:/Users/Andrew/Projects/deep_learning_saves/\" #on personal machine\n",
    "#Model results\n",
    "modelResPath = savePath + \"model_results_save.csv\"\n",
    "#Test ids (isic_ids + target + mod_toggle)\n",
    "test_ids_Path = savePath + \"test_ids.csv\"\n",
    "#Test results (y_test, y_pred)\n",
    "testResPath = savePath + \"test_results_save.csv\"\n",
    "\n",
    "print(\"DATA PATH:\", dataPath)\n",
    "print(\"SAVE PATH:\", savePath)\n",
    "print(\"HDF5 PATH:\", hdf5_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 - AWS Sagemaker and S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HDF5 PATH: C:/Users/Andrew/Downloads/isic-2024-challenge/train-image.hdf5\n"
     ]
    }
   ],
   "source": [
    "if use_sagemaker:\n",
    "    # Configuration S3 and local path\n",
    "    session = sagemaker.Session()\n",
    "    bucket = 'images-projet-deep-learning-01'\n",
    "    local_hdf5_path = os.path.join(os.getcwd(), 'train-image.hdf5')\n",
    "\n",
    "    # Check if the file already exists\n",
    "    if os.path.exists(local_hdf5_path):\n",
    "        print(f\"The file {local_hdf5_path} already exists. Download skipped.\")\n",
    "    else:\n",
    "        # Download the HDF5 file from S3\n",
    "        print(f\"Download of {hdf5_key} from S3 to {local_hdf5_path}...\")\n",
    "        s3 = boto3.client('s3')\n",
    "        s3.download_file(bucket, hdf5_key, local_hdf5_path)\n",
    "        print(\"Download finished.\")\n",
    "    #Update the hdf5 file path with the local one\n",
    "    hdf5_file = local_hdf5_path\n",
    "print(\"HDF5 PATH:\", hdf5_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_sagemaker:\n",
    "    # Configuration of S3 bucket and boto3 client\n",
    "    bucket = 'dsti-a23-deep-learning-outputs-01'\n",
    "    s3 = boto3.client('s3')\n",
    "\n",
    "    # Local path to temporarily save the weights files\n",
    "    localsavePath = os.getcwd() + '/hd5_files'\n",
    "    os.makedirs(localsavePath, exist_ok=True)  # Create the folder if it doesn't exist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 - Load metadata from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import metadata from file\n",
    "metadata = pd.read_csv(metaPath, sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- X_meta NA counts --\n",
      "isic_id                         0\n",
      "target                          0\n",
      "clin_size_long_diam_mm          0\n",
      "tbp_lv_A                        0\n",
      "tbp_lv_Aext                     0\n",
      "tbp_lv_B                        0\n",
      "tbp_lv_H                        0\n",
      "tbp_lv_Hext                     0\n",
      "tbp_lv_areaMM2                  0\n",
      "tbp_lv_area_perim_ratio         0\n",
      "tbp_lv_color_std_mean           0\n",
      "tbp_lv_deltaB                   0\n",
      "tbp_lv_deltaLBnorm              0\n",
      "tbp_lv_minorAxisMM              0\n",
      "tbp_lv_norm_color               0\n",
      "tbp_lv_perimeterMM              0\n",
      "tbp_lv_radial_color_std_max     0\n",
      "tbp_lv_stdLExt                  0\n",
      "tbp_lv_y                        0\n",
      "tbp_lv_dnn_lesion_confidence    0\n",
      "normalized_contrast             0\n",
      "size_to_area_ratio              0\n",
      "luminance_ratio                 0\n",
      "chroma_B_ratio                  0\n",
      "category_Head & Neck            0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Verify that there are no NAs\n",
    "print(\"-- X_meta NA counts --\")\n",
    "print(metadata.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Activate for debugging of the predict function\n",
    "if cheat:\n",
    "    metadata[\"target_cheat\"] = metadata[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the total number of features in the metadata (subtract 'isic_id' and 'target')\n",
    "num_features = len(metadata.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Train, Validate, Test Split + Preparation of Data Augmentation\n",
    "1. Make two separate lists of isic_ids for target=0 and target=1. Transform into tuples (isic_id, target, mod toggle). Base data has mod toggle = 0, meaning no adjustment will be made.\n",
    "2. Reserve 10% of target = 1 for validate\n",
    "3. Split into train-validate & test on both lists (0 and 1). Take the test lists (0 and 1), concatenate and shuffle them\n",
    "4. Create augmentation preparation function for target = 1: mod toggle = strictly positive integer (this adds more isic_ids to the list, with mod toggle non zero))\n",
    "5. Run augmentation preparation function on train-validate and the reserved validation data: mod toggle = strictly positive integer\n",
    "6. Split train-validate on both lists (0 and 1)\n",
    "7. Reduce the validation data on target = 0 by value specified in reduce_frac\n",
    "8. Concatenate and shuffle the train lists and validation lists\n",
    "9. Limit training and validation data to speed up training (take only fraction of prepared lists)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 - Make two separate lists of isic_ids for target=0 (mod_toggle = -1) and target=1 (mod_toggle = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total ids with target = 0: 381533\n",
      "Total ids with target = 1: 381\n"
     ]
    }
   ],
   "source": [
    "#Make a list of isic_ids for each target value (0 and 1)\n",
    "isic_id_target_0 = metadata[metadata['target'] == 0]['isic_id'].tolist()\n",
    "isic_id_target_1 = metadata[metadata['target'] == 1]['isic_id'].tolist()\n",
    "\n",
    "#Retrieve dataframe with isic id and target\n",
    "temp_0 = metadata[metadata[\"isic_id\"].isin(isic_id_target_0)].loc[:,[\"isic_id\",\"target\"]]\n",
    "temp_1 = metadata[metadata[\"isic_id\"].isin(isic_id_target_1)].loc[:,[\"isic_id\",\"target\"]]\n",
    "\n",
    "#Convert into list of tuples... this makes it compatible with data augmentations\n",
    "#Form: (isic_id, target, mod toggle)\n",
    "isic_id_target_0 = list(zip(temp_0.iloc[:,0], temp_0.iloc[:,1], [-1]*len(temp_0)))\n",
    "isic_id_target_1 = list(zip(temp_1.iloc[:,0], temp_1.iloc[:,1], [0]*len(temp_1)))\n",
    "\n",
    "#Delete temporary dataframes (the original metadata dataframe is untouched)\n",
    "del temp_0\n",
    "del temp_1\n",
    "\n",
    "#Count the number of occurrences for each target value\n",
    "print(\"Total ids with target = 0:\", len(isic_id_target_0))\n",
    "print(\"Total ids with target = 1:\", len(isic_id_target_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 - Reserve 10% of target = 1 for validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total ids with target = 0: 381533\n",
      "Total ids with target = 1: 342\n",
      "Total reserved target = 1: 39\n"
     ]
    }
   ],
   "source": [
    "#Keep 10% of isic_Id of target=1 without duplication\n",
    "isic_id_target_1, isic_id_target_1_reserved = train_test_split(isic_id_target_1, test_size = reserve_frac, random_state=split_seed, shuffle=False)\n",
    "\n",
    "#Count the number of occurrences for each target value (AFTER RESERVATION)\n",
    "print(\"Total ids with target = 0:\", len(isic_id_target_0))\n",
    "print(\"Total ids with target = 1:\", len(isic_id_target_1))\n",
    "print(\"Total reserved target = 1:\", len(isic_id_target_1_reserved))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 - Split into train-validate & test on both lists (0 and 1). Take the test lists (0 and 1), concatenate and shuffle them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split out the test ids\n",
    "trainval_0, test_0 = train_test_split(isic_id_target_0, test_size = test_frac, random_state=split_seed, shuffle=True)\n",
    "trainval_1, test_1 = train_test_split(isic_id_target_1, test_size = test_frac, random_state=split_seed, shuffle=True)\n",
    "\n",
    "test_ids = test_0 + test_1\n",
    "np.random.shuffle(test_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 - Create augmentation preparation function for target = 1: mod toggle = strictly positive integer\n",
    "(this adds more isic_ids to the list, with mod toggle non zero))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate a list containing augmentation toggles. Apply only to training and validation sets.\n",
    "def augment_prep(tuple_list, nb_of_augments = 30, shuffle_seed=None):\n",
    "    augment_list = []\n",
    "    \n",
    "    #If augmentation is desired, then the mod toggle is a strictly positive integer\n",
    "    augment_list = [(item[0], item[1], i) for item in tuple_list for i in range(1, nb_of_augments + 1)]\n",
    "    \n",
    "    #Shuffle the list\n",
    "    augment_list.extend(tuple_list)\n",
    "    np.random.seed(shuffle_seed)\n",
    "    np.random.shuffle(augment_list)\n",
    "\n",
    "    return augment_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 - Run augmentation preparation function on train-validate and the reserved validation data: mod toggle = strictly positive integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Augment the training and validation list\n",
    "trainval_1 = augment_prep(trainval_1, nb_of_augments=nb_of_augments, shuffle_seed=50)\n",
    "\n",
    "#Duplicate the reserved training data\n",
    "reserved_1 = augment_prep(isic_id_target_1_reserved, nb_of_augments=nb_of_augments_reserved, shuffle_seed=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 - Split train-validate on both lists (0 and 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split training and validation lists\n",
    "train_0, val_0 = train_test_split(trainval_0, test_size = val_frac, random_state=split_seed, shuffle=True)\n",
    "train_1, val_1 = train_test_split(trainval_1, test_size = val_frac, random_state=split_seed, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7 - Reduce the validation data on target = 0 by value specified in reduce_frac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reduce the validation data of type Target = 0\n",
    "nb_samples = int((1 - reduce_frac) * len(val_0))\n",
    "val_0 = random.sample(val_0, nb_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.8 - Concatenate and shuffle the train and validation lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concatenate\n",
    "train_ids=train_0.copy()\n",
    "train_ids.extend(train_1)\n",
    "val_ids=list(itertools.chain(val_0, val_1, reserved_1))\n",
    "\n",
    "#Shuffle\n",
    "np.random.seed(60)\n",
    "np.random.shuffle(train_ids)\n",
    "np.random.shuffle(val_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Validate/Test Counts: 222974 / 29868 / 76376\n",
      "Train/Validate/Test Fractions: 0.68 / 0.09 / 0.23\n",
      "Proportion of Target = 1 in training data: 0.08284822445666311\n",
      "Proportion of Target = 1 in validation data: 0.3255658229543324\n",
      "Proportion of Target = 1 in test data: 0.0009034251597360428\n"
     ]
    }
   ],
   "source": [
    "#Calculate the proportaion of Target=1 in each set (training, validation, test)\n",
    "def calc_frac_target1(ids):\n",
    "    return sum([item[1] for item in ids]) / len(ids)\n",
    "\n",
    "tot_samples = len(train_ids) + len(val_ids) + len(test_ids)\n",
    "\n",
    "print(\"Train/Validate/Test Counts:\", len(train_ids), \"/\", len(val_ids), \"/\", len(test_ids))\n",
    "print(\"Train/Validate/Test Fractions:\", round(len(train_ids)/tot_samples,2), \"/\", round(len(val_ids)/tot_samples,2), \"/\", round(len(test_ids)/tot_samples,2))\n",
    "print(\"Proportion of Target = 1 in training data:\", calc_frac_target1(train_ids))\n",
    "print(\"Proportion of Target = 1 in validation data:\", calc_frac_target1(val_ids))\n",
    "print(\"Proportion of Target = 1 in test data:\", calc_frac_target1(test_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.9 - Limit training and validation data to speed up training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train ids length before: 222974\n",
      "Train ids length after: 2229\n",
      "Validate ids length before: 29868\n",
      "Validate ids length after: 298\n",
      "Test ids length before: 76376\n",
      "Test ids length after: 7637\n"
     ]
    }
   ],
   "source": [
    "#Choose a portion of TRAINING ids to load into memory\n",
    "def take_fewer_samples(ids, frac_to_use, seed):\n",
    "    if frac_to_use < 1 and frac_to_use > 0:\n",
    "        random.seed(seed)\n",
    "        k = int(frac_to_use * len(ids))\n",
    "        ids_short = random.choices(ids, k=k)\n",
    "        return ids_short\n",
    "    return ids\n",
    "\n",
    "print(\"Train ids length before:\", len(train_ids))\n",
    "train_ids = take_fewer_samples(train_ids, train_frac_to_use, seed=12)\n",
    "print(\"Train ids length after:\", len(train_ids))\n",
    "\n",
    "print(\"Validate ids length before:\", len(val_ids))\n",
    "val_ids = take_fewer_samples(val_ids, val_frac_to_use, seed=12)\n",
    "print(\"Validate ids length after:\", len(val_ids))\n",
    "\n",
    "print(\"Test ids length before:\", len(test_ids))\n",
    "test_ids = take_fewer_samples(test_ids, test_frac_to_use, seed=12)\n",
    "print(\"Test ids length after:\", len(test_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Augmentation functions (used during dataset creation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hair_removal(image, crop_pixels=10):\n",
    "    height_pixels = len(image)  # Image rows\n",
    "    width_pixels = len(image[0])  # Image columns\n",
    "\n",
    "    # Image cropping\n",
    "    height = [crop_pixels, height_pixels - crop_pixels]\n",
    "    width = [crop_pixels, width_pixels - crop_pixels]\n",
    "    img = image[height[0]:height[1], width[0]:width[1]]\n",
    "\n",
    "    # Gray scale\n",
    "    grayScale = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    # Black hat filter\n",
    "    kernel = cv2.getStructuringElement(1, (9, 9))\n",
    "    blackhat = cv2.morphologyEx(grayScale, cv2.MORPH_BLACKHAT, kernel)\n",
    "    # Gaussian filter\n",
    "    bhg = cv2.GaussianBlur(blackhat, (3, 3), cv2.BORDER_DEFAULT)\n",
    "    # Binary thresholding (MASK)\n",
    "    ret, mask = cv2.threshold(bhg, 10, 255, cv2.THRESH_BINARY)\n",
    "    # Replace pixels of the mask\n",
    "    dst = cv2.inpaint(img, mask, 6, cv2.INPAINT_TELEA)\n",
    "\n",
    "    return dst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the augmentation function\n",
    "def augment_image(image, mod_toggle, is_training=False):\n",
    "    \"\"\"\n",
    "    Apply a series of augmentations to create diverse variations of the input image.\n",
    "    Includes random flips, rotations, brightness adjustments, and other transformations.\n",
    "    \"\"\"\n",
    "\n",
    "    # Parameters\n",
    "    if is_training==True:\n",
    "        #Cutout values\n",
    "        height_factor_cut=(0.02, 0.06)\n",
    "        width_factor_cut=(0.02, 0.06)\n",
    "        random_cutout = keras_cv.layers.RandomCutout(height_factor=height_factor_cut, width_factor=width_factor_cut)\n",
    "        #Brightness values\n",
    "        min_delta_bright=0.05\n",
    "        max_delta_bright=0.25\n",
    "        #Saturation values\n",
    "        weak_sat_low=0.7\n",
    "        weak_sat_high=0.95\n",
    "        strong_sat_low=1.05\n",
    "        strong_sat_high=1.8\n",
    "        #Contrast values\n",
    "        weak_cont_low=0.7\n",
    "        weak_cont_high=0.95\n",
    "        strong_cont_low=1.05\n",
    "        strong_cont_high=1.8\n",
    "\n",
    "    else:\n",
    "        #Brightness values\n",
    "        min_delta_bright=0.05\n",
    "        max_delta_bright=0.15\n",
    "        #Saturation values\n",
    "        weak_sat_low=0.8\n",
    "        weak_sat_high=0.95\n",
    "        strong_sat_low=1.05\n",
    "        strong_sat_high=1.2\n",
    "        #Contrast values\n",
    "        weak_cont_low=0.8\n",
    "        weak_cont_high=0.95\n",
    "        strong_cont_low=1.05\n",
    "        strong_cont_high=1.2\n",
    "\n",
    "    #List of base augmentations\n",
    "    base_augments = [\n",
    "        lambda img: tf.image.flip_left_right(img),\n",
    "        lambda img: tf.image.flip_up_down(img),\n",
    "        lambda img: tf.image.rot90(img, k=1),\n",
    "        lambda img: tf.image.rot90(img, k=2),\n",
    "        lambda img: tf.image.rot90(img, k=3),\n",
    "        lambda img: random_cutout(img)\n",
    "    ]\n",
    "\n",
    "    #List of other augmentations that can be performed multiple times\n",
    "    other_augments = [\n",
    "        lambda img: tf.image.adjust_brightness(img, -random.uniform(min_delta_bright, max_delta_bright)),\n",
    "        lambda img: tf.image.adjust_brightness(img, random.uniform(min_delta_bright, max_delta_bright)),\n",
    "        lambda img: tf.image.random_contrast(img, lower=weak_cont_low, upper=weak_cont_high),\n",
    "        lambda img: tf.image.random_contrast(img, lower=strong_cont_low, upper=strong_cont_high),\n",
    "        lambda img: tf.image.random_saturation(img, lower=weak_sat_low, upper=weak_sat_high),\n",
    "        lambda img: tf.image.random_saturation(img, lower=strong_sat_low, upper=strong_sat_high)\n",
    "    ]\n",
    "\n",
    "    #Select augmentations to use based on whether it is training data or not\n",
    "    if is_training:\n",
    "        base_augments = base_augments\n",
    "        other_augments = other_augments \n",
    "    else:\n",
    "        base_selection = [0,1]  #Positions of the augmentations to use in the base_augments list\n",
    "        base_augments = [base_augments[i] for i in base_selection]\n",
    "        other_augments = other_augments\n",
    "\n",
    "    #Engage the augment based on the mod_toggle. The base_augments are always done first.\n",
    "    nb_base_aug = len(base_augments)\n",
    "    \n",
    "    if mod_toggle <= nb_base_aug and mod_toggle > 0:\n",
    "        augmentation = base_augments[mod_toggle - 1] #Mod toggles start at 1\n",
    "        image = augmentation(image)\n",
    "    else:\n",
    "        augmentation = random.choice(other_augments)\n",
    "        image = augmentation(image)\n",
    "        #Apply a second transformation with a certain probability\n",
    "        if tf.random.uniform([]) < 0.85:\n",
    "            augmentation2 = random.choice(base_augments)\n",
    "            image = augmentation2(image)\n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nid_try = \"ISIC_5675460\"\\nwith h5py.File(hdf5_file, \\'r\\') as h5file:\\n    img_try = np.array(Image.open(io.BytesIO(h5file[id_try][()])))\\n\\nfor mod_toggle in range(500):\\n    img = hair_removal(img_try)\\n    img = cv2.resize(img, (100, 100), interpolation= cv2.INTER_AREA)\\n    augment_image(img, mod_toggle, is_training=True)\\n'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TESTING OF AUGMENT_IMAGE FUNCTION\n",
    "\"\"\"\n",
    "id_try = \"ISIC_5675460\"\n",
    "with h5py.File(hdf5_file, 'r') as h5file:\n",
    "    img_try = np.array(Image.open(io.BytesIO(h5file[id_try][()])))\n",
    "\n",
    "for mod_toggle in range(500):\n",
    "    img = hair_removal(img_try)\n",
    "    img = cv2.resize(img, (100, 100), interpolation= cv2.INTER_AREA)\n",
    "    augment_image(img, mod_toggle, is_training=True)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nid_try = \"ISIC_5675460\"\\nwith h5py.File(hdf5_file, \\'r\\') as h5file:\\n    img_try = np.array(Image.open(io.BytesIO(h5file[id_try][()])))\\n\\nheight_factor_cut=(0.02, 0.06)\\nwidth_factor_cut=(0.02, 0.06)\\nrandom_cutout = keras_cv.layers.RandomCutout(height_factor=height_factor_cut, width_factor=width_factor_cut)\\n\\nbase_augments = [\\n    lambda img: img,\\n    lambda img: tf.image.flip_left_right(img),\\n    lambda img: tf.image.flip_up_down(img),\\n    lambda img: tf.image.rot90(img, k=1),\\n    lambda img: tf.image.rot90(img, k=2),\\n    lambda img: tf.image.rot90(img, k=3),\\n    lambda img: random_cutout(img).numpy().astype(int),\\n]\\n\\nimages = []\\nfor augmentation in base_augments:\\n    image = augmentation(img_try)\\n    images.append(image)\\n\\n#Show the images\\nfor image in images:\\n    plt.imshow(image, interpolation=None)\\n    plt.grid(None)\\n    plt.show()\\n'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TESTING OF BASE AUGMENTS\n",
    "\"\"\"\n",
    "id_try = \"ISIC_5675460\"\n",
    "with h5py.File(hdf5_file, 'r') as h5file:\n",
    "    img_try = np.array(Image.open(io.BytesIO(h5file[id_try][()])))\n",
    "\n",
    "height_factor_cut=(0.02, 0.06)\n",
    "width_factor_cut=(0.02, 0.06)\n",
    "random_cutout = keras_cv.layers.RandomCutout(height_factor=height_factor_cut, width_factor=width_factor_cut)\n",
    "\n",
    "base_augments = [\n",
    "    lambda img: img,\n",
    "    lambda img: tf.image.flip_left_right(img),\n",
    "    lambda img: tf.image.flip_up_down(img),\n",
    "    lambda img: tf.image.rot90(img, k=1),\n",
    "    lambda img: tf.image.rot90(img, k=2),\n",
    "    lambda img: tf.image.rot90(img, k=3),\n",
    "    lambda img: random_cutout(img).numpy().astype(int),\n",
    "]\n",
    "\n",
    "images = []\n",
    "for augmentation in base_augments:\n",
    "    image = augmentation(img_try)\n",
    "    images.append(image)\n",
    "\n",
    "#Show the images\n",
    "for image in images:\n",
    "    plt.imshow(image, interpolation=None)\n",
    "    plt.grid(None)\n",
    "    plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nid_try = \"ISIC_5675460\"\\nwith h5py.File(hdf5_file, \\'r\\') as h5file:\\n    img_try = np.array(Image.open(io.BytesIO(h5file[id_try][()])))\\n\\n#Brightness values\\nmin_delta_bright=0.05\\nmax_delta_bright=0.25\\n#Saturation values\\nweak_sat_low=0.7\\nweak_sat_high=0.95\\nstrong_sat_low=1.05\\nstrong_sat_high=1.8\\n#Contrast values\\nweak_cont_low=0.7\\nweak_cont_high=0.95\\nstrong_cont_low=1.05\\nstrong_cont_high=1.8\\n\\n#List of other augmentations that can be performed multiple times\\nother_augments = [\\n    lambda img: tf.image.adjust_brightness(img, -random.uniform(min_delta_bright, max_delta_bright)),\\n    lambda img: tf.image.adjust_brightness(img, random.uniform(min_delta_bright, max_delta_bright)),\\n    lambda img: tf.image.random_contrast(img, lower=weak_cont_low, upper=weak_cont_high),\\n    lambda img: tf.image.random_contrast(img, lower=strong_cont_low, upper=strong_cont_high),\\n    lambda img: tf.image.random_saturation(img, lower=weak_sat_low, upper=weak_sat_high),\\n    lambda img: tf.image.random_saturation(img, lower=strong_sat_low, upper=strong_sat_high)\\n]\\n\\nimages = []\\nfor augmentation in other_augments:\\n    image = augmentation(img_try)\\n    if tf.random.uniform([]) < 1:\\n        augmentation2 = random.choice(base_augments)\\n        image = augmentation2(image)\\n    images.append(image)\\n\\n#Show the images\\nfor image in images:\\n    plt.imshow(image, interpolation=None)\\n    plt.grid(None)\\n    plt.show()\\n'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TESTING OF OTHER AUGMENTS\n",
    "\"\"\"\n",
    "id_try = \"ISIC_5675460\"\n",
    "with h5py.File(hdf5_file, 'r') as h5file:\n",
    "    img_try = np.array(Image.open(io.BytesIO(h5file[id_try][()])))\n",
    "\n",
    "#Brightness values\n",
    "min_delta_bright=0.05\n",
    "max_delta_bright=0.25\n",
    "#Saturation values\n",
    "weak_sat_low=0.7\n",
    "weak_sat_high=0.95\n",
    "strong_sat_low=1.05\n",
    "strong_sat_high=1.8\n",
    "#Contrast values\n",
    "weak_cont_low=0.7\n",
    "weak_cont_high=0.95\n",
    "strong_cont_low=1.05\n",
    "strong_cont_high=1.8\n",
    "\n",
    "#List of other augmentations that can be performed multiple times\n",
    "other_augments = [\n",
    "    lambda img: tf.image.adjust_brightness(img, -random.uniform(min_delta_bright, max_delta_bright)),\n",
    "    lambda img: tf.image.adjust_brightness(img, random.uniform(min_delta_bright, max_delta_bright)),\n",
    "    lambda img: tf.image.random_contrast(img, lower=weak_cont_low, upper=weak_cont_high),\n",
    "    lambda img: tf.image.random_contrast(img, lower=strong_cont_low, upper=strong_cont_high),\n",
    "    lambda img: tf.image.random_saturation(img, lower=weak_sat_low, upper=weak_sat_high),\n",
    "    lambda img: tf.image.random_saturation(img, lower=strong_sat_low, upper=strong_sat_high)\n",
    "]\n",
    "\n",
    "images = []\n",
    "for augmentation in other_augments:\n",
    "    image = augmentation(img_try)\n",
    "    if tf.random.uniform([]) < 1:\n",
    "        augmentation2 = random.choice(base_augments)\n",
    "        image = augmentation2(image)\n",
    "    images.append(image)\n",
    "\n",
    "#Show the images\n",
    "for image in images:\n",
    "    plt.imshow(image, interpolation=None)\n",
    "    plt.grid(None)\n",
    "    plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Metadata Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a metadata dictionary for efficient lookup\n",
    "#Structure: {index: value} where value is (mod_toggle, metadata array)\n",
    "\n",
    "#Objective: We need \"train_ids\" to efficiently retrieve and augment images from the HDF5 file. This already exists.\n",
    "#           We need to accesss metadata through a dictionary to improve speed. A common reference is needed for both.\n",
    "\n",
    "#Idea:      Since train_ids is a LIST of tuples (isic_id, target, mod toggle), it is accessed via indices (ex. train_ids[10]).\n",
    "#           We need to make a dictionary that, for each train_ids index, lists all the metadata associated to the isic_id.\n",
    "#           Thus, when train_ids[10] is called, we call the dictionary and request key=10 to get the metadata.\n",
    "#           Result: very fast data retrieval\n",
    "\n",
    "def make_meta_dict(metadata, isic_ids_tuple):\n",
    "    #Reindex. The metadata must be contiguously indexed. Holes in index numbering will not work.\n",
    "    metadata = metadata.reset_index(drop=True)\n",
    "\n",
    "    #Get the column number for \"isic_id\" and \"target\" in the metadata dataframe.\n",
    "    #This allows us to know where these items are located in the metadata retrieved for each item.\n",
    "    #This is used later to filter out these items from the metadata.\n",
    "    col_num_id = metadata.columns.get_loc(\"isic_id\")\n",
    "    col_num_target = metadata.columns.get_loc(\"target\")\n",
    "\n",
    "    #The metadata contains all unique isic_ids. Since the dataframe is reindexed, it is possible to make\n",
    "    #a dictionary of (isic_id: row number) for fast retrieval of all metadata associated to an isic_id.\n",
    "\n",
    "    #Take the metadata dataframe and create a dictionary that stores (isic_id, row number).\n",
    "    metadata_index_dict = metadata[\"isic_id\"].to_dict()\n",
    "    metadata_index_dict = dict((v, k) for k, v in metadata_index_dict.items())\n",
    "\n",
    "    #Make a dictionary of (index: (mod toggle, metadata)), where index is the position of a sample in train_ids and metadata is the metadata\n",
    "    #associated to the sample's isic_id. We thus create a dict_of_meta that is a mirror image of the \"isic_ids_tuple\" list.\n",
    "    #We must be careful to not shuffle \"isic_ids_tuple\".\n",
    "    dict_of_meta = {}\n",
    "    for pos, tup in enumerate(isic_ids_tuple):\n",
    "        # Use the lookup table to directly find the index\n",
    "        index = metadata_index_dict.get(tup[0], -1)  # -1 if not found\n",
    "\n",
    "        if index != -1:\n",
    "            # Access the row directly without masking\n",
    "            dict_of_meta.update({pos: (tup[2], np.array(metadata.iloc[index].values))})\n",
    "            # Process the row as needed\n",
    "        else:\n",
    "            raise Exception(\"isic_id values are not all unique\")\n",
    "    return dict_of_meta, col_num_id, col_num_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the metadata dictionaries for train-validate-test\n",
    "train_meta_dict, train_pos_isic_id, train_pos_target = make_meta_dict(metadata, train_ids)\n",
    "val_meta_dict, val_pos_isic_id, val_pos_target = make_meta_dict(metadata, val_ids)\n",
    "test_meta_dict, test_pos_isic_id, test_pos_target = make_meta_dict(metadata, test_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAMPLE OF ITEMS FROM VALIDATION META DICTIONARY\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: (-1,\n",
       "  array(['ISIC_2616085', 0, 1.5388517006266915, -1.2524657233277192,\n",
       "         -1.2452222417171193, 0.1868914403136948, 1.4960138499873652,\n",
       "         1.489513140754254, 1.8021660580507075, -0.6862781732305044,\n",
       "         1.677094276692222, -0.0455043959593955, 1.2672573777554574,\n",
       "         2.0602732369643184, 2.008066526011159, 1.3695108120693,\n",
       "         1.953681888526051, -0.7484396702379293, -0.8953769058753382,\n",
       "         0.3161680798812683, -1.2876282077603696, -1.6346095333407162,\n",
       "         -1.1097941207868922, -0.0581298061805623, 0], dtype=object)),\n",
       " 1: (-1,\n",
       "  array(['ISIC_8709951', 0, -0.6494530928191448, -1.081600627344342,\n",
       "         -0.4235174651583185, -1.5485131072274367, -0.4555333575576532,\n",
       "         -0.1392156484397454, -0.551998692600733, -0.2078883459230511,\n",
       "         -0.3977677002097972, -2.111830220186955, -0.4596378996752979,\n",
       "         -1.0444821496756154, -0.7950928351930998, -0.723981626503269,\n",
       "         -1.0820389833969275, -1.1643532764463809, -2.285388717800972,\n",
       "         0.2139391542933429, -1.1153677345241426, 1.2503540414117424,\n",
       "         0.5949542942367879, -2.2767698200752715, 0], dtype=object)),\n",
       " 2: (-1,\n",
       "  array(['ISIC_8349805', 0, 1.9283928680463691, -0.9551692081315144,\n",
       "         -0.6533184656298395, -0.6062101963830554, 0.4648026070467946,\n",
       "         0.2834778447153891, 0.9566832883820686, 2.172351243410327,\n",
       "         0.7269732906730317, -0.2765962322175834, -0.4557167030006051,\n",
       "         1.3032603979921618, 0.7864402272578477, 1.9404629638079347,\n",
       "         0.6765105149877656, 0.6644029055895377, 0.3530110996090735,\n",
       "         -4.041799396015473, -0.0631549701086019, -0.9099092446381204,\n",
       "         0.2998106953098069, -0.2276765773895339, 0], dtype=object))}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Look some dictionary elements to understand the structure {index, (mod_toggle, metadata)}\n",
    "print(\"SAMPLE OF ITEMS FROM VALIDATION META DICTIONARY\")\n",
    "dict(filter(lambda item: item[0] in {0, 1, 2}, val_meta_dict.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Dataset generation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMAGE generator in a class\n",
    "class hdf5_generator_all_included(Generator):\n",
    "    def __init__(self, file, meta_dict, dict_pos_isic_id, dict_pos_target, num_features, imgSize, is_training=False, shuffle_seed=None, apply_hair_removal=False):\n",
    "        self.file = file\n",
    "        self.meta_dict = meta_dict\n",
    "        self.pos_mod_toggle = 0                     #Position of 'mod_toggle' within dictionary: structure {key: value} where value is (mod_toggle, metadata array)\n",
    "        self.pos_metadata_array = 1                 #Position of 'metadata array' within dictionary: structure {key: value} where value is (mod_toggle, metadata array)\n",
    "        self.dict_pos_isic_id = dict_pos_isic_id    #Set position of 'isic_id\" within the metadata array: structure [var0, var1, var2, var3, ...]\n",
    "        self.dict_pos_target = dict_pos_target      #Set position of 'target\" within the metadata array: structure [var0, var1, var2, var3, ...]\n",
    "        self.num_features = num_features\n",
    "        self.imgSize = imgSize\n",
    "        self.is_training = is_training\n",
    "        self.shuffle_seed = shuffle_seed\n",
    "        self.apply_hair_removal = apply_hair_removal\n",
    "        self.len = len(meta_dict)\n",
    "        self.start = 0\n",
    "        self.stop = self.len\n",
    "        self.i = self.start\n",
    "        self.error_check()\n",
    "        self.open_hdf5()\n",
    "        self.order_and_shuffle()\n",
    "        \n",
    "    def send(self, value):\n",
    "        if self.i < self.stop:\n",
    "            if self.i == self.start:\n",
    "                self.open_hdf5()\n",
    "\n",
    "            #Retrieve index of isic_id according to the shuffled order\n",
    "            index = self.order[self.i]\n",
    "\n",
    "            #Retrieve target... remember that each item of the the meta_dict is a tuple of (mod toggle, metadata)\n",
    "            target = self.meta_dict[index][self.pos_metadata_array][self.dict_pos_target]\n",
    "            target = np.reshape(target, (1,1))\n",
    "            target = tf.cast(target, dtype=tf.int32)\n",
    "\n",
    "            #Retrieve metadata... remember that each item of the the meta_dict is a tuple of (mod toggle, metadata)\n",
    "            meta = np.delete(self.meta_dict[index][self.pos_metadata_array], [self.dict_pos_isic_id, self.dict_pos_target], 0)\n",
    "            meta = meta.astype(dtype=float)\n",
    "            meta = tf.cast(meta, dtype=tf.float32)\n",
    "            meta = tf.reshape(meta, shape=(1, self.num_features))\n",
    "\n",
    "            try:\n",
    "                #Retrieve isic_id\n",
    "                img_name = self.meta_dict[index][self.pos_metadata_array][self.dict_pos_isic_id]\n",
    "                \n",
    "                # Load image data from HDF5\n",
    "                img = np.array(Image.open(io.BytesIO(self.h5file[img_name][()])))\n",
    "\n",
    "                # Clean image\n",
    "                if self.apply_hair_removal:\n",
    "                    img = hair_removal(img)\n",
    "\n",
    "                # Resize the image\n",
    "                img = cv2.resize(img, (self.imgSize, self.imgSize), interpolation= cv2.INTER_AREA)\n",
    "\n",
    "                # Apply augmentations if needed\n",
    "                mod_toggle = self.meta_dict[index][self.pos_mod_toggle]\n",
    "                if mod_toggle > 0:\n",
    "                    img=augment_image(img, mod_toggle, self.is_training)\n",
    "                    \n",
    "                # Standardize and return as TensorFlow constant\n",
    "                img = tf.constant(img / 255, dtype=tf.float32)\n",
    "                \n",
    "                #Augment counter\n",
    "                self.i = self.i + 1\n",
    "                return (img, meta), target\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error loading image {img_name}: {e}\")\n",
    "                # log the error to a file for later analysis\n",
    "                with open('image_errors.log', 'a') as f:\n",
    "                    f.write(f\"Error loading image {img_name}: {e}\\n\")\n",
    "\n",
    "            if self.i == self.stop:\n",
    "                self.h5file.close()\n",
    "        raise StopIteration\n",
    "\n",
    "    def throw(self, typ, val=None, tb=None):\n",
    "        #Close HDF5 file and terminate generator\n",
    "        try:\n",
    "            self.h5file.close()\n",
    "            super().throw(typ, val, tb)\n",
    "        except:\n",
    "            super().throw(typ, val, tb)\n",
    "\n",
    "    def error_check(self):\n",
    "        #Seed type check\n",
    "        try:\n",
    "            int(self.shuffle_seed) == self.shuffle_seed\n",
    "        except:\n",
    "            if self.shuffle_seed != None:\n",
    "                raise Exception(\"Seed must either be an integer or None\")\n",
    "\n",
    "    def order_and_shuffle(self):\n",
    "        np.random.seed(self.shuffle_seed)\n",
    "        self.order = np.array(list(self.meta_dict.keys()), dtype=int)\n",
    "        if self.shuffle_seed != None:\n",
    "            np.random.shuffle(self.order)\n",
    "\n",
    "    def open_hdf5(self):\n",
    "        self.h5file = h5py.File(self.file, 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(hdf5_file, meta_dict, dict_pos_isic_id, dict_pos_target, imgSize=100, batch_size=32, is_training=False, shuffle_seed = None, apply_hair_removal=False):\n",
    "    num_features = len(val_meta_dict[0][1]) - 2 #Subtract isic_id and target\n",
    "\n",
    "    combined_generator = hdf5_generator_all_included(hdf5_file, meta_dict, dict_pos_isic_id, dict_pos_target, num_features, imgSize, is_training, shuffle_seed, apply_hair_removal)\n",
    "\n",
    "    # Generate image dataset\n",
    "    element_spec = ((tf.TensorSpec(shape=(imgSize, imgSize, 3), dtype=tf.float32),\n",
    "                    tf.TensorSpec(shape=(1, num_features), dtype=tf.float32)),\n",
    "                    tf.TensorSpec(shape=(1, 1), dtype=tf.int32))\n",
    "\n",
    "\n",
    "    img_dataset = tf.data.Dataset.from_generator(\n",
    "        lambda: combined_generator,\n",
    "        output_signature=element_spec\n",
    "    )\n",
    "\n",
    "    return img_dataset.batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) Alternative dataset function to load all in memory (not a generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_in_memory_dataset(hdf5_file, meta_dict, dict_pos_isic_id, dict_pos_target, imgSize=100, batch_size=32, is_training=False, shuffle_seed=None, apply_hair_removal=False):\n",
    "    #Position of elements in the value part of the dictionary (it is a tuple with multiple elements)\n",
    "    pos_mod_toggle = 0\n",
    "    pos_metadata_array = 1\n",
    "\n",
    "    num_features = len(meta_dict[0][pos_metadata_array]) - 2  # Subtract isic_id and target columns\n",
    "\n",
    "    # Initialize lists to hold images, metadata, and targets\n",
    "    images = []\n",
    "    metas = []\n",
    "    targets = []\n",
    "\n",
    "    np.random.seed(shuffle_seed)\n",
    "    order = np.array(list(meta_dict.keys()), dtype=int)\n",
    "    if shuffle_seed is not None:\n",
    "        np.random.shuffle(order)\n",
    "        \n",
    "    with h5py.File(hdf5_file, 'r') as h5file:\n",
    "        for i in range(len(meta_dict)):\n",
    "            index = order[i]\n",
    "            \n",
    "            # Retrieve target\n",
    "            target = meta_dict[index][pos_metadata_array][dict_pos_target]\n",
    "            target = np.reshape(target, (1, 1))\n",
    "            target = tf.cast(target, dtype=tf.int32)\n",
    "\n",
    "            # Retrieve metadata\n",
    "            meta = np.delete(meta_dict[index][pos_metadata_array], [dict_pos_isic_id, dict_pos_target], 0)\n",
    "            meta = meta.astype(dtype=float)\n",
    "            meta = tf.cast(meta, dtype=tf.float32)\n",
    "            meta = tf.reshape(meta, shape=(1, num_features))\n",
    "\n",
    "            try:\n",
    "                # Retrieve isic_id and load image\n",
    "                img_name = meta_dict[index][pos_metadata_array][dict_pos_isic_id]\n",
    "                \n",
    "                # Load image data from HDF5\n",
    "                img = np.array(Image.open(io.BytesIO(h5file[img_name][()])))\n",
    "\n",
    "                # Clean image\n",
    "                if apply_hair_removal:\n",
    "                    img = hair_removal(img)\n",
    "\n",
    "                # Resize the image\n",
    "                img = cv2.resize(img, (imgSize, imgSize), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "                # Apply augmentations if needed\n",
    "                mod_toggle = meta_dict[index][pos_mod_toggle]\n",
    "                if mod_toggle > 0:\n",
    "                    img=augment_image(img, mod_toggle, is_training)\n",
    "\n",
    "                # Normalize the image\n",
    "                img = tf.constant(img / 255, dtype=tf.float32)\n",
    "\n",
    "                # Add processed data to lists\n",
    "                images.append(img)\n",
    "                metas.append(meta)\n",
    "                targets.append(target)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading image {img_name}: {e}\")\n",
    "                with open('image_errors.log', 'a') as f:\n",
    "                    f.write(f\"Error loading image {img_name}: {e}\\n\")\n",
    "                continue\n",
    "\n",
    "    # Convert lists to tensors\n",
    "    images_tensor = tf.stack(images, axis=0)\n",
    "    metas_tensor = tf.stack(metas, axis=0)\n",
    "    targets_tensor = tf.stack(targets, axis=0)\n",
    "\n",
    "    # Combine the images, metadata, and targets into a tf.data.Dataset\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(((images_tensor, metas_tensor), targets_tensor))\n",
    "\n",
    "    # Batch and prefetch the dataset\n",
    "    return dataset.batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8) CNN MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 - Model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hybrid CNN model than runs images through convolutional layers, then concatenates output with metadata, and runs the data through a NN\n",
    "@tf.keras.utils.register_keras_serializable(package=\"MyLayers\", name=\"KernelMult\")\n",
    "class Hybrid_model_series(tf.keras.Model):\n",
    "    def __init__(self, neurons, dropout, num_features, activ = 'leaky_relu', img_size = 100, img_channels = 3, **kwargs):\n",
    "        #Run the constructor of the parent class\n",
    "        super(). __init__(**kwargs)\n",
    "        \n",
    "        #Save inputs\n",
    "        self.neurons = neurons\n",
    "        self.dropout = dropout\n",
    "        self.activ = activ\n",
    "        self.img_size = img_size\n",
    "        self.img_channels = img_channels\n",
    "        self.num_features = num_features\n",
    "\n",
    "        #Weight and bias initializers\n",
    "        self.kernel_initializer = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.05, seed=None)\n",
    "        self.bias_initializer = tf.keras.initializers.RandomUniform(minval=-0.05, maxval=0.05, seed=None)\n",
    "\n",
    "        #Layers\n",
    "        self.conv1 = tf.keras.layers.Conv2D(filters=32, kernel_size=5, strides=(1, 1), activation='relu', padding='same', input_shape=(self.img_size, self.img_size, self.img_channels),\n",
    "                                            kernel_initializer=self.kernel_initializer, bias_initializer=self.bias_initializer)\n",
    "        self.conv2 = tf.keras.layers.Conv2D(64, 5, activation='relu', kernel_initializer=self.kernel_initializer, bias_initializer=self.bias_initializer)\n",
    "        self.pool = tf.keras.layers.MaxPool2D(pool_size=(2,2))\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.dense1 = tf.keras.layers.Dense(self.neurons, activation = self.activ, kernel_initializer=self.kernel_initializer, bias_initializer=self.bias_initializer)\n",
    "        self.dropout1 = tf.keras.layers.Dropout(self.dropout)\n",
    "        self.dense2 = tf.keras.layers.Dense(self.neurons, activation = self.activ, kernel_initializer=self.kernel_initializer, bias_initializer=self.bias_initializer)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(self.dropout)\n",
    "        self.dense3 = tf.keras.layers.Dense(1, activation='sigmoid', kernel_initializer=self.kernel_initializer, bias_initializer=self.bias_initializer)\n",
    "        self.concatenate = keras.layers.Concatenate(axis=1)\n",
    "    \n",
    "    def call(self, inputs, training=False):\n",
    "        flattened_inputs = tf.nest.flatten(inputs)\n",
    "        x_image, x_meta = flattened_inputs\n",
    "        # Convolutions & pooling\n",
    "        x = self.conv1(x_image)\n",
    "        x = self.pool(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.pool(x)\n",
    "        # Flattening of images and concatenation with other data\n",
    "        x = self.flatten(x)\n",
    "        # Reshape metadata to match dimensions\n",
    "        x_meta = keras.layers.Reshape(target_shape=([x_meta.shape[-1]]))(x_meta)\n",
    "        # Concatenate image and metadata\n",
    "        x_all = self.concatenate([x, x_meta])\n",
    "        # Neural Network\n",
    "        x_all = self.dense1(x_all)\n",
    "        x_all = self.dropout1(x_all, training=training)\n",
    "        x_all = self.dense2(x_all)\n",
    "        x_all = self.dropout2(x_all, training=training)\n",
    "        output = self.dense3(x_all)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hybrid CNN model than runs metadata through a NN, images through convolutional layers, then concatenates results before a final layer (1 neuron)\n",
    "@tf.keras.utils.register_keras_serializable(package=\"MyLayers\", name=\"KernelMult\")\n",
    "class Hybrid_model_parallel(tf.keras.Model):\n",
    "    def __init__(self, neurons, dropout, num_features, activ = 'leaky_relu', img_size = 100, img_channels = 3, **kwargs):\n",
    "        #Run the constructor of the parent class\n",
    "        super(). __init__(**kwargs)\n",
    "        \n",
    "        #Save inputs\n",
    "        self.neurons = neurons\n",
    "        self.dropout = dropout\n",
    "        self.activ = activ\n",
    "        self.img_size = img_size\n",
    "        self.img_channels = img_channels\n",
    "        self.num_features = num_features\n",
    "\n",
    "        #Weight and bias initializers\n",
    "        self.kernel_initializer = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.05, seed=None)\n",
    "        self.bias_initializer = tf.keras.initializers.RandomUniform(minval=-0.05, maxval=0.05, seed=None)\n",
    "\n",
    "        #Convolution Layers\n",
    "        self.conv1 = tf.keras.layers.Conv2D(filters=32, kernel_size=5, strides=(1, 1), activation='relu', padding='same', input_shape=(self.img_size, self.img_size, self.img_channels),\n",
    "                                            kernel_initializer=self.kernel_initializer, bias_initializer=self.bias_initializer)\n",
    "        self.conv2 = tf.keras.layers.Conv2D(64, 5, activation='relu', kernel_initializer=self.kernel_initializer, bias_initializer=self.bias_initializer)\n",
    "        self.pool = tf.keras.layers.MaxPool2D(pool_size=(2,2))\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "\n",
    "        #Neural network layers (metadata)\n",
    "        self.dense1 = tf.keras.layers.Dense(self.neurons, activation = self.activ, input_shape=(1,self.num_features), kernel_initializer=self.kernel_initializer, bias_initializer=self.bias_initializer)\n",
    "        self.dropout1 = tf.keras.layers.Dropout(self.dropout)\n",
    "        self.dense2 = tf.keras.layers.Dense(self.neurons, activation = self.activ, kernel_initializer=self.kernel_initializer, bias_initializer=self.bias_initializer)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(self.dropout)\n",
    "\n",
    "        #Final layer\n",
    "        self.concatenate = keras.layers.Concatenate(axis=1)\n",
    "        self.dense3 = tf.keras.layers.Dense(1, activation='sigmoid', kernel_initializer=self.kernel_initializer, bias_initializer=self.bias_initializer)\n",
    "    \n",
    "    def call(self, inputs, training=False):\n",
    "        flattened_inputs = tf.nest.flatten(inputs)\n",
    "        x_image, x_meta = flattened_inputs\n",
    "        #CONVOLUTIONAL LAYERS (IMAGES)\n",
    "        # Convolutions & pooling\n",
    "        x1 = self.conv1(x_image)\n",
    "        x1 = self.pool(x1)\n",
    "        x1 = self.conv2(x1)\n",
    "        x1 = self.pool(x1)\n",
    "        # Flattening of images and concatenation with other data\n",
    "        x1 = self.flatten(x1)\n",
    "\n",
    "        #NEURAL NETWORK LAYERS (METADATA ONLY)\n",
    "        # Neural Network\n",
    "        x2 = keras.layers.Reshape(target_shape=([x_meta.shape[-1]]))(x_meta)\n",
    "        x2 = self.dense1(x2)\n",
    "        x2 = self.dropout1(x2, training=training)\n",
    "        x2 = self.dense2(x2)\n",
    "        x2 = self.dropout2(x2, training=training)\n",
    "\n",
    "        #FINAL LAYER (OUTPUT)\n",
    "        # Concatenate image and metadata\n",
    "        x_all = self.concatenate([x1, x2])\n",
    "        output = self.dense3(x_all)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 - Model compiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Andrew\\anaconda3\\envs\\cancer_test2\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "#Set seed\n",
    "tf.random.set_seed(71)\n",
    "\n",
    "#Initialize model\n",
    "if model_choice == 0:\n",
    "    model = Hybrid_model_series(neurons=nb_neurons_hidden_layers, dropout=dropout, num_features=num_features, activ='leaky_relu')\n",
    "else:\n",
    "    model = Hybrid_model_parallel(neurons=nb_neurons_hidden_layers, dropout=dropout, num_features=num_features, activ='leaky_relu')\n",
    "\n",
    "\n",
    "#Define optimizer and loss function\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "loss = tf.keras.losses.BinaryCrossentropy(from_logits=False,\n",
    "                                          label_smoothing=0.0,\n",
    "                                          axis=-1,\n",
    "                                          reduction='sum_over_batch_size',\n",
    "                                          name='binary_crossentropy')\n",
    "\n",
    "#Compile the model with loss, optimizer, and metrics\n",
    "model.compile(loss = loss,\n",
    "              optimizer = optimizer,\n",
    "              metrics = [\n",
    "                  tf.keras.metrics.BinaryAccuracy(),\n",
    "                  tf.keras.metrics.FalseNegatives(),\n",
    "                  tf.keras.metrics.FalsePositives(),\n",
    "                  tf.keras.metrics.TrueNegatives(),\n",
    "                  tf.keras.metrics.TruePositives(),\n",
    "                  tf.keras.metrics.AUC(curve='ROC', name='AUC_ROC'),\n",
    "                  tf.keras.metrics.AUC(curve='PR', name='AUC_PR'),\n",
    "                  ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3 - Model loss function weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define function to calculate weights to use in loss function\n",
    "def compute_class_weights(meta_dict, pos_target):\n",
    "    # Initialize counters for target=0 and target=1\n",
    "    target_0_count = 0\n",
    "    target_1_count = 0\n",
    "\n",
    "    # Calculate total number of images\n",
    "    total = len(meta_dict)\n",
    "    # Calculate number of target = 1 by summing the target value for each dict item\n",
    "    target_1_count = sum([meta_dict[key][1][pos_target] for key in range(total)])\n",
    "    # Calculate number of target = 1\n",
    "    target_0_count = total - target_1_count\n",
    "\n",
    "    # Calculate class weights based on the counts, avoid division by zero\n",
    "    if target_1_count > 0 :\n",
    "        if target_1_count < target_0_count:\n",
    "            weight_for_0 = 1\n",
    "            weight_for_1 = target_0_count/target_1_count\n",
    "        elif target_0_count > 0:\n",
    "            weight_for_0 = target_1_count/target_0_count\n",
    "            weight_for_1 = 1\n",
    "        else:\n",
    "            weight_for_0 = 0\n",
    "            weight_for_1=target_1_count\n",
    "    else:\n",
    "        weight_for_0 = target_0_count\n",
    "        weight_for_1 = 0\n",
    "        \n",
    "\n",
    "    return weight_for_0, weight_for_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get weights for training\n",
    "weight_for_0, weight_for_1 = compute_class_weights(train_meta_dict, train_pos_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.4 - Model Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training batches in dataset: 70\n",
      "Total validate batches in dataset: 10\n"
     ]
    }
   ],
   "source": [
    "#Determine the number of batches (includes last incomplete batch)\n",
    "nb_training_batches = int(np.ceil(len(train_meta_dict)/train_batch_size))\n",
    "nb_validate_batches = int(np.ceil(len(val_meta_dict)/val_batch_size))\n",
    "\n",
    "#Print results\n",
    "print(\"Total training batches in dataset:\", nb_training_batches)\n",
    "print(\"Total validate batches in dataset:\", nb_validate_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load training dataset into memory to speed up the model\n",
    "if save_train_in_memory:\n",
    "    train_in_memory = make_in_memory_dataset(hdf5_file, train_meta_dict, train_pos_isic_id, train_pos_target, apply_hair_removal=apply_hair_removal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load validation dataset into memory to speed up the validation steps of the model\n",
    "if save_val_in_memory:\n",
    "    val_in_memory = make_in_memory_dataset(hdf5_file, val_meta_dict, val_pos_isic_id, val_pos_target, apply_hair_removal=apply_hair_removal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clear the memory leak in Keras\n",
    "class CustomCallback(tf.keras.callbacks.Callback):\n",
    "  def on_epoch_end(self, epoch, logs=None):\n",
    "    gc.collect()\n",
    "    #print(f\"Epoch {epoch+1} finished. Validation loss: {logs['val_loss']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Andrew\\anaconda3\\envs\\cancer_test2\\Lib\\site-packages\\numpy\\core\\getlimits.py:549: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
      "  setattr(self, word, getattr(machar, word).flat[0])\n",
      "c:\\Users\\Andrew\\anaconda3\\envs\\cancer_test2\\Lib\\site-packages\\numpy\\core\\getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
      "  return self._float_to_str(self.smallest_subnormal)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 259ms/step - AUC_PR: 0.1553 - AUC_ROC: 0.6357 - binary_accuracy: 0.5541 - false_negatives: 31.1972 - false_positives: 523.0986 - loss: 1.4568 - true_negatives: 532.3662 - true_positives: 64.5775 - val_AUC_PR: 0.7490 - val_AUC_ROC: 0.8477 - val_binary_accuracy: 0.6409 - val_false_negatives: 10.0000 - val_false_positives: 97.0000 - val_loss: 0.7384 - val_true_negatives: 96.0000 - val_true_positives: 95.0000\n",
      "WARNING:tensorflow:From c:\\Users\\Andrew\\anaconda3\\envs\\cancer_test2\\Lib\\site-packages\\keras\\src\\backend\\common\\global_state.py:82: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "EPOCH 2\n",
      "\u001b[1m10/70\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 207ms/step - AUC_PR: 0.3986 - AUC_ROC: 0.8656 - binary_accuracy: 0.5934 - false_negatives: 1.9000 - false_positives: 63.1000 - loss: 0.9737 - true_negatives: 100.2000 - true_positives: 10.8000"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 29\u001b[0m\n\u001b[0;32m     26\u001b[0m         mod \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(train_in_memory, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, steps_per_epoch \u001b[38;5;241m=\u001b[39m nb_training_batches, validation_data \u001b[38;5;241m=\u001b[39m val_dataset, validation_steps \u001b[38;5;241m=\u001b[39m nb_validate_batches, callbacks \u001b[38;5;241m=\u001b[39m [CustomCallback()],\n\u001b[0;32m     27\u001b[0m                     class_weight\u001b[38;5;241m=\u001b[39m{\u001b[38;5;241m0\u001b[39m: weight_for_0, \u001b[38;5;241m1\u001b[39m: weight_for_1})\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 29\u001b[0m         mod \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnb_training_batches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mval_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnb_validate_batches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mCustomCallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_for_0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_for_1\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m#Save results\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Andrew\\anaconda3\\envs\\cancer_test2\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\Andrew\\anaconda3\\envs\\cancer_test2\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:320\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator\u001b[38;5;241m.\u001b[39menumerate_epoch():\n\u001b[0;32m    319\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m--> 320\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    321\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n\u001b[0;32m    322\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n",
      "File \u001b[1;32mc:\\Users\\Andrew\\anaconda3\\envs\\cancer_test2\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\Andrew\\anaconda3\\envs\\cancer_test2\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\Andrew\\anaconda3\\envs\\cancer_test2\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[1;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[0;32m    880\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[0;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Andrew\\anaconda3\\envs\\cancer_test2\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Andrew\\anaconda3\\envs\\cancer_test2\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1324\u001b[0m     args,\n\u001b[0;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1326\u001b[0m     executing_eagerly)\n\u001b[0;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\Andrew\\anaconda3\\envs\\cancer_test2\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32mc:\\Users\\Andrew\\anaconda3\\envs\\cancer_test2\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Andrew\\anaconda3\\envs\\cancer_test2\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1500\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1498\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1499\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1500\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1501\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1503\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1504\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1505\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1506\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1507\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1508\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1509\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1510\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1514\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1515\u001b[0m   )\n",
      "File \u001b[1;32mc:\\Users\\Andrew\\anaconda3\\envs\\cancer_test2\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Run the model through epochs\n",
    "for epoch in range(1, nb_epochs + 1):\n",
    "    #Make datasets\n",
    "    print(\"EPOCH\", epoch)\n",
    "\n",
    "    #Reinitialize the training dataset with a new shuffle each time\n",
    "    shuffle_seed = 8 + epoch #Next initialization of datasets will have a different shuffle\n",
    "    \n",
    "    #Reinitialize dataset generators (case when the dataset is not all stored in RAM)\n",
    "    if save_train_in_memory == False:\n",
    "        train_dataset = make_dataset(hdf5_file, train_meta_dict, train_pos_isic_id, train_pos_target, batch_size = train_batch_size, is_training=True, shuffle_seed=shuffle_seed, apply_hair_removal=apply_hair_removal)\n",
    "    if save_val_in_memory == False:\n",
    "        val_dataset = make_dataset(hdf5_file, val_meta_dict, val_pos_isic_id, val_pos_target, batch_size = val_batch_size, is_training=False, shuffle_seed=shuffle_seed, apply_hair_removal=apply_hair_removal)\n",
    "\n",
    "\n",
    "    #Fit the model, using validation data either stored in memory or on the hard disk\n",
    "    if save_val_in_memory:\n",
    "        if save_train_in_memory:\n",
    "            mod = model.fit(train_in_memory, epochs=1, steps_per_epoch = nb_training_batches, validation_data = val_in_memory, validation_steps = nb_validate_batches, callbacks = [CustomCallback()],\n",
    "                        class_weight={0: weight_for_0, 1: weight_for_1})\n",
    "        else:\n",
    "            mod = model.fit(train_dataset, epochs=1, steps_per_epoch = nb_training_batches, validation_data = val_in_memory, validation_steps = nb_validate_batches, callbacks = [CustomCallback()],\n",
    "                        class_weight={0: weight_for_0, 1: weight_for_1})\n",
    "    else:\n",
    "        if save_train_in_memory:\n",
    "            mod = model.fit(train_in_memory, epochs=1, steps_per_epoch = nb_training_batches, validation_data = val_dataset, validation_steps = nb_validate_batches, callbacks = [CustomCallback()],\n",
    "                        class_weight={0: weight_for_0, 1: weight_for_1})\n",
    "        else:\n",
    "            mod = model.fit(train_dataset, epochs=1, steps_per_epoch = nb_training_batches, validation_data = val_dataset, validation_steps = nb_validate_batches, callbacks = [CustomCallback()],\n",
    "                        class_weight={0: weight_for_0, 1: weight_for_1})\n",
    "    \n",
    "    #Save results\n",
    "    if epoch == 1:\n",
    "        results = mod.history\n",
    "    else:\n",
    "        for key in mod.history:   \n",
    "            results[key] += mod.history[key]\n",
    "            \n",
    "    #Export model structure (json) - REIMPORT NOT WORKING\n",
    "    \"\"\"\n",
    "    if epoch == 1:\n",
    "        model_json = model.to_json()\n",
    "        with open(savePath + \"model.json\", \"w\") as json_file:\n",
    "            json_file.write(model_json)\n",
    "    \"\"\"\n",
    "    if use_sagemaker == False:\n",
    "        # Periodic saving of weights \n",
    "        if wt_save_freq > 0:\n",
    "            if (epoch % wt_save_freq == 0):\n",
    "                filename = \"Model_\" + \"Epoch_\" + str(epoch) + \".weights.h5\"\n",
    "                model.save_weights(savePath + filename)\n",
    "        # Saving of final weights if requested\n",
    "        if final_wt_save and epoch == nb_epochs:\n",
    "            filename = \"Model_\" + \"Last_Epoch\" + \".weights.h5\"\n",
    "            model.save_weights(savePath + filename)\n",
    "    else:\n",
    "        # Periodic saving of weights locally + transfer to S3\n",
    "        if wt_save_freq > 0 and (epoch % wt_save_freq == 0):\n",
    "            filename = f\"Model_Epoch_{epoch}.weights.h5\"\n",
    "            local_weight_path = os.path.join(localsavePath, filename)\n",
    "\n",
    "            # Save weights locally\n",
    "            model.save_weights(local_weight_path)\n",
    "\n",
    "            # Send the weight to the S3 bucket\n",
    "            s3.upload_file(local_weight_path, bucket, f\"enregistrement/{filename}\")\n",
    "\n",
    "            # Delete local files after S3 transfer\n",
    "            #os.remove(local_weight_path)\n",
    "\n",
    "        # Saving of final weights if requested\n",
    "        if final_wt_save and epoch == nb_epochs:\n",
    "            filename = \"Model_Last_Epoch.weights.h5\"\n",
    "            local_weight_path = os.path.join(localsavePath, filename)\n",
    "\n",
    "            # Save weights locally\n",
    "            model.save_weights(local_weight_path)\n",
    "\n",
    "            # Send the weight to the S3 bucket\n",
    "            s3.upload_file(local_weight_path, bucket, f\"enregistrement/{filename}\")\n",
    "\n",
    "            # Delete local files after S3 transfer\n",
    "            #os.remove(local_weight_path)\n",
    "\n",
    "    #Clean memory after use\n",
    "    del mod\n",
    "    #If save_val_in_memory is false, we have a generator. In this case, we want to delete the generator.\n",
    "    if save_train_in_memory == False:\n",
    "        del train_dataset\n",
    "    #If save_val_in_memory is false, we have a generator. In this case, we want to delete the generator.\n",
    "    if save_val_in_memory == False:\n",
    "        del val_dataset\n",
    "    tf.keras.backend.clear_session()\n",
    "    gc.collect()\n",
    "\n",
    "    #Early termination (check after 15 epochs)\n",
    "    if epoch >= 15 and early_break == True:\n",
    "        #Calculate previous three changes, if positive, then loss is increasing\n",
    "        change1 = results[\"val_loss\"][-1] - results[\"val_loss\"][-2]\n",
    "        change2 = results[\"val_loss\"][-2] - results[\"val_loss\"][-3]\n",
    "        change3 = results[\"val_loss\"][-3] - results[\"val_loss\"][-4]\n",
    "\n",
    "        #Three consecutive increases in validation loss will stop the model\n",
    "        if change1 > 0 and change2 > 0 and change3 > 0:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"hybrid_model_series\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"hybrid_model_series\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,432</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">46</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">46</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">51,264</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">33856</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">36</span>)             │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,219,680</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">36</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,332</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">37</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ concatenate (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">33879</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │         \u001b[38;5;34m2,432\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m46\u001b[0m, \u001b[38;5;34m46\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m51,264\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m33856\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m36\u001b[0m)             │     \u001b[38;5;34m1,219,680\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m36\u001b[0m)             │         \u001b[38;5;34m1,332\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m37\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ concatenate (\u001b[38;5;33mConcatenate\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m33879\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,824,237</span> (14.59 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,824,237\u001b[0m (14.59 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,274,745</span> (4.86 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,274,745\u001b[0m (4.86 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,549,492</span> (9.73 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m2,549,492\u001b[0m (9.73 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<KerasVariable shape=(5, 5, 3, 32), dtype=float32, path=hybrid_model_series/conv2d/kernel>,\n",
       " <KerasVariable shape=(32,), dtype=float32, path=hybrid_model_series/conv2d/bias>,\n",
       " <KerasVariable shape=(5, 5, 32, 64), dtype=float32, path=hybrid_model_series/conv2d_1/kernel>,\n",
       " <KerasVariable shape=(64,), dtype=float32, path=hybrid_model_series/conv2d_1/bias>,\n",
       " <KerasVariable shape=(33879, 36), dtype=float32, path=hybrid_model_series/dense/kernel>,\n",
       " <KerasVariable shape=(36,), dtype=float32, path=hybrid_model_series/dense/bias>,\n",
       " <KerasVariable shape=(36, 36), dtype=float32, path=hybrid_model_series/dense_1/kernel>,\n",
       " <KerasVariable shape=(36,), dtype=float32, path=hybrid_model_series/dense_1/bias>,\n",
       " <KerasVariable shape=(36, 1), dtype=float32, path=hybrid_model_series/dense_2/kernel>,\n",
       " <KerasVariable shape=(1,), dtype=float32, path=hybrid_model_series/dense_2/bias>]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Examine the weights objects\n",
    "model.weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5 - Save results and test ids (weights saved within epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n#Import results from file\\nimported_results = pd.read_csv(modelResPath).to_dict(orient='list')\\n\""
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Save results to file\n",
    "pd.DataFrame.from_dict(results).to_csv(modelResPath, index=False)\n",
    "\n",
    "\"\"\"\n",
    "#Import results from file\n",
    "imported_results = pd.read_csv(modelResPath).to_dict(orient='list')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#Import train_ids from file and regenerate test_meta_dict\\ndf = pd.read_csv(test_meta_dict_Path)\\nimported_train_ids = [(df.iloc[i,0], df.iloc[i,1], df.iloc[i,2]) for i in range(len(df))]\\ntest_meta_dict, test_pos_isic_id, test_pos_target = make_meta_dict(metadata, imported_train_ids)\\ndel df\\n'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Save test_ids to file\n",
    "pd.DataFrame(test_ids).to_csv(test_ids_Path, index=False)\n",
    "\n",
    "'''\n",
    "#Import train_ids from file and regenerate test_meta_dict\n",
    "df = pd.read_csv(test_meta_dict_Path)\n",
    "imported_train_ids = [(df.iloc[i,0], df.iloc[i,1], df.iloc[i,2]) for i in range(len(df))]\n",
    "test_meta_dict, test_pos_isic_id, test_pos_target = make_meta_dict(metadata, imported_train_ids)\n",
    "del df\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimported_weights = model.load_weights(savePath + \"Model_Last_Epoch.weights.h5\", skip_mismatch=False)\\n\\n#Taken from https://machinelearningmastery.com/save-load-keras-deep-learning-models/\\n# load json and create model\\njson_file = open(savePath + \\'model.json\\', \\'r\\')\\nloaded_model_json = json_file.read()\\njson_file.close()\\nloaded_model = tf.keras.models.model_from_json(loaded_model_json)\\n# load weights into new model\\n#loaded_model.load_weights(\"Model_Last_Epoch.weights.h5\")\\n#print(\"Loaded model from disk\")\\n'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try importing hdf5 and json for model\n",
    "\"\"\"\n",
    "imported_weights = model.load_weights(savePath + \"Model_Last_Epoch.weights.h5\", skip_mismatch=False)\n",
    "\n",
    "#Taken from https://machinelearningmastery.com/save-load-keras-deep-learning-models/\n",
    "# load json and create model\n",
    "json_file = open(savePath + 'model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = tf.keras.models.model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "#loaded_model.load_weights(\"Model_Last_Epoch.weights.h5\")\n",
    "#print(\"Loaded model from disk\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.6 - Plot the losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 1.0)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plot the training and validation losses\n",
    "\n",
    "#Convert loss results into a dataframe\n",
    "result_preproc = pd.DataFrame({\n",
    "    'Epoch': [i+1 for i in range(len(results[\"loss\"]))], \n",
    "    'Train': results[\"loss\"],\n",
    "    'Validate': results[\"val_loss\"]\n",
    "    })\n",
    "\n",
    "# Convert dataframe from wide to long format\n",
    "df = pd.melt(result_preproc, ['Epoch'])\n",
    "\n",
    "#Make plot\n",
    "g = sns.lineplot(data=df, x='Epoch', y='value', hue='variable')\n",
    "g.set_title(\"Loss Curves\")\n",
    "g.legend_.set_title(\"Loss\")\n",
    "g.set_ylabel('Loss')\n",
    "g.set_ylim(0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9) Predict Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1 - Create a function to make reinitalization of test dataset easier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simple simple function to ititialize the test dataset using global variables\n",
    "def init_test_dataset():\n",
    "    return make_dataset(hdf5_file, test_meta_dict, test_pos_isic_id, test_pos_target, batch_size = test_batch_size, apply_hair_removal=apply_hair_removal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2 - Retrieve y_test values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total test batches in dataset: 239\n"
     ]
    }
   ],
   "source": [
    "#Test dataset basic size information: nb of samples and batch size\n",
    "nb_test_batches = int(np.ceil(len(test_meta_dict)/test_batch_size))\n",
    "print(\"Total test batches in dataset:\", nb_test_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retrieve real values for the target in the test dataset\n",
    "y_test = []\n",
    "test_dataset = init_test_dataset()\n",
    "for item in test_dataset.take(nb_test_batches):\n",
    "    img_meta, targ = item\n",
    "    y_test.extend(targ.numpy().flatten())\n",
    "#Convert to numpy array (mathematical operations are faster)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3 - Generate predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 103ms/step\n",
      "Shape of prediction data: (7637,)\n"
     ]
    }
   ],
   "source": [
    "#Reinitialize the test dataset (necessary to start at beginning)\n",
    "test_dataset = init_test_dataset()\n",
    "#Retrieve predictions\n",
    "y_prob = model.predict(test_dataset, steps = nb_test_batches)\n",
    "y_prob = y_prob.flatten()\n",
    "#Put predictionsin a numpy array\n",
    "y_pred = np.array([round(i) for i  in y_prob])\n",
    "print(\"Shape of prediction data:\", y_pred.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.4 - Save y_test and y_pred in exported file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimported_test_results = pd.read_csv(testResPath)\\ny_test = np.array(imported_test_results[\"y_test\"])\\ny_prob = np.array(imported_test_results[\"y_prob\"])\\ny_pred = np.array(imported_test_results[\"y_pred\"])\\ndel imported_test_results\\n'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Save test results to file\n",
    "pd.DataFrame({\"y_test\": y_test, \"y_prob\": y_prob, \"y_pred\": y_pred}).to_csv(testResPath, index=False)\n",
    "\n",
    "\n",
    "#Import results from file\n",
    "\"\"\"\n",
    "imported_test_results = pd.read_csv(testResPath)\n",
    "y_test = np.array(imported_test_results[\"y_test\"])\n",
    "y_prob = np.array(imported_test_results[\"y_prob\"])\n",
    "y_pred = np.array(imported_test_results[\"y_pred\"])\n",
    "del imported_test_results\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.5 - Calculate metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the loss\n",
    "loss = sum(abs(y_test - y_pred))/len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Determine true/false positives and negatives\n",
    "pos_indices = y_test == 1\n",
    "neg_indices = y_test == 0\n",
    "\n",
    "#True positives\n",
    "true_pos = sum(abs(y_test[pos_indices] == y_pred[pos_indices]))\n",
    "\n",
    "#False negatives\n",
    "false_neg = sum(abs(y_test[pos_indices] != y_pred[pos_indices]))\n",
    "\n",
    "#True negatives\n",
    "true_neg = sum(abs(y_test[neg_indices] == y_pred[neg_indices]))\n",
    "\n",
    "#False positives\n",
    "false_pos = sum(abs(y_test[neg_indices] != y_pred[neg_indices]))\n",
    "\n",
    "#Precision\n",
    "try:\n",
    "    precision = true_pos / (true_pos + false_pos)\n",
    "except:\n",
    "    precision = np.nan\n",
    "\n",
    "#Recall (sensitivity)\n",
    "try:\n",
    "    recall = true_pos / (true_pos + false_neg)\n",
    "except:\n",
    "    recall = np.nan\n",
    "\n",
    "#Specificity\n",
    "try:\n",
    "    specificity = true_neg / (true_neg + false_pos)\n",
    "except:\n",
    "    specificity = np.nan\n",
    "\n",
    "#F1 Score\n",
    "try:\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "except:\n",
    "    f1_score = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUC and partial AUC\n",
    "precision_prob, recall_prob, _ = precision_recall_curve(y_test, y_prob)\n",
    "auc_pr = auc(recall_prob, precision_prob)\n",
    "auc_roc = roc_auc_score(y_test, y_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the partial AUC\n",
    "def score(y_true, y_prob, min_tpr: float=0.8) -> float:\n",
    "   \n",
    "    # rescale the target to [0, 1]\n",
    "    y_true = abs(np.asarray(y_true) - 1)\n",
    "   \n",
    "    #  flip the submissions to their compliments\n",
    "    y_pred = -1.0 * np.asarray(y_prob)\n",
    "   \n",
    "    max_fpr = abs(1-min_tpr)\n",
    "   \n",
    "    # using sklearn.metric functions: (1) roc_curve and (2) auc\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_pred, sample_weight=None)\n",
    "    if max_fpr is None or max_fpr == 1:\n",
    "        return auc(fpr, tpr)\n",
    "    if max_fpr <= 0 or max_fpr > 1:\n",
    "        raise ValueError(\"Expected min_tpr in range [0, 1), got: %r\" % min_tpr)\n",
    "   \n",
    "    # Add a single point at max_fpr by linear interpolation\n",
    "    stop = np.searchsorted(fpr, max_fpr, \"right\")\n",
    "    x_interp = [fpr[stop - 1], fpr[stop]]\n",
    "    y_interp = [tpr[stop - 1], tpr[stop]]\n",
    "    tpr = np.append(tpr[:stop], np.interp(max_fpr, x_interp, y_interp))\n",
    "    fpr = np.append(fpr[:stop], max_fpr)\n",
    "    partial_auc = auc(fpr, tpr)\n",
    " \n",
    "    return (partial_auc)\n",
    "\n",
    "partial_auc = score(y_test, y_prob, min_tpr=0.8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---TEST RESULTS---\n",
      "True positives: 2\n",
      "False positives: 319\n",
      "True negatives: 7313\n",
      "False negatives: 3\n",
      "\n",
      "Sensitivity: 0.4\n",
      "Specificity: 0.9582023060796646\n",
      "\n",
      "Precision: 0.006230529595015576\n",
      "Recall: 0.4\n",
      "\n",
      "F1 Score: 0.012269938650306749\n",
      "Loss on test data: 0.04216315307057745\n",
      "\n",
      "AUC-ROC Score:  0.9544811320754717\n",
      "Partial AUC > 0.8:  0.1791928721174004\n",
      "AUC-PR:  0.05978842978920731\n"
     ]
    }
   ],
   "source": [
    "print(\"---TEST RESULTS---\")\n",
    "print(\"True positives:\", true_pos)\n",
    "print(\"False positives:\", false_pos)\n",
    "print(\"True negatives:\", true_neg)\n",
    "print(\"False negatives:\", false_neg)\n",
    "print()\n",
    "print(\"Sensitivity:\", recall)\n",
    "print(\"Specificity:\", specificity)\n",
    "print()\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print()\n",
    "print(\"F1 Score:\", f1_score)\n",
    "print(\"Loss on test data:\", loss)\n",
    "print()\n",
    "print(\"AUC-ROC Score: \", auc_roc)\n",
    "print(\"Partial AUC > 0.8: \", partial_auc)\n",
    "print(\"AUC-PR: \", auc_pr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Display matrix\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix)\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title(\"Confusion Matrix of Model Predictions\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cancer_test2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

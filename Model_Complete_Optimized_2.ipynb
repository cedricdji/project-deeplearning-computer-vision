{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL - IMAGE LOADING & NEURAL NETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Andrew\\anaconda3\\envs\\cancer_isic\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#Import libraries\n",
    "import gc\n",
    "import csv\n",
    "import os\n",
    "import io\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import keras_cv\n",
    "import random\n",
    "from collections.abc import Generator\n",
    "import itertools\n",
    "import datetime\n",
    "\n",
    "TF_ENABLE_ONEDNN_OPTS=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) HYPERPARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fraction of data to use (CEDRIC can reduce to speed up tests... go to 0.02!)\n",
    "train_frac_to_use = 1   #Reduce training data to this fraction. Set to \"1\" to use all data.\n",
    "val_frac_to_use = 1     #Reduce validation data to this fraction. Set to \"1\" to use all data.\n",
    "test_frac_to_use = 1     #Reduce validation data to this fraction. Set to \"1\" to use all data.\n",
    "\n",
    "#Memory management (Speeds up model and currently works with a machine with 16GB of memory)\n",
    "save_val_in_memory = True  #Place all validation directly in memory to accelerate the validation step in the model\n",
    "\n",
    "#Splitting of train-validate-test\n",
    "split_seed = 88                 #Seed to use for all train-validate-test splits, including the split of reserved Target=1 data\n",
    "reserve_frac = 0.1              #Fraction of total original data of Target = 1 (reserved for use in validation data)\n",
    "test_frac = 0.2                 #Fraction of total original data, excluding the reserved fraction, to use as the test data\n",
    "nb_of_augments = 100            #Number of augments to perform on Target = 1 images in train-validate sets\n",
    "val_frac = 0.33                 #Fraction of augmented train-validate list to use as the validation data. The rest becomes the training data.\n",
    "nb_of_augments_reserved = 15    #Number of augmentations to perform on reserved validation fraction (Target = 1). Note: this is added to the validation data.\n",
    "reduce_frac = 0.8               #Fraction of Target = 0 samples to remove from the validation data (improves balance)\n",
    "\n",
    "#Image resizing: all images are adjusted to this size so the the CNN receives same number of data points each time\n",
    "imgSize = 100\n",
    "\n",
    "#Hair removal (applies to all images)\n",
    "apply_hair_removal = False\n",
    "\n",
    "#Batch sizes\n",
    "train_batch_size = 32\n",
    "val_batch_size = 32\n",
    "test_batch_size = 32\n",
    "\n",
    "#Neural Network Parameters\n",
    "nb_neurons_hidden_layers = 36  #Number of neurons in each hidden layer\n",
    "dropout = 0.1                  #Fraction of neurons to drop\n",
    "\n",
    "#Optimizer Parameters\n",
    "learning_rate = 0.01           #Initial learning rate for the Adam optimizer\n",
    "\n",
    "#Epoch management (CEDRIC... reduce number of epochs if needed)\n",
    "nb_epochs = 5\n",
    "early_break = False #End early in case of increasing validation loss\n",
    "\n",
    "#Debugging\n",
    "cheat = False #add the target to the metadata so the model can precisely learn the correct response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) IMPORT DATA & DECLARE SAVES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 - Declare file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORT FILES\n",
    "#Directory for data files - FULL DATA\n",
    "dataPath = \"C:/Users/Andrew/Downloads/isic-2024-challenge/\" #slash required at end\n",
    "#Metadata file paths\n",
    "metaPath = dataPath + \"cleaned-metadata.csv\"\n",
    "#Image file path\n",
    "hdf5_file = dataPath + \"train-image.hdf5\"\n",
    "\n",
    "#SAVE FILES\n",
    "#Directory for saved files \n",
    "savePath = \"C:/Users/Andrew/Projects/deep_learning_saves/\" #slash required at end\n",
    "#Model results\n",
    "modelResPath = savePath + \"model_results_save.csv\"\n",
    "#Test results (y_test, y_pred)\n",
    "testResPath = savePath + \"test_results_save.csv\"\n",
    "\n",
    "#ALTERNATIVE 1 QUI MARCHE PARFOIS\n",
    "#base_path = \"C:/Users/admin/Documents/DSTI/DeepLearning/Project/Computer_Vision/isic-2024-challenge\"\n",
    "#hdf5_file = os.path.join(base_path, \"sampleclaire-image.hdf5\")\n",
    "#metadata = pd.read_csv(metaPath, sep=\",\")\n",
    "\n",
    "#ALTERNATIVE 2 QUI MARCHE PARFOIS\n",
    "#base_path = \"C:/Users/admin/Documents/DSTI/DeepLearning/Project/Computer_Vision/isic-2024-challenge\"\n",
    "#metadata = pd.read_csv(os.path.join(base_path, \"train-metadata.csv\"))\n",
    "#hdf5_file = os.path.join(base_path, \"train-image.hdf5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 - Load metadata from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Andrew\\AppData\\Local\\Temp\\ipykernel_2520\\327758908.py:2: DtypeWarning: Columns (51,52) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  metadata = pd.read_csv(metaPath, sep=\",\")\n"
     ]
    }
   ],
   "source": [
    "#Import metadata from file\n",
    "metadata = pd.read_csv(metaPath, sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- X_meta NA counts --\n",
      "isic_id                           0\n",
      "age_approx                     2798\n",
      "target                            0\n",
      "clin_size_long_diam_mm            0\n",
      "tbp_lv_areaMM2                    0\n",
      "tbp_lv_area_perim_ratio           0\n",
      "tbp_lv_eccentricity               0\n",
      "tbp_lv_minorAxisMM                0\n",
      "tbp_lv_color_std_mean             0\n",
      "tbp_lv_deltaLBnorm                0\n",
      "tbp_lv_radial_color_std_max       0\n",
      "tbp_lv_location                   0\n",
      "dtype: int64\n",
      "Number of unknown for tbp_lv_location 5756\n"
     ]
    }
   ],
   "source": [
    "#METADATA: color and size features having no NAs\n",
    "metadata = metadata[[\"isic_id\",\n",
    "                     \"age_approx\",\n",
    "                     \"target\",\n",
    "                     \"clin_size_long_diam_mm\",\n",
    "                     \"tbp_lv_areaMM2\",\n",
    "                     \"tbp_lv_area_perim_ratio\",\n",
    "                     \"tbp_lv_eccentricity\",\n",
    "                     \"tbp_lv_minorAxisMM\",\n",
    "                     \"tbp_lv_color_std_mean\",\n",
    "                     \"tbp_lv_deltaLBnorm\",\n",
    "                     \"tbp_lv_radial_color_std_max\",\n",
    "                     \"tbp_lv_location\"]]\n",
    "\n",
    "#Verify that there are no NAs\n",
    "print(\"-- X_meta NA counts --\")\n",
    "print(metadata.isna().sum())\n",
    "\n",
    "#Check number of Unknoxn for tbp_lv_location\n",
    "loc_unknown=metadata[metadata[\"tbp_lv_location\"]==\"Unknown\"]\n",
    "print(\"Number of unknown for tbp_lv_location\", len(loc_unknown))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Activate for debugging of the predict function\n",
    "if cheat:\n",
    "    metadata[\"target_cheat\"] = metadata[\"target\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 - Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata=metadata[metadata[\"tbp_lv_location\"]!=\"Unknown\"]\n",
    "\n",
    "loc_unknown2=metadata[metadata[\"tbp_lv_location\"]==\"Unknown\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply One-hot encoding for location\n",
    "location=pd.get_dummies(metadata[\"tbp_lv_location\"],prefix='category')\n",
    "location = location.astype(int)\n",
    "metadata = pd.concat([metadata, location], axis=1)\n",
    "metadata=metadata.drop(\"tbp_lv_location\",axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- X_meta NA counts --\n",
      "isic_id                             0\n",
      "age_approx                          0\n",
      "target                              0\n",
      "clin_size_long_diam_mm              0\n",
      "tbp_lv_areaMM2                      0\n",
      "tbp_lv_area_perim_ratio             0\n",
      "tbp_lv_eccentricity                 0\n",
      "tbp_lv_minorAxisMM                  0\n",
      "tbp_lv_color_std_mean               0\n",
      "tbp_lv_deltaLBnorm                  0\n",
      "tbp_lv_radial_color_std_max         0\n",
      "category_Head & Neck                0\n",
      "category_Left Arm                   0\n",
      "category_Left Arm - Lower           0\n",
      "category_Left Arm - Upper           0\n",
      "category_Left Leg                   0\n",
      "category_Left Leg - Lower           0\n",
      "category_Left Leg - Upper           0\n",
      "category_Right Arm                  0\n",
      "category_Right Arm - Lower          0\n",
      "category_Right Arm - Upper          0\n",
      "category_Right Leg                  0\n",
      "category_Right Leg - Lower          0\n",
      "category_Right Leg - Upper          0\n",
      "category_Torso Back                 0\n",
      "category_Torso Back Bottom Third    0\n",
      "category_Torso Back Middle Third    0\n",
      "category_Torso Back Top Third       0\n",
      "category_Torso Front                0\n",
      "category_Torso Front Bottom Half    0\n",
      "category_Torso Front Top Half       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Calculate the mean of age_approx for each target group\n",
    "mean_age_malign = metadata.loc[metadata[\"target\"] == 1, \"age_approx\"].mean()\n",
    "mean_age_benign = metadata.loc[metadata[\"target\"] == 0, \"age_approx\"].mean()\n",
    "\n",
    "# Define a function to fill NA based on the target value\n",
    "def fill_na_by_target(row):\n",
    "    if pd.isna(row['age_approx']):\n",
    "        if row['target'] == 1:\n",
    "            return mean_age_malign\n",
    "        elif row['target'] == 0:\n",
    "            return mean_age_benign\n",
    "    return row['age_approx']\n",
    "\n",
    "# Apply the function to the age_approx column\n",
    "metadata['age_approx'] = metadata.apply(fill_na_by_target, axis=1)\n",
    "\n",
    "#Verify that there are no NAs\n",
    "print(\"-- X_meta NA counts --\")\n",
    "print(metadata.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Train, Validate, Test Split + Preparation of Data Augmentation\n",
    "1. Make two separate lists of isic_ids for target=0 and target=1. Transform into tuples (isic_id, target, mod toggle). Base data has mod toggle = 0, meaning no adjustment will be made.\n",
    "2. Reserve 10% of target = 1 for validate\n",
    "3. Split into train-validate & test on both lists (0 and 1). Take the test lists (0 and 1), concatenate and shuffle them\n",
    "4. Create augmentation preparation function for target = 1: mod toggle = strictly positive integer (this adds more isic_ids to the list, with mod toggle non zero))\n",
    "5. Run augmentation preparation function on train-validate and the reserved validation data: mod toggle = strictly positive integer\n",
    "6. Split train-validate on both lists (0 and 1)\n",
    "7. Reduce the validation data on target = 0 by value specified in reduce_frac\n",
    "8. Concatenate and shuffle the train lists and validation lists\n",
    "9. Limit training and validation data to speed up training (take only fraction of prepared lists)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 - Make two separate lists of isic_ids for target=0 (mod_toggle = -1) and target=1 (mod_toggle = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total ids with target = 0: 394910\n",
      "Total ids with target = 1: 393\n"
     ]
    }
   ],
   "source": [
    "#Make a list of isic_ids for each target value (0 and 1)\n",
    "isic_id_target_0 = metadata[metadata['target'] == 0]['isic_id'].tolist()\n",
    "isic_id_target_1 = metadata[metadata['target'] == 1]['isic_id'].tolist()\n",
    "\n",
    "#Retrieve dataframe with isic id and target\n",
    "temp_0 = metadata[metadata[\"isic_id\"].isin(isic_id_target_0)].loc[:,[\"isic_id\",\"target\"]]\n",
    "temp_1 = metadata[metadata[\"isic_id\"].isin(isic_id_target_1)].loc[:,[\"isic_id\",\"target\"]]\n",
    "\n",
    "#Convert into list of tuples... this makes it compatible with data augmentations\n",
    "#Form: (isic_id, target, mod toggle)\n",
    "isic_id_target_0 = list(zip(temp_0.iloc[:,0], temp_0.iloc[:,1], [-1]*len(temp_0)))\n",
    "isic_id_target_1 = list(zip(temp_1.iloc[:,0], temp_1.iloc[:,1], [0]*len(temp_1)))\n",
    "\n",
    "#Delete temporary dataframes (the original metadata dataframe is untouched)\n",
    "del temp_0\n",
    "del temp_1\n",
    "\n",
    "#Count the number of occurrences for each target value\n",
    "print(\"Total ids with target = 0:\", len(isic_id_target_0))\n",
    "print(\"Total ids with target = 1:\", len(isic_id_target_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 - Reserve 10% of target = 1 for validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total ids with target = 0: 394910\n",
      "Total ids with target = 1: 353\n",
      "Total reserved target = 1: 40\n"
     ]
    }
   ],
   "source": [
    "#Keep 10% of isic_Id of target=1 without duplication\n",
    "isic_id_target_1, isic_id_target_1_reserved = train_test_split(isic_id_target_1, test_size = reserve_frac, random_state=split_seed, shuffle=False)\n",
    "\n",
    "#Count the number of occurrences for each target value (AFTER RESERVATION)\n",
    "print(\"Total ids with target = 0:\", len(isic_id_target_0))\n",
    "print(\"Total ids with target = 1:\", len(isic_id_target_1))\n",
    "print(\"Total reserved target = 1:\", len(isic_id_target_1_reserved))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 - Split into train-validate & test on both lists (0 and 1). Take the test lists (0 and 1), concatenate and shuffle them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split out the test ids\n",
    "trainval_0, test_0 = train_test_split(isic_id_target_0, test_size = test_frac, random_state=split_seed, shuffle=True)\n",
    "trainval_1, test_1 = train_test_split(isic_id_target_1, test_size = test_frac, random_state=split_seed, shuffle=True)\n",
    "\n",
    "test_ids = test_0 + test_1\n",
    "np.random.shuffle(test_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 - Create augmentation preparation function for target = 1: mod toggle = strictly positive integer\n",
    "(this adds more isic_ids to the list, with mod toggle non zero))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate a list containing augmentation toggles. Apply only to training and validation sets.\n",
    "def augment_prep(tuple_list, nb_of_augments = 30, shuffle_seed=None):\n",
    "    augment_list = []\n",
    "    \n",
    "    #If augmentation is desired, then the mod toggle is a strictly positive integer\n",
    "    augment_list = [(item[0], item[1], i) for item in tuple_list for i in range(1, nb_of_augments + 1)]\n",
    "    \n",
    "    #Shuffle the list\n",
    "    augment_list.extend(tuple_list)\n",
    "    np.random.seed(shuffle_seed)\n",
    "    np.random.shuffle(augment_list)\n",
    "\n",
    "    return augment_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 - Run augmentation preparation function on train-validate and the reserved validation data: mod toggle = strictly positive integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Augment the training and validation list\n",
    "trainval_1 = augment_prep(trainval_1, nb_of_augments=nb_of_augments, shuffle_seed=50)\n",
    "\n",
    "#Duplicate the reserved training data\n",
    "reserved_1 = augment_prep(isic_id_target_1_reserved, nb_of_augments=nb_of_augments_reserved, shuffle_seed=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 - Split train-validate on both lists (0 and 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split training and validation lists\n",
    "train_0, val_0 = train_test_split(trainval_0, test_size = val_frac, random_state=split_seed, shuffle=True)\n",
    "train_1, val_1 = train_test_split(trainval_1, test_size = val_frac, random_state=split_seed, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7 - Reduce the validation data on target = 0 by value specified in reduce_frac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reduce the validation data of type Target = 0\n",
    "nb_samples = int((1 - reduce_frac) * len(val_0))\n",
    "val_0 = random.sample(val_0, nb_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.8 - Concatenate and shuffle the train and validation lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concatenate\n",
    "train_ids=train_0.copy()\n",
    "train_ids.extend(train_1)\n",
    "val_ids=list(itertools.chain(val_0, val_1, reserved_1))\n",
    "\n",
    "#Shuffle\n",
    "np.random.seed(60)\n",
    "np.random.shuffle(train_ids)\n",
    "np.random.shuffle(val_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Validate/Test Counts: 230753 / 30891 / 79053\n",
      "Train/Validate/Test Fractions: 0.68 / 0.09 / 0.23\n",
      "Proportion of Target = 1 in training data: 0.08269448284529345\n",
      "Proportion of Target = 1 in validation data: 0.3250137580525072\n",
      "Proportion of Target = 1 in test data: 0.0008981316332081009\n"
     ]
    }
   ],
   "source": [
    "#Calculate the proportaion of Target=1 in each set (training, validation, test)\n",
    "def calc_frac_target1(ids):\n",
    "    return sum([item[1] for item in ids]) / len(ids)\n",
    "\n",
    "tot_samples = len(train_ids) + len(val_ids) + len(test_ids)\n",
    "\n",
    "print(\"Train/Validate/Test Counts:\", len(train_ids), \"/\", len(val_ids), \"/\", len(test_ids))\n",
    "print(\"Train/Validate/Test Fractions:\", round(len(train_ids)/tot_samples,2), \"/\", round(len(val_ids)/tot_samples,2), \"/\", round(len(test_ids)/tot_samples,2))\n",
    "print(\"Proportion of Target = 1 in training data:\", calc_frac_target1(train_ids))\n",
    "print(\"Proportion of Target = 1 in validation data:\", calc_frac_target1(val_ids))\n",
    "print(\"Proportion of Target = 1 in test data:\", calc_frac_target1(test_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.9 - Limit training and validation data to speed up training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train ids length before: 230753\n",
      "Train ids length after: 2307\n",
      "Validate ids length before: 30891\n",
      "Validate ids length after: 308\n",
      "Test ids length before: 79053\n",
      "Test ids length after: 790\n"
     ]
    }
   ],
   "source": [
    "#Choose a portion of TRAINING ids to load into memory\n",
    "def take_fewer_samples(ids, frac_to_use, seed):\n",
    "    if frac_to_use < 1 and frac_to_use > 0:\n",
    "        random.seed(seed)\n",
    "        k = int(frac_to_use * len(ids))\n",
    "        ids_short = random.choices(ids, k=k)\n",
    "        return ids_short\n",
    "    return ids\n",
    "\n",
    "print(\"Train ids length before:\", len(train_ids))\n",
    "train_ids = take_fewer_samples(train_ids, train_frac_to_use, seed=12)\n",
    "print(\"Train ids length after:\", len(train_ids))\n",
    "\n",
    "print(\"Validate ids length before:\", len(val_ids))\n",
    "val_ids = take_fewer_samples(val_ids, val_frac_to_use, seed=12)\n",
    "print(\"Validate ids length after:\", len(val_ids))\n",
    "\n",
    "print(\"Test ids length before:\", len(test_ids))\n",
    "test_ids = take_fewer_samples(test_ids, test_frac_to_use, seed=12)\n",
    "print(\"Test ids length after:\", len(test_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Augmentation functions (used during dataset creation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hair_removal(image, crop_pixels=10):\n",
    "    height_pixels = len(image)  # Image rows\n",
    "    width_pixels = len(image[0])  # Image columns\n",
    "\n",
    "    # Image cropping\n",
    "    height = [crop_pixels, height_pixels - crop_pixels]\n",
    "    width = [crop_pixels, width_pixels - crop_pixels]\n",
    "    img = image[height[0]:height[1], width[0]:width[1]]\n",
    "\n",
    "    # Gray scale\n",
    "    grayScale = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    # Black hat filter\n",
    "    kernel = cv2.getStructuringElement(1, (9, 9))\n",
    "    blackhat = cv2.morphologyEx(grayScale, cv2.MORPH_BLACKHAT, kernel)\n",
    "    # Gaussian filter\n",
    "    bhg = cv2.GaussianBlur(blackhat, (3, 3), cv2.BORDER_DEFAULT)\n",
    "    # Binary thresholding (MASK)\n",
    "    ret, mask = cv2.threshold(bhg, 10, 255, cv2.THRESH_BINARY)\n",
    "    # Replace pixels of the mask\n",
    "    dst = cv2.inpaint(img, mask, 6, cv2.INPAINT_TELEA)\n",
    "\n",
    "    return dst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Define the augmentation function\\ndef augment_image(image, is_training=False):\\n    #Apply a series of augmentations to create diverse variations of the input image.\\n    #Includes random flips, rotations, brightness adjustments, and other transformations.\\n    # Apply various augmentations\\n\\n    # parameter\\n    if is_training==True:\\n        height_factor_cut=(0.02, 0.06),\\n        width_factor_cut=(0.02, 0.06),\\n        max_delta_brigth=0.25,\\n        lower_sat=0.7, \\n        upper_sat=1.8,\\n        lower_cont=0.7, \\n        upper_cont=1.8,\\n        minval_rot=0,\\n        maxvalue_rot=4\\n    else:\\n        height_factor_cut=(0, 0),\\n        width_factor_cut=(0, 0),\\n        max_delta_brigth=0.15,\\n        lower_sat=0.8, \\n        upper_sat=1.2,\\n        lower_cont=0.8, \\n        upper_cont=1.2,\\n        minval_rot=0,\\n        maxvalue_rot=0\\n        \\n    # RandomCutout initialization\\n    cutout_layer = keras_cv.layers.RandomCutout(height_factor=height_factor_cut, width_factor=width_factor_cut)\\n    \\n    # List of augmentations\\n    augmentations = [\\n        tf.image.random_flip_left_right,  \\n        tf.image.random_flip_up_down,   \\n        tf.image.random_brightness,      \\n        tf.image.random_contrast,         \\n        tf.image.random_saturation,\\n        lambda img: tf.image.rot90(img, tf.random.uniform(shape=[], minval=minval_rot, maxval=maxvalue_rot, dtype=tf.int32)),\\n        lambda img: cutout_layer(img) \\n    ]\\n\\n    # Shuffle and pick one augmentation\\n    augmentation = augmentations[tf.random.uniform(shape=[], minval=0, maxval=len(augmentations), dtype=tf.int32)]\\n    \\n    # Apply augmentation with 95% probability\\n    #if tf.random.uniform([]) < 0.95:\\n    # Apply augmentation \\n    if augmentation in [tf.image.random_flip_left_right, tf.image.random_flip_up_down]:\\n        image = augmentation(image) \\n    elif augmentation == tf.image.random_brightness:\\n        image = augmentation(image, max_delta=max_delta_brigth)  \\n    elif augmentation == tf.image.random_contrast:\\n        image = augmentation(image, lower_cont, upper_cont)\\n    elif augmentation == tf.image.random_saturation:\\n        image = augmentation(image, lower_sat, upper_sat)  \\n    else:\\n        image = augmentation(image)\\n    \\n    return image\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Define the augmentation function\n",
    "def augment_image(image, is_training=False):\n",
    "    #Apply a series of augmentations to create diverse variations of the input image.\n",
    "    #Includes random flips, rotations, brightness adjustments, and other transformations.\n",
    "    # Apply various augmentations\n",
    "\n",
    "    # parameter\n",
    "    if is_training==True:\n",
    "        height_factor_cut=(0.02, 0.06),\n",
    "        width_factor_cut=(0.02, 0.06),\n",
    "        max_delta_brigth=0.25,\n",
    "        lower_sat=0.7, \n",
    "        upper_sat=1.8,\n",
    "        lower_cont=0.7, \n",
    "        upper_cont=1.8,\n",
    "        minval_rot=0,\n",
    "        maxvalue_rot=4\n",
    "    else:\n",
    "        height_factor_cut=(0, 0),\n",
    "        width_factor_cut=(0, 0),\n",
    "        max_delta_brigth=0.15,\n",
    "        lower_sat=0.8, \n",
    "        upper_sat=1.2,\n",
    "        lower_cont=0.8, \n",
    "        upper_cont=1.2,\n",
    "        minval_rot=0,\n",
    "        maxvalue_rot=0\n",
    "        \n",
    "    # RandomCutout initialization\n",
    "    cutout_layer = keras_cv.layers.RandomCutout(height_factor=height_factor_cut, width_factor=width_factor_cut)\n",
    "    \n",
    "    # List of augmentations\n",
    "    augmentations = [\n",
    "        tf.image.random_flip_left_right,  \n",
    "        tf.image.random_flip_up_down,   \n",
    "        tf.image.random_brightness,      \n",
    "        tf.image.random_contrast,         \n",
    "        tf.image.random_saturation,\n",
    "        lambda img: tf.image.rot90(img, tf.random.uniform(shape=[], minval=minval_rot, maxval=maxvalue_rot, dtype=tf.int32)),\n",
    "        lambda img: cutout_layer(img) \n",
    "    ]\n",
    "\n",
    "    # Shuffle and pick one augmentation\n",
    "    augmentation = augmentations[tf.random.uniform(shape=[], minval=0, maxval=len(augmentations), dtype=tf.int32)]\n",
    "    \n",
    "    # Apply augmentation with 95% probability\n",
    "    #if tf.random.uniform([]) < 0.95:\n",
    "    # Apply augmentation \n",
    "    if augmentation in [tf.image.random_flip_left_right, tf.image.random_flip_up_down]:\n",
    "        image = augmentation(image) \n",
    "    elif augmentation == tf.image.random_brightness:\n",
    "        image = augmentation(image, max_delta=max_delta_brigth)  \n",
    "    elif augmentation == tf.image.random_contrast:\n",
    "        image = augmentation(image, lower_cont, upper_cont)\n",
    "    elif augmentation == tf.image.random_saturation:\n",
    "        image = augmentation(image, lower_sat, upper_sat)  \n",
    "    else:\n",
    "        image = augmentation(image)\n",
    "    \n",
    "    return image\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the augmentation function\n",
    "def augment_image(image, mod_toggle, is_training=False):\n",
    "    \"\"\"\n",
    "    Apply a series of augmentations to create diverse variations of the input image.\n",
    "    Includes random flips, rotations, brightness adjustments, and other transformations.\n",
    "    \"\"\"\n",
    "\n",
    "    # Parameters\n",
    "    if is_training==True:\n",
    "        #Cutout values\n",
    "        height_factor_cut=(0.02, 0.06)\n",
    "        width_factor_cut=(0.02, 0.06)\n",
    "        random_cutout = keras_cv.layers.RandomCutout(height_factor=height_factor_cut, width_factor=width_factor_cut)\n",
    "        #Brightness values\n",
    "        min_delta_bright=0.05\n",
    "        max_delta_bright=0.25\n",
    "        #Saturation values\n",
    "        weak_sat_low=0.7\n",
    "        weak_sat_high=0.95\n",
    "        strong_sat_low=1.05\n",
    "        strong_sat_high=1.8\n",
    "        #Contrast values\n",
    "        weak_cont_low=0.7\n",
    "        weak_cont_high=0.95\n",
    "        strong_cont_low=1.05\n",
    "        strong_cont_high=1.8\n",
    "\n",
    "    else:\n",
    "        #Brightness values\n",
    "        min_delta_bright=0.05\n",
    "        max_delta_bright=0.15\n",
    "        #Saturation values\n",
    "        weak_sat_low=0.8\n",
    "        weak_sat_high=0.95\n",
    "        strong_sat_low=1.05\n",
    "        strong_sat_high=1.2\n",
    "        #Contrast values\n",
    "        weak_cont_low=0.8\n",
    "        weak_cont_high=0.95\n",
    "        strong_cont_low=1.05\n",
    "        strong_cont_high=1.2\n",
    "\n",
    "    #List of base augmentations\n",
    "    base_augments = [\n",
    "        lambda img: tf.image.flip_left_right(img),\n",
    "        lambda img: tf.image.flip_up_down(img),\n",
    "        lambda img: tf.image.rot90(img, k=1),\n",
    "        lambda img: tf.image.rot90(img, k=2),\n",
    "        lambda img: tf.image.rot90(img, k=3),\n",
    "        lambda img: random_cutout(img)\n",
    "    ]\n",
    "\n",
    "    #List of other augmentations that can be performed multiple times\n",
    "    other_augments = [\n",
    "        lambda img: tf.image.adjust_brightness(img, -random.uniform(min_delta_bright, max_delta_bright)),\n",
    "        lambda img: tf.image.adjust_brightness(img, random.uniform(min_delta_bright, max_delta_bright)),\n",
    "        lambda img: tf.image.random_contrast(img, lower=weak_cont_low, upper=weak_cont_high),\n",
    "        lambda img: tf.image.random_contrast(img, lower=strong_cont_low, upper=strong_cont_high),\n",
    "        lambda img: tf.image.random_saturation(img, lower=weak_sat_low, upper=weak_sat_high),\n",
    "        lambda img: tf.image.random_saturation(img, lower=strong_sat_low, upper=strong_sat_high)\n",
    "    ]\n",
    "\n",
    "    #Select augmentations to use based on whether it is training data or not\n",
    "    if is_training:\n",
    "        base_augments = base_augments\n",
    "        other_augments = other_augments \n",
    "    else:\n",
    "        base_selection = [0,1]  #Positions of the augmentations to use in the base_augments list\n",
    "        base_augments = [base_augments[i] for i in base_selection]\n",
    "        other_augments = other_augments\n",
    "\n",
    "    #Engage the augment based on the mod_toggle. The base_augments are always done first.\n",
    "    nb_base_aug = len(base_augments)\n",
    "    \n",
    "    if mod_toggle <= nb_base_aug and mod_toggle > 0:\n",
    "        augmentation = base_augments[mod_toggle - 1] #Mod toggles start at 1\n",
    "        image = augmentation(image)\n",
    "    else:\n",
    "        augmentation = random.choice(other_augments)\n",
    "        image = augmentation(image)\n",
    "        #Apply a second transformation with a certain probability\n",
    "        if tf.random.uniform([]) < 0.85:\n",
    "            augmentation2 = random.choice(base_augments)\n",
    "            image = augmentation2(image)\n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nid_try = \"ISIC_5675460\"\\nwith h5py.File(hdf5_file, \\'r\\') as h5file:\\n    img_try = np.array(Image.open(io.BytesIO(h5file[id_try][()])))\\n\\nfor mod_toggle in range(500):\\n    img = hair_removal(img_try)\\n    img = cv2.resize(img, (100, 100), interpolation= cv2.INTER_AREA)\\n    augment_image(img, mod_toggle, is_training=True)\\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TESTING OF AUGMENT_IMAGE FUNCTION\n",
    "\"\"\"\n",
    "id_try = \"ISIC_5675460\"\n",
    "with h5py.File(hdf5_file, 'r') as h5file:\n",
    "    img_try = np.array(Image.open(io.BytesIO(h5file[id_try][()])))\n",
    "\n",
    "for mod_toggle in range(500):\n",
    "    img = hair_removal(img_try)\n",
    "    img = cv2.resize(img, (100, 100), interpolation= cv2.INTER_AREA)\n",
    "    augment_image(img, mod_toggle, is_training=True)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nid_try = \"ISIC_5675460\"\\nwith h5py.File(hdf5_file, \\'r\\') as h5file:\\n    img_try = np.array(Image.open(io.BytesIO(h5file[id_try][()])))\\n\\nheight_factor_cut=(0.02, 0.06)\\nwidth_factor_cut=(0.02, 0.06)\\nrandom_cutout = keras_cv.layers.RandomCutout(height_factor=height_factor_cut, width_factor=width_factor_cut)\\n\\nbase_augments = [\\n    lambda img: img,\\n    lambda img: tf.image.flip_left_right(img),\\n    lambda img: tf.image.flip_up_down(img),\\n    lambda img: tf.image.rot90(img, k=1),\\n    lambda img: tf.image.rot90(img, k=2),\\n    lambda img: tf.image.rot90(img, k=3),\\n    lambda img: random_cutout(img).numpy().astype(int),\\n]\\n\\nimages = []\\nfor augmentation in base_augments:\\n    image = augmentation(img_try)\\n    images.append(image)\\n\\n#Show the images\\nfor image in images:\\n    plt.imshow(image, interpolation=None)\\n    plt.grid(None)\\n    plt.show()\\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TESTING OF BASE AUGMENTS\n",
    "\"\"\"\n",
    "id_try = \"ISIC_5675460\"\n",
    "with h5py.File(hdf5_file, 'r') as h5file:\n",
    "    img_try = np.array(Image.open(io.BytesIO(h5file[id_try][()])))\n",
    "\n",
    "height_factor_cut=(0.02, 0.06)\n",
    "width_factor_cut=(0.02, 0.06)\n",
    "random_cutout = keras_cv.layers.RandomCutout(height_factor=height_factor_cut, width_factor=width_factor_cut)\n",
    "\n",
    "base_augments = [\n",
    "    lambda img: img,\n",
    "    lambda img: tf.image.flip_left_right(img),\n",
    "    lambda img: tf.image.flip_up_down(img),\n",
    "    lambda img: tf.image.rot90(img, k=1),\n",
    "    lambda img: tf.image.rot90(img, k=2),\n",
    "    lambda img: tf.image.rot90(img, k=3),\n",
    "    lambda img: random_cutout(img).numpy().astype(int),\n",
    "]\n",
    "\n",
    "images = []\n",
    "for augmentation in base_augments:\n",
    "    image = augmentation(img_try)\n",
    "    images.append(image)\n",
    "\n",
    "#Show the images\n",
    "for image in images:\n",
    "    plt.imshow(image, interpolation=None)\n",
    "    plt.grid(None)\n",
    "    plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nid_try = \"ISIC_5675460\"\\nwith h5py.File(hdf5_file, \\'r\\') as h5file:\\n    img_try = np.array(Image.open(io.BytesIO(h5file[id_try][()])))\\n\\n#Brightness values\\nmin_delta_bright=0.05\\nmax_delta_bright=0.25\\n#Saturation values\\nweak_sat_low=0.7\\nweak_sat_high=0.95\\nstrong_sat_low=1.05\\nstrong_sat_high=1.8\\n#Contrast values\\nweak_cont_low=0.7\\nweak_cont_high=0.95\\nstrong_cont_low=1.05\\nstrong_cont_high=1.8\\n\\n#List of other augmentations that can be performed multiple times\\nother_augments = [\\n    lambda img: tf.image.adjust_brightness(img, -random.uniform(min_delta_bright, max_delta_bright)),\\n    lambda img: tf.image.adjust_brightness(img, random.uniform(min_delta_bright, max_delta_bright)),\\n    lambda img: tf.image.random_contrast(img, lower=weak_cont_low, upper=weak_cont_high),\\n    lambda img: tf.image.random_contrast(img, lower=strong_cont_low, upper=strong_cont_high),\\n    lambda img: tf.image.random_saturation(img, lower=weak_sat_low, upper=weak_sat_high),\\n    lambda img: tf.image.random_saturation(img, lower=strong_sat_low, upper=strong_sat_high)\\n]\\n\\nimages = []\\nfor augmentation in other_augments:\\n    image = augmentation(img_try)\\n    if tf.random.uniform([]) < 1:\\n        augmentation2 = random.choice(base_augments)\\n        image = augmentation2(image)\\n    images.append(image)\\n\\n#Show the images\\nfor image in images:\\n    plt.imshow(image, interpolation=None)\\n    plt.grid(None)\\n    plt.show()\\n'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TESTING OF OTHER AUGMENTS\n",
    "\"\"\"\n",
    "id_try = \"ISIC_5675460\"\n",
    "with h5py.File(hdf5_file, 'r') as h5file:\n",
    "    img_try = np.array(Image.open(io.BytesIO(h5file[id_try][()])))\n",
    "\n",
    "#Brightness values\n",
    "min_delta_bright=0.05\n",
    "max_delta_bright=0.25\n",
    "#Saturation values\n",
    "weak_sat_low=0.7\n",
    "weak_sat_high=0.95\n",
    "strong_sat_low=1.05\n",
    "strong_sat_high=1.8\n",
    "#Contrast values\n",
    "weak_cont_low=0.7\n",
    "weak_cont_high=0.95\n",
    "strong_cont_low=1.05\n",
    "strong_cont_high=1.8\n",
    "\n",
    "#List of other augmentations that can be performed multiple times\n",
    "other_augments = [\n",
    "    lambda img: tf.image.adjust_brightness(img, -random.uniform(min_delta_bright, max_delta_bright)),\n",
    "    lambda img: tf.image.adjust_brightness(img, random.uniform(min_delta_bright, max_delta_bright)),\n",
    "    lambda img: tf.image.random_contrast(img, lower=weak_cont_low, upper=weak_cont_high),\n",
    "    lambda img: tf.image.random_contrast(img, lower=strong_cont_low, upper=strong_cont_high),\n",
    "    lambda img: tf.image.random_saturation(img, lower=weak_sat_low, upper=weak_sat_high),\n",
    "    lambda img: tf.image.random_saturation(img, lower=strong_sat_low, upper=strong_sat_high)\n",
    "]\n",
    "\n",
    "images = []\n",
    "for augmentation in other_augments:\n",
    "    image = augmentation(img_try)\n",
    "    if tf.random.uniform([]) < 1:\n",
    "        augmentation2 = random.choice(base_augments)\n",
    "        image = augmentation2(image)\n",
    "    images.append(image)\n",
    "\n",
    "#Show the images\n",
    "for image in images:\n",
    "    plt.imshow(image, interpolation=None)\n",
    "    plt.grid(None)\n",
    "    plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Metadata Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a metadata dictionary for efficient lookup\n",
    "#Structure: {index: value} where value is (mod_toggle, metadata array)\n",
    "\n",
    "#Objective: We need \"train_ids\" to efficiently retrieve and augment images from the HDF5 file. This already exists.\n",
    "#           We need to accesss metadata through a dictionary to improve speed. A common reference is needed for both.\n",
    "\n",
    "#Idea:      Since train_ids is a LIST of tuples (isic_id, target, mod toggle), it is accessed via indices (ex. train_ids[10]).\n",
    "#           We need to make a dictionary that, for each train_ids index, lists all the metadata associated to the isic_id.\n",
    "#           Thus, when train_ids[10] is called, we call the dictionary and request key=10 to get the metadata.\n",
    "#           Result: very fast data retrieval\n",
    "\n",
    "def make_meta_dict(metadata, isic_ids_tuple):\n",
    "    #Reindex. The metadata must be contiguously indexed. Holes in index numbering will not work.\n",
    "    metadata = metadata.reset_index(drop=True)\n",
    "\n",
    "    #Get the column number for \"isic_id\" and \"target\" in the metadata dataframe.\n",
    "    #This allows us to know where these items are located in the metadata retrieved for each item.\n",
    "    #This is used later to filter out these items from the metadata.\n",
    "    col_num_id = metadata.columns.get_loc(\"isic_id\")\n",
    "    col_num_target = metadata.columns.get_loc(\"target\")\n",
    "\n",
    "    #The metadata contains all unique isic_ids. Since the dataframe is reindexed, it is possible to make\n",
    "    #a dictionary of (isic_id: row number) for fast retrieval of all metadata associated to an isic_id.\n",
    "\n",
    "    #Take the metadata dataframe and create a dictionary that stores (isic_id, row number).\n",
    "    metadata_index_dict = metadata[\"isic_id\"].to_dict()\n",
    "    metadata_index_dict = dict((v, k) for k, v in metadata_index_dict.items())\n",
    "\n",
    "    #Make a dictionary of (index: (mod toggle, metadata)), where index is the position of a sample in train_ids and metadata is the metadata\n",
    "    #associated to the sample's isic_id. We thus create a dict_of_meta that is a mirror image of the \"isic_ids_tuple\" list.\n",
    "    #We must be careful to not shuffle \"isic_ids_tuple\".\n",
    "    dict_of_meta = {}\n",
    "    for pos, tup in enumerate(isic_ids_tuple):\n",
    "        # Use the lookup table to directly find the index\n",
    "        index = metadata_index_dict.get(tup[0], -1)  # -1 if not found\n",
    "\n",
    "        if index != -1:\n",
    "            # Access the row directly without masking\n",
    "            dict_of_meta.update({pos: (tup[2], np.array(metadata.iloc[index].values))})\n",
    "            # Process the row as needed\n",
    "        else:\n",
    "            raise Exception(\"isic_id values are not all unique\")\n",
    "    return dict_of_meta, col_num_id, col_num_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the metadata dictionaries for train-validate-test\n",
    "train_meta_dict, train_pos_isic_id, train_pos_target = make_meta_dict(metadata, train_ids)\n",
    "val_meta_dict, val_pos_isic_id, val_pos_target = make_meta_dict(metadata, val_ids)\n",
    "test_meta_dict, test_pos_isic_id, test_pos_target = make_meta_dict(metadata, test_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAMPLE OF ITEMS FROM VALIDATION META DICTIONARY\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: (-1,\n",
       "  array(['ISIC_0897475', 65.0, 0, 3.46, 4.691312, 18.14697, 0.8424182,\n",
       "         1.827323, 0.6924084, 6.596266, 1.35737, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=object)),\n",
       " 1: (-1,\n",
       "  array(['ISIC_0484254', 55.0, 0, 3.03, 4.391068, 21.19606, 0.778765,\n",
       "         2.109872, 0.9747766, 5.343132, 0.8539952, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0], dtype=object)),\n",
       " 2: (-1,\n",
       "  array(['ISIC_1655105', 65.0, 0, 3.91, 6.00487896415838, 21.1067857006202,\n",
       "         0.885269227600192, 2.06409153872973, 0.840973837855876,\n",
       "         5.6532749542864, 0.960669349052525, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=object))}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Look some dictionary elements to understand the structure {index, (mod_toggle, metadata)}\n",
    "print(\"SAMPLE OF ITEMS FROM VALIDATION META DICTIONARY\")\n",
    "dict(filter(lambda item: item[0] in {0, 1, 2}, val_meta_dict.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Dataset generation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMAGE generator in a class\n",
    "class hdf5_generator_all_included(Generator):\n",
    "    def __init__(self, file, meta_dict, dict_pos_isic_id, dict_pos_target, num_features, imgSize, is_training=False, shuffle_seed=None, apply_hair_removal=False):\n",
    "        self.file = file\n",
    "        self.meta_dict = meta_dict\n",
    "        self.pos_mod_toggle = 0                     #Position of 'mod_toggle' within dictionary: structure {key: value} where value is (mod_toggle, metadata array)\n",
    "        self.pos_metadata_array = 1                 #Position of 'metadata array' within dictionary: structure {key: value} where value is (mod_toggle, metadata array)\n",
    "        self.dict_pos_isic_id = dict_pos_isic_id    #Set position of 'isic_id\" within the metadata array: structure [var0, var1, var2, var3, ...]\n",
    "        self.dict_pos_target = dict_pos_target      #Set position of 'target\" within the metadata array: structure [var0, var1, var2, var3, ...]\n",
    "        self.num_features = num_features\n",
    "        self.imgSize = imgSize\n",
    "        self.is_training = is_training\n",
    "        self.shuffle_seed = shuffle_seed\n",
    "        self.apply_hair_removal = apply_hair_removal\n",
    "        self.len = len(meta_dict)\n",
    "        self.start = 0\n",
    "        self.stop = self.len\n",
    "        self.i = self.start\n",
    "        self.error_check()\n",
    "        self.open_hdf5()\n",
    "        self.order_and_shuffle()\n",
    "        \n",
    "    def send(self, value):\n",
    "        if self.i < self.stop:\n",
    "            if self.i == self.start:\n",
    "                self.open_hdf5()\n",
    "\n",
    "            #Retrieve index of isic_id according to the shuffled order\n",
    "            index = self.order[self.i]\n",
    "\n",
    "            #Retrieve target... remember that each item of the the meta_dict is a tuple of (mod toggle, metadata)\n",
    "            target = self.meta_dict[index][self.pos_metadata_array][self.dict_pos_target]\n",
    "            target = np.reshape(target, (1,1))\n",
    "            target = tf.cast(target, dtype=tf.int32)\n",
    "\n",
    "            #Retrieve metadata... remember that each item of the the meta_dict is a tuple of (mod toggle, metadata)\n",
    "            meta = np.delete(self.meta_dict[index][self.pos_metadata_array], [self.dict_pos_isic_id, self.dict_pos_target], 0)\n",
    "            meta = meta.astype(dtype=float)\n",
    "            meta = tf.cast(meta, dtype=tf.float32)\n",
    "            meta = tf.reshape(meta, shape=(1, self.num_features))\n",
    "\n",
    "            try:\n",
    "                #Retrieve isic_id\n",
    "                img_name = self.meta_dict[index][self.pos_metadata_array][self.dict_pos_isic_id]\n",
    "                \n",
    "                # Load image data from HDF5\n",
    "                img = np.array(Image.open(io.BytesIO(self.h5file[img_name][()])))\n",
    "\n",
    "                # Clean image\n",
    "                if self.apply_hair_removal:\n",
    "                    img = hair_removal(img)\n",
    "\n",
    "                # Resize the image\n",
    "                img = cv2.resize(img, (self.imgSize, self.imgSize), interpolation= cv2.INTER_AREA)\n",
    "\n",
    "                # Apply augmentations if needed\n",
    "                mod_toggle = self.meta_dict[index][self.pos_mod_toggle]\n",
    "                if mod_toggle > 0:\n",
    "                    img=augment_image(img, mod_toggle, self.is_training)\n",
    "                    \n",
    "                # Standardize and return as TensorFlow constant\n",
    "                img = tf.constant(img / 255, dtype=tf.float32)\n",
    "                \n",
    "                #Augment counter\n",
    "                self.i = self.i + 1\n",
    "                return (img, meta), target\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error loading image {img_name}: {e}\")\n",
    "                # log the error to a file for later analysis\n",
    "                with open('image_errors.log', 'a') as f:\n",
    "                    f.write(f\"Error loading image {img_name}: {e}\\n\")\n",
    "\n",
    "            if self.i == self.stop:\n",
    "                self.h5file.close()\n",
    "        raise StopIteration\n",
    "\n",
    "    def throw(self, typ, val=None, tb=None):\n",
    "        #Close HDF5 file and terminate generator\n",
    "        try:\n",
    "            self.h5file.close()\n",
    "            super().throw(typ, val, tb)\n",
    "        except:\n",
    "            super().throw(typ, val, tb)\n",
    "\n",
    "    def error_check(self):\n",
    "        #Seed type check\n",
    "        try:\n",
    "            int(self.shuffle_seed) == self.shuffle_seed\n",
    "        except:\n",
    "            if self.shuffle_seed != None:\n",
    "                raise Exception(\"Seed must either be an integer or None\")\n",
    "\n",
    "    def order_and_shuffle(self):\n",
    "        np.random.seed(self.shuffle_seed)\n",
    "        self.order = np.array(list(self.meta_dict.keys()), dtype=int)\n",
    "        if self.shuffle_seed != None:\n",
    "            np.random.shuffle(self.order)\n",
    "\n",
    "    def open_hdf5(self):\n",
    "        self.h5file = h5py.File(self.file, 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(hdf5_file, meta_dict, dict_pos_isic_id, dict_pos_target, imgSize=100, batch_size=32, is_training=False, shuffle_seed = None, apply_hair_removal=False):\n",
    "    num_features = len(val_meta_dict[0][1]) - 2 #Subtract isic_id and target\n",
    "\n",
    "    combined_generator = hdf5_generator_all_included(hdf5_file, meta_dict, dict_pos_isic_id, dict_pos_target, num_features, imgSize, is_training, shuffle_seed, apply_hair_removal)\n",
    "\n",
    "    # Generate image dataset\n",
    "    element_spec = ((tf.TensorSpec(shape=(imgSize, imgSize, 3), dtype=tf.float32),\n",
    "                    tf.TensorSpec(shape=(1, num_features), dtype=tf.float32)),\n",
    "                    tf.TensorSpec(shape=(1, 1), dtype=tf.int32))\n",
    "\n",
    "\n",
    "    img_dataset = tf.data.Dataset.from_generator(\n",
    "        lambda: combined_generator,\n",
    "        output_signature=element_spec\n",
    "    )\n",
    "\n",
    "    return img_dataset.batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) Alternative dataset function to load all in memory (not a generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_in_memory_dataset(hdf5_file, meta_dict, dict_pos_isic_id, dict_pos_target, imgSize=100, batch_size=32, is_training=False, shuffle_seed=None, apply_hair_removal=False):\n",
    "    #Position of elements in the value part of the dictionary (it is a tuple with multiple elements)\n",
    "    pos_mod_toggle = 0\n",
    "    pos_metadata_array = 1\n",
    "\n",
    "    num_features = len(meta_dict[0][pos_metadata_array]) - 2  # Subtract isic_id and target columns\n",
    "\n",
    "    # Initialize lists to hold images, metadata, and targets\n",
    "    images = []\n",
    "    metas = []\n",
    "    targets = []\n",
    "\n",
    "    np.random.seed(shuffle_seed)\n",
    "    order = np.array(list(meta_dict.keys()), dtype=int)\n",
    "    if shuffle_seed is not None:\n",
    "        np.random.shuffle(order)\n",
    "\n",
    "    with h5py.File(hdf5_file, 'r') as h5file:\n",
    "        for i in range(len(meta_dict)):\n",
    "            index = order[i]\n",
    "            \n",
    "            # Retrieve target\n",
    "            target = meta_dict[index][pos_metadata_array][dict_pos_target]\n",
    "            target = np.reshape(target, (1, 1))\n",
    "            target = tf.cast(target, dtype=tf.int32)\n",
    "\n",
    "            # Retrieve metadata\n",
    "            meta = np.delete(meta_dict[index][pos_metadata_array], [dict_pos_isic_id, dict_pos_target], 0)\n",
    "            meta = meta.astype(dtype=float)\n",
    "            meta = tf.cast(meta, dtype=tf.float32)\n",
    "            meta = tf.reshape(meta, shape=(1, num_features))\n",
    "\n",
    "            try:\n",
    "                # Retrieve isic_id and load image\n",
    "                img_name = meta_dict[index][pos_metadata_array][dict_pos_isic_id]\n",
    "                \n",
    "                # Load image data from HDF5\n",
    "                img = np.array(Image.open(io.BytesIO(h5file[img_name][()])))\n",
    "\n",
    "                # Clean image\n",
    "                if apply_hair_removal:\n",
    "                    img = hair_removal(img)\n",
    "\n",
    "                # Resize the image\n",
    "                img = cv2.resize(img, (imgSize, imgSize), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "                # Apply augmentations if needed\n",
    "                mod_toggle = meta_dict[index][pos_mod_toggle]\n",
    "                if mod_toggle > 0:\n",
    "                    img=augment_image(img, mod_toggle, is_training)\n",
    "\n",
    "                # Normalize the image\n",
    "                img = tf.constant(img / 255, dtype=tf.float32)\n",
    "\n",
    "                # Add processed data to lists\n",
    "                images.append(img)\n",
    "                metas.append(meta)\n",
    "                targets.append(target)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading image {img_name}: {e}\")\n",
    "                with open('image_errors.log', 'a') as f:\n",
    "                    f.write(f\"Error loading image {img_name}: {e}\\n\")\n",
    "                continue\n",
    "\n",
    "    # Convert lists to tensors\n",
    "    images_tensor = tf.stack(images, axis=0)\n",
    "    metas_tensor = tf.stack(metas, axis=0)\n",
    "    targets_tensor = tf.stack(targets, axis=0)\n",
    "\n",
    "    # Combine the images, metadata, and targets into a tf.data.Dataset\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(((images_tensor, metas_tensor), targets_tensor))\n",
    "\n",
    "    # Batch and prefetch the dataset\n",
    "    return dataset.batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8) CNN MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 - Model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simple CNN model using only images and target (FOR TESTING)\n",
    "class CNN_model(tf.keras.Model):\n",
    "    def __init__(self, neurons = 8, activ = 'leaky_relu', img_size = 100, img_channels=3):\n",
    "        #Run the constructor of the parent class\n",
    "        super().__init__()\n",
    "\n",
    "        #Weight and bias initializers\n",
    "        kernel_initializer = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.05, seed=None)\n",
    "        bias_initializer = tf.keras.initializers.RandomUniform(minval=-0.05, maxval=0.05, seed=None)\n",
    "        \n",
    "        #Image size declaration\n",
    "        self.img_size = img_size\n",
    "        self.img_channels = img_channels\n",
    "\n",
    "        #Layers\n",
    "        self.conv1 = tf.keras.layers.Conv2D(filters=16, kernel_size=5, strides=(1, 1), activation='relu', padding='same', input_shape=(img_size, img_size, img_channels),\n",
    "                                            kernel_initializer=kernel_initializer, bias_initializer=bias_initializer)\n",
    "        self.pool1 = tf.keras.layers.MaxPool2D(pool_size=(2,2))\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.dense1 = tf.keras.layers.Dense(neurons, activation = activ, kernel_initializer=kernel_initializer, bias_initializer=bias_initializer)\n",
    "        self.dense2 = tf.keras.layers.Dense(1, activation='sigmoid', kernel_initializer=kernel_initializer, bias_initializer=bias_initializer)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x_image, x_meta = inputs\n",
    "\n",
    "        # Convolutions\n",
    "        x1 = self.conv1(x_image)\n",
    "        x1 = self.pool1(x1)\n",
    "\n",
    "        # Flattening of images for input layer\n",
    "        x1 = self.flatten(x1)\n",
    "\n",
    "        # Hidden layers of neural network\n",
    "        x1 = self.dense1(x1)\n",
    "\n",
    "        # Output layer of neural network\n",
    "        output = self.dense2(x1)\n",
    "\n",
    "        return output \n",
    "\n",
    "#Metadata Neural Network (FOR TESTING)\n",
    "class Meta_model(tf.keras.Model):\n",
    "    def __init__(self, neurons = 8, activ = 'tanh',**kwargs):\n",
    "        if kwargs:  \n",
    "            self.name=kwargs['name'] \n",
    "            \n",
    "        #Run the constructor of the parent class\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        #Weight and bias initializers\n",
    "        kernel_initializer = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.05, seed=None)\n",
    "        bias_initializer = tf.keras.initializers.RandomUniform(minval=-0.05, maxval=0.05, seed=None)\n",
    "\n",
    "        #Layers\n",
    "        self.dense1 = tf.keras.layers.Dense(neurons, activation = activ, kernel_initializer=kernel_initializer, bias_initializer=bias_initializer)\n",
    "        self.dense2 = tf.keras.layers.Dense(neurons, activation = activ, kernel_initializer=kernel_initializer, bias_initializer=bias_initializer)\n",
    "        self.dense3 = tf.keras.layers.Dense(1, activation='sigmoid', kernel_initializer=kernel_initializer, bias_initializer=bias_initializer)\n",
    "        self.dropout = tf.keras.layers.Dropout(0.25)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        x_image, x_meta = inputs\n",
    "        x_all = tf.reshape(x_meta, (tf.shape(x_meta)[0], x_meta.shape[-1]))\n",
    "        # Neural Network\n",
    "        x_all = self.dense1(x_all)\n",
    "        x_all = self.dense2(x_all)\n",
    "        if training:\n",
    "            x_all = self.dropout(x_all, training=training)\n",
    "        output = self.dense3(x_all)\n",
    "        return output\n",
    "\n",
    "#Hybrid CNN model taking metadata (FULL MODEL)\n",
    "@tf.keras.utils.register_keras_serializable(package=\"MyLayers\", name=\"KernelMult\")\n",
    "class Hybrid_model(tf.keras.Model):\n",
    "    def __init__(self, neurons = 8, dropout = 0, activ = 'leaky_relu', img_size = 100, img_channels = 3, **kwargs):\n",
    "        #Run the constructor of the parent class\n",
    "        super(). __init__(**kwargs)\n",
    "        \n",
    "        #Save inputs\n",
    "        self.neurons = neurons\n",
    "        self.dropout = dropout\n",
    "        self.activ = activ\n",
    "        self.img_size = img_size\n",
    "        self.img_channels = img_channels\n",
    "\n",
    "        #Weight and bias initializers\n",
    "        self.kernel_initializer = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.05, seed=None)\n",
    "        self.bias_initializer = tf.keras.initializers.RandomUniform(minval=-0.05, maxval=0.05, seed=None)\n",
    "\n",
    "        #Layers\n",
    "        self.conv1 = tf.keras.layers.Conv2D(filters=32, kernel_size=5, strides=(1, 1), activation='relu', padding='same', input_shape=(self.img_size, self.img_size, self.img_channels),\n",
    "                                            kernel_initializer=self.kernel_initializer, bias_initializer=self.bias_initializer)\n",
    "        self.conv2 = tf.keras.layers.Conv2D(64, 5, activation='relu', kernel_initializer=self.kernel_initializer, bias_initializer=self.bias_initializer)\n",
    "        self.pool = tf.keras.layers.MaxPool2D(pool_size=(2,2))\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.dense1 = tf.keras.layers.Dense(self.neurons, activation = self.activ, kernel_initializer=self.kernel_initializer, bias_initializer=self.bias_initializer)\n",
    "        self.dropout1 = tf.keras.layers.Dropout(self.dropout)\n",
    "        self.dense2 = tf.keras.layers.Dense(self.neurons, activation = self.activ, kernel_initializer=self.kernel_initializer, bias_initializer=self.bias_initializer)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(self.dropout)\n",
    "        self.dense3 = tf.keras.layers.Dense(1, activation='sigmoid', kernel_initializer=self.kernel_initializer, bias_initializer=self.bias_initializer)\n",
    "        self.concatenate = keras.layers.Concatenate(axis=1)\n",
    "        \n",
    "    def call(self, inputs, training=False):\n",
    "        flattened_inputs = tf.nest.flatten(inputs)\n",
    "        x_image, x_meta = flattened_inputs\n",
    "        # Convolutions\n",
    "        x = self.conv1(x_image)\n",
    "        x = self.pool(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.pool(x)\n",
    "        # Flattening of images and concatenation with other data\n",
    "        x = self.flatten(x)\n",
    "        # Reshape metadata to match dimensions\n",
    "        #x_meta = tf.reshape(x_meta, (tf.shape(x_meta)[0], x_meta.shape[-1]))\n",
    "        x_meta = keras.layers.Reshape(target_shape=([x_meta.shape[-1]]))(x_meta)\n",
    "        # Concatenate image and metadata\n",
    "        x_all = self.concatenate([x, x_meta])\n",
    "        # Neural Network\n",
    "        x_all = self.dense1(x_all)\n",
    "        if training:\n",
    "            x_all = self.dropout1(x_all, training=training)\n",
    "        x_all = self.dense2(x_all)\n",
    "        if training:\n",
    "            x_all = self.dropout2(x_all, training=training)\n",
    "        output = self.dense3(x_all)\n",
    "        return output\n",
    "           \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            'conv1' : self.conv1,\n",
    "            'conv2' : self.conv2,\n",
    "            'pool' : self.pool,\n",
    "            'flatten' : self.flatten,\n",
    "            'dense1' : self.dense1,\n",
    "            'dropout1' : self.dropout1,\n",
    "            'dense2' : self.dense2,\n",
    "            'dropout2' : self.dropout2,\n",
    "            'dense3' : self.dense3,\n",
    "            'concatenate' : self.concatenate,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        config[\"conv1\"] = keras.layers.deserialize(config[\"conv1\"])\n",
    "        config[\"conv2\"] = keras.layers.deserialize(config[\"conv2\"])\n",
    "        config[\"pool\"] = keras.layers.deserialize(config[\"pool\"])\n",
    "        config[\"flatten\"] = keras.layers.deserialize(config[\"flatten\"])\n",
    "        config[\"dense1\"] = keras.layers.deserialize(config[\"dense1\"])\n",
    "        config[\"dropout1\"] = keras.layers.deserialize(config[\"dropout1\"])\n",
    "        config[\"dense2\"] = keras.layers.deserialize(config[\"dense2\"])\n",
    "        config[\"dropout2\"] = keras.layers.deserialize(config[\"dropout2\"])\n",
    "        config[\"dense3\"] = keras.layers.deserialize(config[\"dense3\"])\n",
    "        config[\"concatenate\"] = keras.layers.deserialize(config[\"concatenate\"])\n",
    "        return cls(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n#MANUAL DEFINITION OF LAYERS - ATTEMPT TO BE ABLE TO EXPORT MODEL - SLOWER!!!\\nfrom keras import Model\\nkernel_initializer = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.05, seed=None)\\nbias_initializer = tf.keras.initializers.RandomUniform(minval=-0.05, maxval=0.05, seed=None)\\n\\ninput_img = keras.layers.Input(shape=(100,100,3))\\nx = keras.layers.Conv2D(filters=32, kernel_size=5, strides=(1, 1), activation='relu', padding='same', input_shape=(100, 100, 3), kernel_initializer=kernel_initializer, bias_initializer=bias_initializer)(input_img)\\nx = keras.layers.Conv2D(64, 5, activation='relu', kernel_initializer=kernel_initializer, bias_initializer=bias_initializer)(x)\\nx = keras.layers.MaxPool2D(pool_size=(2,2))(x)\\nx = keras.layers.Flatten()(x)\\nx = Model(inputs=input_img, outputs=x)\\n\\ninput_meta = keras.layers.Input(shape=(1,29))\\nx_meta = keras.layers.Reshape(target_shape=([29]))(input_meta)\\n#x_meta = tf.reshape(input_meta, (tf.shape(input_meta)[0], input_meta.shape[-1]))\\nx_meta = Model(inputs=input_meta, outputs=x_meta)\\n\\ncombined = keras.layers.Concatenate(axis=1)([x.output, x_meta.output])\\n\\nx_all = keras.layers.Dense(8, activation = 'leaky_relu', kernel_initializer=kernel_initializer, bias_initializer=bias_initializer)(combined)\\nx_all = tf.keras.layers.Dropout(0.1)(x_all)\\nx_all = tf.keras.layers.Dense(8, activation = 'leaky_relu', kernel_initializer=kernel_initializer, bias_initializer=bias_initializer)(x_all)\\nx_all = tf.keras.layers.Dropout(0.1)(x_all)\\nx_all = tf.keras.layers.Dense(1, activation='sigmoid', kernel_initializer=kernel_initializer, bias_initializer=bias_initializer)(x_all)\\nmodel = Model(inputs=[input_img, input_meta], outputs=x_all)\\n\""
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "#MANUAL DEFINITION OF LAYERS - ATTEMPT TO BE ABLE TO EXPORT MODEL - SLOWER!!!\n",
    "from keras import Model\n",
    "kernel_initializer = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.05, seed=None)\n",
    "bias_initializer = tf.keras.initializers.RandomUniform(minval=-0.05, maxval=0.05, seed=None)\n",
    "\n",
    "input_img = keras.layers.Input(shape=(100,100,3))\n",
    "x = keras.layers.Conv2D(filters=32, kernel_size=5, strides=(1, 1), activation='relu', padding='same', input_shape=(100, 100, 3), kernel_initializer=kernel_initializer, bias_initializer=bias_initializer)(input_img)\n",
    "x = keras.layers.Conv2D(64, 5, activation='relu', kernel_initializer=kernel_initializer, bias_initializer=bias_initializer)(x)\n",
    "x = keras.layers.MaxPool2D(pool_size=(2,2))(x)\n",
    "x = keras.layers.Flatten()(x)\n",
    "x = Model(inputs=input_img, outputs=x)\n",
    "\n",
    "input_meta = keras.layers.Input(shape=(1,29))\n",
    "x_meta = keras.layers.Reshape(target_shape=([29]))(input_meta)\n",
    "#x_meta = tf.reshape(input_meta, (tf.shape(input_meta)[0], input_meta.shape[-1]))\n",
    "x_meta = Model(inputs=input_meta, outputs=x_meta)\n",
    "\n",
    "combined = keras.layers.Concatenate(axis=1)([x.output, x_meta.output])\n",
    "\n",
    "x_all = keras.layers.Dense(8, activation = 'leaky_relu', kernel_initializer=kernel_initializer, bias_initializer=bias_initializer)(combined)\n",
    "x_all = tf.keras.layers.Dropout(0.1)(x_all)\n",
    "x_all = tf.keras.layers.Dense(8, activation = 'leaky_relu', kernel_initializer=kernel_initializer, bias_initializer=bias_initializer)(x_all)\n",
    "x_all = tf.keras.layers.Dropout(0.1)(x_all)\n",
    "x_all = tf.keras.layers.Dense(1, activation='sigmoid', kernel_initializer=kernel_initializer, bias_initializer=bias_initializer)(x_all)\n",
    "model = Model(inputs=[input_img, input_meta], outputs=x_all)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 - Model compiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Andrew\\anaconda3\\envs\\cancer_isic\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "#Set seed\n",
    "tf.random.set_seed(71)\n",
    "\n",
    "#Initialize model - the full model is Hybrid_model. The others are only for testing\n",
    "#model = CNN_model(neurons=8, activ='tanh')\n",
    "model = Hybrid_model(neurons=nb_neurons_hidden_layers, dropout=dropout, activ='leaky_relu')\n",
    "#model = Meta_model(neurons=18, activ='tanh')\n",
    "\n",
    "#Define optimizer and loss function\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "loss = tf.keras.losses.BinaryCrossentropy(from_logits=False,\n",
    "                                          label_smoothing=0.0,\n",
    "                                          axis=-1,\n",
    "                                          reduction='sum_over_batch_size',\n",
    "                                          name='binary_crossentropy')\n",
    "\n",
    "#Compile the model with loss, optimizer, and metrics\n",
    "model.compile(loss = loss,\n",
    "              optimizer = optimizer,\n",
    "              metrics = [\n",
    "                  tf.keras.metrics.BinaryAccuracy(),\n",
    "                  tf.keras.metrics.FalseNegatives(),\n",
    "                  tf.keras.metrics.FalsePositives(),\n",
    "                  tf.keras.metrics.TrueNegatives(),\n",
    "                  tf.keras.metrics.TruePositives()\n",
    "                  ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3 - Model loss function weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define function to calculate weights to use in loss function\n",
    "def compute_class_weights(meta_dict, pos_target):\n",
    "    # Initialize counters for target=0 and target=1\n",
    "    target_0_count = 0\n",
    "    target_1_count = 0\n",
    "\n",
    "    # Calculate total number of images\n",
    "    total = len(meta_dict)\n",
    "    # Calculate number of target = 1 by summing the target value for each dict item\n",
    "    target_1_count = sum([meta_dict[key][1][pos_target] for key in range(total)])\n",
    "    # Calculate number of target = 1\n",
    "    target_0_count = total - target_1_count\n",
    "\n",
    "    # Calculate class weights based on the counts, avoid division by zero\n",
    "    if target_1_count > 0 :\n",
    "        if target_1_count < target_0_count:\n",
    "            weight_for_0 = 1\n",
    "            weight_for_1 = target_0_count/target_1_count\n",
    "        elif target_0_count > 0:\n",
    "            weight_for_0 = target_1_count/target_0_count\n",
    "            weight_for_1 = 1\n",
    "        else:\n",
    "            weight_for_0 = 0\n",
    "            weight_for_1=target_1_count\n",
    "    else:\n",
    "        weight_for_0 = target_0_count\n",
    "        weight_for_1 = 0\n",
    "        \n",
    "\n",
    "    return weight_for_0, weight_for_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get weights for training\n",
    "weight_for_0, weight_for_1 = compute_class_weights(train_meta_dict, train_pos_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.4 - Model Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training batches in dataset: 73\n",
      "Total validate batches in dataset: 10\n"
     ]
    }
   ],
   "source": [
    "#Determine the number of batches (includes last incomplete batch)\n",
    "nb_training_batches = int(np.ceil(len(train_meta_dict)/train_batch_size))\n",
    "nb_validate_batches = int(np.ceil(len(val_meta_dict)/val_batch_size))\n",
    "\n",
    "#Print results\n",
    "print(\"Total training batches in dataset:\", nb_training_batches)\n",
    "print(\"Total validate batches in dataset:\", nb_validate_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load validation dataset into memory to speed up the model val_loss calc\n",
    "if save_val_in_memory:\n",
    "    val_in_memory = make_in_memory_dataset(hdf5_file, val_meta_dict, val_pos_isic_id, val_pos_target, apply_hair_removal=apply_hair_removal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clear the memory leak in Keras\n",
    "class CustomCallback(tf.keras.callbacks.Callback):\n",
    "  def on_epoch_end(self, epoch, logs=None):\n",
    "    gc.collect()\n",
    "    #print(f\"Epoch {epoch+1} finished. Validation loss: {logs['val_loss']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 195ms/step - binary_accuracy: 0.5313 - false_negatives: 43.3378 - false_positives: 499.5000 - loss: 2.2925 - true_negatives: 612.6892 - true_positives: 43.2568 - val_binary_accuracy: 0.6753 - val_false_negatives: 62.0000 - val_false_positives: 38.0000 - val_loss: 0.6507 - val_true_negatives: 153.0000 - val_true_positives: 55.0000\n",
      "WARNING:tensorflow:From c:\\Users\\Andrew\\anaconda3\\envs\\cancer_isic\\Lib\\site-packages\\keras\\src\\backend\\common\\global_state.py:82: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "EPOCH 2\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 199ms/step - binary_accuracy: 0.7686 - false_negatives: 41.3649 - false_positives: 208.5270 - loss: 1.0874 - true_negatives: 909.8514 - true_positives: 39.0405 - val_binary_accuracy: 0.6818 - val_false_negatives: 59.0000 - val_false_positives: 39.0000 - val_loss: 0.6491 - val_true_negatives: 152.0000 - val_true_positives: 58.0000\n",
      "EPOCH 3\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 200ms/step - binary_accuracy: 0.8421 - false_negatives: 47.8243 - false_positives: 140.8513 - loss: 1.1202 - true_negatives: 970.3378 - true_positives: 39.7703 - val_binary_accuracy: 0.7825 - val_false_negatives: 47.0000 - val_false_positives: 20.0000 - val_loss: 0.5784 - val_true_negatives: 171.0000 - val_true_positives: 70.0000\n",
      "EPOCH 4\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 192ms/step - binary_accuracy: 0.8109 - false_negatives: 28.9324 - false_positives: 209.5000 - loss: 1.0092 - true_negatives: 900.8243 - true_positives: 59.5270 - val_binary_accuracy: 0.7338 - val_false_negatives: 73.0000 - val_false_positives: 9.0000 - val_loss: 0.5005 - val_true_negatives: 182.0000 - val_true_positives: 44.0000\n",
      "EPOCH 5\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 182ms/step - binary_accuracy: 0.8315 - false_negatives: 23.7973 - false_positives: 199.3243 - loss: 0.8881 - true_negatives: 915.9865 - true_positives: 59.6757 - val_binary_accuracy: 0.7435 - val_false_negatives: 51.0000 - val_false_positives: 28.0000 - val_loss: 0.5598 - val_true_negatives: 163.0000 - val_true_positives: 66.0000\n"
     ]
    }
   ],
   "source": [
    "#Run the model through epochs\n",
    "for epoch in range(1, nb_epochs + 1):\n",
    "    #Make datasets\n",
    "    print(\"EPOCH\", epoch)\n",
    "\n",
    "    #Reinitialize the training dataset with a new shuffle each time\n",
    "    shuffle_seed = 8 + epoch #Next initialization of datasets will have a different shuffle\n",
    "    train_dataset = make_dataset(hdf5_file, train_meta_dict, train_pos_isic_id, train_pos_target, batch_size = train_batch_size, is_training=True, shuffle_seed=shuffle_seed, apply_hair_removal=apply_hair_removal)\n",
    "\n",
    "    #Fit the model, using validation data either stored in memory or on the hard disk\n",
    "    if save_val_in_memory:\n",
    "        mod = model.fit(train_dataset, epochs=1, steps_per_epoch = nb_training_batches, validation_data = val_in_memory, callbacks = [CustomCallback()],\n",
    "                        class_weight={0: weight_for_0, 1: weight_for_1})\n",
    "    else:\n",
    "        val_dataset = make_dataset(hdf5_file, val_meta_dict, val_pos_isic_id, val_pos_target, batch_size = val_batch_size, is_training=False, shuffle_seed=shuffle_seed, apply_hair_removal=apply_hair_removal)\n",
    "        mod = model.fit(train_dataset, epochs=1, steps_per_epoch = nb_training_batches, validation_data = val_dataset, validation_steps = nb_validate_batches, callbacks = [CustomCallback()],\n",
    "                        class_weight={0: weight_for_0, 1: weight_for_1})\n",
    "    \n",
    "    #Save results\n",
    "    if epoch == 1:\n",
    "        results = mod.history\n",
    "    else:\n",
    "        for key in mod.history:   \n",
    "            results[key] += mod.history[key]\n",
    "            \n",
    "    #Export model structure and weights (json and H5) - NOT WORKING\n",
    "    \n",
    "    \"\"\"\n",
    "    #Save occasionally\n",
    "    if epoch == 1:\n",
    "        model_json = model.to_json()\n",
    "        with open(savePath + \"model.json\", \"w\") as json_file:\n",
    "            json_file.write(model_json)\n",
    "            \n",
    "    wt_save_freq = 5\n",
    "    if (epoch % wt_save_freq == 0):\n",
    "        now = datetime.datetime.now()\n",
    "\n",
    "        if apply_hair_removal:\n",
    "            modifier = \"with_hair_removal_\"\n",
    "        else:\n",
    "            modifier = \"no_hair_removal_\"\n",
    "        #filename = \"Model_\" + modifier + \"Epoch_\" + str(epoch) + \"_\" + now.strftime(\"%Y-%m-%d_%Hh%Mm%Ss\") + \".weights.h5\"\n",
    "        filename = \"Model\" + \".weights.h5\"\n",
    "        model.save_weights(savePath + filename)\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    #Clean memory after use\n",
    "    del mod\n",
    "    del train_dataset\n",
    "    #If save_val_in_memory is not true, we have a generator. In this case, we want to delete the generator.\n",
    "    if save_val_in_memory != True:\n",
    "        del val_dataset\n",
    "    tf.keras.backend.clear_session()\n",
    "    gc.collect()\n",
    "\n",
    "    #Early termination (check after 15 epochs)\n",
    "    if epoch >= 15 and early_break == True:\n",
    "        #Calculate previous three changes, if positive, then loss is increasing\n",
    "        change1 = results[\"val_loss\"][-1] - results[\"val_loss\"][-2]\n",
    "        change2 = results[\"val_loss\"][-2] - results[\"val_loss\"][-3]\n",
    "        change3 = results[\"val_loss\"][-3] - results[\"val_loss\"][-4]\n",
    "\n",
    "        #Three consecutive increases in validation loss will stop the model\n",
    "        if change1 > 0 and change2 > 0 and change3 > 0:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"hybrid_model\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"hybrid_model\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,432</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">46</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">46</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">51,264</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">33856</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">36</span>)             │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,219,896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">36</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">36</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,332</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">36</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">37</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ concatenate (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">33885</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │         \u001b[38;5;34m2,432\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m46\u001b[0m, \u001b[38;5;34m46\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m51,264\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m33856\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m36\u001b[0m)             │     \u001b[38;5;34m1,219,896\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m36\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m36\u001b[0m)             │         \u001b[38;5;34m1,332\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m36\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m37\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ concatenate (\u001b[38;5;33mConcatenate\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m33885\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,824,885</span> (14.59 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,824,885\u001b[0m (14.59 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,274,961</span> (4.86 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,274,961\u001b[0m (4.86 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,549,924</span> (9.73 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m2,549,924\u001b[0m (9.73 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n#Import results from file\\nimported_results = pd.read_csv(modelResPath).to_dict(orient='list')\\n\""
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Save results to file\n",
    "pd.DataFrame.from_dict(results).to_csv(modelResPath, index=False)\n",
    "\n",
    "\"\"\"\n",
    "#Import results from file\n",
    "imported_results = pd.read_csv(modelResPath).to_dict(orient='list')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<KerasVariable shape=(5, 5, 3, 32), dtype=float32, path=hybrid_model/conv2d/kernel>,\n",
       " <KerasVariable shape=(32,), dtype=float32, path=hybrid_model/conv2d/bias>,\n",
       " <KerasVariable shape=(5, 5, 32, 64), dtype=float32, path=hybrid_model/conv2d_1/kernel>,\n",
       " <KerasVariable shape=(64,), dtype=float32, path=hybrid_model/conv2d_1/bias>,\n",
       " <KerasVariable shape=(33885, 36), dtype=float32, path=hybrid_model/dense/kernel>,\n",
       " <KerasVariable shape=(36,), dtype=float32, path=hybrid_model/dense/bias>,\n",
       " <KerasVariable shape=(36, 36), dtype=float32, path=hybrid_model/dense_1/kernel>,\n",
       " <KerasVariable shape=(36,), dtype=float32, path=hybrid_model/dense_1/bias>,\n",
       " <KerasVariable shape=(36, 1), dtype=float32, path=hybrid_model/dense_2/kernel>,\n",
       " <KerasVariable shape=(1,), dtype=float32, path=hybrid_model/dense_2/bias>]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Examine the weights objects\n",
    "model.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"hybrid_model\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"hybrid_model\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,432</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">46</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">46</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">51,264</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">33856</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">36</span>)             │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,219,896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">36</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">36</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,332</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">36</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">37</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ concatenate (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">33885</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │         \u001b[38;5;34m2,432\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m46\u001b[0m, \u001b[38;5;34m46\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m51,264\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m33856\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m36\u001b[0m)             │     \u001b[38;5;34m1,219,896\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m36\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m36\u001b[0m)             │         \u001b[38;5;34m1,332\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m36\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m37\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ concatenate (\u001b[38;5;33mConcatenate\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m33885\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,824,885</span> (14.59 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,824,885\u001b[0m (14.59 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,274,961</span> (4.86 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,274,961\u001b[0m (4.86 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,549,924</span> (9.73 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m2,549,924\u001b[0m (9.73 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try importing hdf5 and json for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimported_weights = model.load_weights(savePath + \"Model.weights.h5\", skip_mismatch=False)\\n\\n#Taken from https://machinelearningmastery.com/save-load-keras-deep-learning-models/\\n# load json and create model\\njson_file = open(savePath + \\'model.json\\', \\'r\\')\\nloaded_model_json = json_file.read()\\njson_file.close()\\n#loaded_model = tf.keras.models.model_from_json(loaded_model_json)\\n# load weights into new model\\n#loaded_model.load_weights(\"model.h5\")\\n#print(\"Loaded model from disk\")\\n'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "imported_weights = model.load_weights(savePath + \"Model.weights.h5\", skip_mismatch=False)\n",
    "\n",
    "#Taken from https://machinelearningmastery.com/save-load-keras-deep-learning-models/\n",
    "# load json and create model\n",
    "json_file = open(savePath + 'model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "#loaded_model = tf.keras.models.model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "#loaded_model.load_weights(\"model.h5\")\n",
    "#print(\"Loaded model from disk\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5 - Plot the losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 1.0)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAHHCAYAAABDUnkqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAABC1ElEQVR4nO3deXgUVd728buz7xuEJEAgsi9CEDAxLAqKRmXQuCIyEBBFVHhBHkdBlMUN92EURh0XcOYZBPQZ1BkRhGhAWUT2ZQARA0TIQoQkJEAC6Xr/6NDQZCEJSTpdfD/XVZfp6lPdv0OpuTl1qo7FMAxDAAAAJuHm7AIAAABqE+EGAACYCuEGAACYCuEGAACYCuEGAACYCuEGAACYCuEGAACYCuEGAACYCuEGAACYCuEGAACYCuEGgN28efNksVi0YcMGZ5dSJVu2bNEf//hHRUdHy9vbW2FhYRowYIDmzp2rkpISZ5cHwEk8nF0AANTEBx98oDFjxigiIkLDhg1T27Ztdfz4caWkpGjUqFHKyMjQ008/7ewyATgB4QaAy1m3bp3GjBmjhIQELVmyRIGBgfb3JkyYoA0bNmjHjh218l2FhYXy9/evlc8CUD+4LAWg2jZv3qxbbrlFQUFBCggI0A033KB169Y5tDl9+rRmzJihtm3bysfHR40aNVKfPn20fPlye5vMzEyNHDlSzZs3l7e3t6KionT77bdr//79lX7/jBkzZLFY9M9//tMh2JzVs2dPjRgxQpKUmpoqi8Wi1NRUhzb79++XxWLRvHnz7PtGjBihgIAA7du3T7feeqsCAwM1dOhQjR07VgEBATpx4kSZ7xoyZIgiIyMdLoN9/fXX6tu3r/z9/RUYGKiBAwdq586dDsfVtO8ALo6RGwDVsnPnTvXt21dBQUF68skn5enpqffee0/9+vXTypUrFR8fL0maPn26Zs6cqQcffFBxcXHKz8/Xhg0btGnTJt14442SpLvuuks7d+7UuHHjFBMTo+zsbC1fvlwHDx5UTExMud9/4sQJpaSk6Nprr1WLFi1qvX9nzpxRYmKi+vTpo9dff11+fn6KiYnRnDlz9NVXX+mee+5xqOXf//63RowYIXd3d0nSP/7xDyUnJysxMVGvvPKKTpw4oXfeeUd9+vTR5s2b7f2qSd8BVJEBAKXmzp1rSDJ++umnCtskJSUZXl5exr59++z7Dh8+bAQGBhrXXnutfV9sbKwxcODACj/n2LFjhiTjtddeq1aNW7duNSQZ48ePr1L77777zpBkfPfddw7709LSDEnG3Llz7fuSk5MNScakSZMc2lqtVqNZs2bGXXfd5bB/0aJFhiRj1apVhmEYxvHjx42QkBDjoYcecmiXmZlpBAcH2/fXtO8AqobLUgCqrKSkRN98842SkpLUqlUr+/6oqCjdf//9+uGHH5Sfny9JCgkJ0c6dO7V3795yP8vX11deXl5KTU3VsWPHqlzD2c8v73JUbXnkkUccXlssFt1zzz1asmSJCgoK7PsXLlyoZs2aqU+fPpKk5cuXKzc3V0OGDFFOTo59c3d3V3x8vL777jtJNe87gKoh3ACosiNHjujEiRNq3759mfc6duwoq9Wq9PR0SdJzzz2n3NxctWvXTl26dNGf/vQnbdu2zd7e29tbr7zyir7++mtFRETo2muv1auvvqrMzMxKawgKCpIkHT9+vBZ7do6Hh4eaN29eZv/gwYN18uRJffnll5KkgoICLVmyRPfcc48sFosk2YPc9ddfr/DwcIftm2++UXZ2tqSa9x1A1RBuANSJa6+9Vvv27dNHH32kK6+8Uh988IG6d++uDz74wN5mwoQJ+vnnnzVz5kz5+Pjo2WefVceOHbV58+YKP7dNmzby8PDQ9u3bq1TH2eBxoYqeg+Pt7S03t7L/a7zmmmsUExOjRYsWSZL+/e9/6+TJkxo8eLC9jdVqlWSbd7N8+fIy2xdffGFvW5O+A6gawg2AKgsPD5efn5/27NlT5r3du3fLzc1N0dHR9n1hYWEaOXKkPvnkE6Wnp6tr166aPn26w3GtW7fW//zP/+ibb77Rjh07VFxcrDfeeKPCGvz8/HT99ddr1apV9lGiyoSGhkqScnNzHfYfOHDgosde6N5779XSpUuVn5+vhQsXKiYmRtdcc41DXySpSZMmGjBgQJmtX79+Dp9X3b4DqBrCDYAqc3d310033aQvvvjC4ZblrKwszZ8/X3369LFfNvr9998djg0ICFCbNm1UVFQkyXan0alTpxzatG7dWoGBgfY2FZk2bZoMw9CwYcMc5sCctXHjRn388ceSpJYtW8rd3V2rVq1yaPPXv/61ap0+z+DBg1VUVKSPP/5YS5cu1b333uvwfmJiooKCgvTSSy/p9OnTZY4/cuSIpEvrO4CL41ZwAGV89NFHWrp0aZn948eP1wsvvKDly5erT58+evTRR+Xh4aH33ntPRUVFevXVV+1tO3XqpH79+qlHjx4KCwvThg0b9Nlnn2ns2LGSpJ9//lk33HCD7r33XnXq1EkeHh5avHixsrKydN9991VaX69evTRnzhw9+uij6tChg8MTilNTU/Xll1/qhRdekCQFBwfrnnvu0dtvvy2LxaLWrVvrP//5j33+S3V0795dbdq00ZQpU1RUVORwSUqyzQd65513NGzYMHXv3l333XefwsPDdfDgQX311Vfq3bu3Zs+efUl9B1AFzr5dC0DDcfZW8Iq29PR0wzAMY9OmTUZiYqIREBBg+Pn5Gf379zfWrFnj8FkvvPCCERcXZ4SEhBi+vr5Ghw4djBdffNEoLi42DMMwcnJyjMcee8zo0KGD4e/vbwQHBxvx8fHGokWLqlzvxo0bjfvvv99o2rSp4enpaYSGhho33HCD8fHHHxslJSX2dkeOHDHuuusuw8/PzwgNDTUefvhhY8eOHeXeCu7v71/pd06ZMsWQZLRp06bCNt99952RmJhoBAcHGz4+Pkbr1q2NESNGGBs2bKi1vgOomMUwDMNpyQoAAKCWMecGAACYCuEGAACYCuEGAACYilPDzapVqzRo0CA1bdpUFotFn3/++UWPSU1NVffu3eXt7a02bdo4rOgLAADg1HBTWFio2NhYzZkzp0rt09LSNHDgQPXv319btmzRhAkT9OCDD2rZsmV1XCkAAHAVDeZuKYvFosWLFyspKanCNk899ZS++uor7dixw77vvvvuU25ubrnP5AAAAJcfl3qI39q1azVgwACHfYmJiZowYUKFxxQVFTk88dNqtero0aNq1KhRhWvOAACAhsUwDB0/flxNmzYtd/2387lUuMnMzFRERITDvoiICOXn5+vkyZPy9fUtc8zMmTM1Y8aM+ioRAADUofT0dDVv3rzSNi4Vbmpi8uTJmjhxov11Xl6eWrRoofT0dPsaOAAA1MTxU6d14PdCpeUUan/OCe3PKVTa74Xa//sJFZ+xVnhcI39PxTQK0BXhfopp5K+Yxv66orG/moX4ysOdG5nLk5+fr+joaAUGBl60rUuFm8jISGVlZTnsy8rKUlBQULmjNpLk7e0tb2/vMvuDgoIINwCASxIUJDVr0ki9LthvtRo6lHtSv+YUal92gX7NKdCvRwq170iBsvKLdOyMdCyrSJuziiQdsx/n6W5Ry0b+ah3ur1bhAWrV2F+tmwSodeMABft51mvfGqqqTClxqXCTkJCgJUuWOOxbvny5EhISnFQRAABlublZFB3mp+gwP13XLtzhvYKiM0orDTq/HinQvtIAlJZTqKIzVv2SXaBfsgskOf5lvpG/l1qHB6hVuL9taxyg1k0CFB3KaM+FnBpuCgoK9Msvv9hfp6WlacuWLQoLC1OLFi00efJkHTp0SH//+98lSWPGjNHs2bP15JNP6oEHHtC3336rRYsW6auvvnJWFwAAqJYAbw91aR6sLs2DHfZbrYYO5520j/D8eqRQv+YUaF92oTLzT+n3wmL9XnhU6/cfdTjO092iFmF+pcHHFn5ahweodbi/Qvy86rNrDYZTbwVPTU1V//79y+xPTk7WvHnzNGLECO3fv1+pqakOxzz++OP673//q+bNm+vZZ5/ViBEjqvyd+fn5Cg4OVl5eHpelAAAuobDojNJybKFn35FC24jPkUKl5RTo1OmK5/aE+XvZLnE1Phd6WoX7KzrMT54uNtpTnd/fDeY5N/Wlqn84JSUlOn36dD1Whqry8vK66G2AAHA5sFoNZeSfss3rOVJgm+NTOuqTkXeqwuM83Cxq0cjPHnZaNw5Q6ya2EBTq3zBHewg3lbjYH45hGMrMzFRubm79F4cqcXNz0xVXXCEvr4b5HyAANATnj/bYLnGdm9tz8nRJhceF+nmeN7cnwP5zCyeP9hBuKnGxP5yMjAzl5uaqSZMm8vPz40F/DYzVatXhw4fl6empFi1acH4AoJqsVkOZ+afOhZ7zLnUdrsJoT6vGAaV3c/nb5/mE1cNoT3XCjUvdLVXXSkpK7MGmUaNGzi4HFQgPD9fhw4d15swZeXpyayQAVIebm0VNQ3zVNMRXfds63sl1oviMfZTn1yMFDpObT54uKQ1DhVqxy/EzQ86O9jS2jfZ0iApU//ZN6rFXjgg35zk7x8bPz8/JlaAyZy9HlZSUEG4AoBb5eXnoymbBurKZ451chlE62pNd6PDMnl+PFOpQ7knlnjitjQeOaeMB2zN7OkUFEW4aGi51NGycHwCoXxaLRVHBvooK9lWfto0d3jtRbJvbc3ZU59ecAjUPLf/BuvWFcAMAAGrMz8tDnZsGq3PT4Is3rifcTwsAAEyFcONiRowYoaSkJGeXAQBAg0W4AQAApkK4MZGVK1cqLi5O3t7eioqK0qRJk3TmzBn7+5999pm6dOkiX19fNWrUSAMGDFBhYaEk27IWcXFx8vf3V0hIiHr37q0DBw44qysAANQY4cYkDh06pFtvvVVXX321tm7dqnfeeUcffvihXnjhBUm2hxMOGTJEDzzwgHbt2qXU1FTdeeedMgxDZ86cUVJSkq677jpt27ZNa9eu1ejRo7krCQDgkrhbyiT++te/Kjo6WrNnz5bFYlGHDh10+PBhPfXUU5o6daoyMjJ05swZ3XnnnWrZsqUkqUuXLpKko0ePKi8vT3/4wx/UunVrSVLHjh2d1hcAAC4FIzcmsWvXLiUkJDiMtvTu3VsFBQX67bffFBsbqxtuuEFdunTRPffco/fff1/HjtkethQWFqYRI0YoMTFRgwYN0l/+8hdlZGQ4qysAAFwSws1lwt3dXcuXL9fXX3+tTp066e2331b79u2VlpYmSZo7d67Wrl2rXr16aeHChWrXrp3WrVvn5KoBAKg+wo1JdOzYUWvXrtX566CuXr1agYGBat68uSTbEyZ79+6tGTNmaPPmzfLy8tLixYvt7a+66ipNnjxZa9as0ZVXXqn58+fXez8AALhUzLlxQXl5edqyZYvDvtGjR2vWrFkaN26cxo4dqz179mjatGmaOHGi3Nzc9OOPPyolJUU33XSTmjRpoh9//FFHjhxRx44dlZaWpr/97W+67bbb1LRpU+3Zs0d79+7V8OHDndNBAAAuAeHGBaWmpuqqq65y2Ddq1CgtWbJEf/rTnxQbG6uwsDCNGjVKzzzzjCQpKChIq1at0qxZs5Sfn6+WLVvqjTfe0C233KKsrCzt3r1bH3/8sX7//XdFRUXpscce08MPP+yM7gEAcEksxvnXMS4D+fn5Cg4OVl5enoKCghzeO3XqlNLS0nTFFVfIx8fHSRXiYjhPAHD5qez394WYcwMAAEyFcAMAAEyFcAMAAEyFcAMAAEyFcAMAAEyFcAMAAEyFcAMAAEyFcAMAAEyFcAMAAEyFcIMKxcTEaNasWc4uAwCAaiHcmIDFYql0mz59eo0+96efftLo0aNrt1gAAOoYC2eaQEZGhv3nhQsXaurUqdqzZ499X0BAgP1nwzBUUlIiD4+Ln/rw8PDaLRQAgHrAyI0JREZG2rfg4GBZLBb76927dyswMFBff/21evToIW9vb/3www/at2+fbr/9dkVERCggIEBXX321VqxY4fC5F16Wslgs+uCDD3THHXfIz89Pbdu21ZdfflnPvQUAoHKEm4swDEMnis84ZavNBdsnTZqkl19+Wbt27VLXrl1VUFCgW2+9VSkpKdq8ebNuvvlmDRo0SAcPHqz0c2bMmKF7771X27Zt06233qqhQ4fq6NGjtVYnAACXistSF3HydIk6TV3mlO/+73OJ8vOqnVP03HPP6cYbb7S/DgsLU2xsrP31888/r8WLF+vLL7/U2LFjK/ycESNGaMiQIZKkl156SW+99ZbWr1+vm2++uVbqBADgUjFyc5no2bOnw+uCggI98cQT6tixo0JCQhQQEKBdu3ZddOSma9eu9p/9/f0VFBSk7OzsOqkZAICaYOTmInw93fXf5xKd9t21xd/f3+H1E088oeXLl+v1119XmzZt5Ovrq7vvvlvFxcWVfo6np6fDa4vFIqvVWmt1AgBwqQg3F2GxWGrt0lBDsnr1ao0YMUJ33HGHJNtIzv79+51bFAAAtYDLUpeptm3b6l//+pe2bNmirVu36v7772cEBgBgCoSby9Sbb76p0NBQ9erVS4MGDVJiYqK6d+/u7LIAALhkFqM27zd2Afn5+QoODlZeXp6CgoIc3jt16pTS0tJ0xRVXyMfHx0kV4mI4TwBw+ans9/eFGLkBAACmQrgBAACmQrgBAACmQrgBAACmQrgBAACmQrgBAACmQrgBAACmQrgBAACmQrgBAACmQriBJKlfv36aMGGC/XVMTIxmzZpV6TEWi0Wff/55ndYFAEB1EW5MYNCgQbr55pvLfe/777+XxWLRtm3bqvWZP/30k0aPHl0b5dlNnz5d3bp1q9XPBADgQoQbExg1apSWL1+u3377rcx7c+fOVc+ePdW1a9dqfWZ4eLj8/Pxqq0QAAOoN4cYE/vCHPyg8PFzz5s1z2F9QUKBPP/1USUlJGjJkiJo1ayY/Pz916dJFn3zySaWfeeFlqb179+raa6+Vj4+POnXqpOXLl5c55qmnnlK7du3k5+enVq1a6dlnn9Xp06clSfPmzdOMGTO0detWWSwWWSwWe725ubl68MEHFR4erqCgIF1//fXaunXrJf2ZAAAuXx7OLqDBMwzp9AnnfLenn2SxXLSZh4eHhg8frnnz5mnKlCmylB7z6aefqqSkRH/84x/16aef6qmnnlJQUJC++uorDRs2TK1bt1ZcXNxFP99qterOO+9URESEfvzxR+Xl5TnMzzkrMDBQ8+bNU9OmTbV9+3Y99NBDCgwM1JNPPqnBgwdrx44dWrp0qVasWCFJCg4OliTdc8898vX11ddff63g4GC99957uuGGG/Tzzz8rLCysGn9gAAAQbi7u9AnppabO+e6nD0te/lVq+sADD+i1117TypUr1a9fP0m2S1J33XWXWrZsqSeeeMLedty4cVq2bJkWLVpUpXCzYsUK7d69W8uWLVPTprY/i5deekm33HKLQ7tnnnnG/nNMTIyeeOIJLViwQE8++aR8fX0VEBAgDw8PRUZG2tv98MMPWr9+vbKzs+Xt7S1Jev311/X555/rs88+q/V5PwAA8yPcmESHDh3Uq1cvffTRR+rXr59++eUXff/993ruuedUUlKil156SYsWLdKhQ4dUXFysoqKiKs+p2bVrl6Kjo+3BRpISEhLKtFu4cKHeeust7du3TwUFBTpz5oyCgoIq/eytW7eqoKBAjRo1cth/8uRJ7du3r0r1AQBwPsLNxXj62UZQnPXd1TBq1CiNGzdOc+bM0dy5c9W6dWtdd911euWVV/SXv/xFs2bNUpcuXeTv768JEyaouLi41kpdu3athg4dqhkzZigxMVHBwcFasGCB3njjjUqPKygoUFRUlFJTU8u8FxISUmv1AQAuH4Sbi7FYqnxpyNnuvfdejR8/XvPnz9ff//53PfLII7JYLFq9erVuv/12/fGPf5Rkm0Pz888/q1OnTlX63I4dOyo9PV0ZGRmKioqSJK1bt86hzZo1a9SyZUtNmTLFvu/AgQMObby8vFRSUuKwr3v37srMzJSHh4diYmKq22UAAMrgbikTCQgI0ODBgzV58mRlZGRoxIgRkqS2bdtq+fLlWrNmjXbt2qWHH35YWVlZVf7cAQMGqF27dkpOTtbWrVv1/fffO4SYs99x8OBBLViwQPv27dNbb72lxYsXO7SJiYlRWlqatmzZopycHBUVFWnAgAFKSEhQUlKSvvnmG+3fv19r1qzRlClTtGHDhkv+MwEAXH4INyYzatQoHTt2TImJifY5Ms8884y6d++uxMRE9evXT5GRkUpKSqryZ7q5uWnx4sU6efKk4uLi9OCDD+rFF190aHPbbbfp8ccf19ixY9WtWzetWbNGzz77rEObu+66SzfffLP69++v8PBwffLJJ7JYLFqyZImuvfZajRw5Uu3atdN9992nAwcOKCIi4pL/PAAAlx+LYRiGs4uoT/n5+QoODlZeXl6Zya6nTp1SWlqarrjiCvn4+DipQlwM5wkALj+V/f6+ECM3AADAVAg3AADAVAg3AADAVJwebubMmaOYmBj5+PgoPj5e69evr7T9rFmz1L59e/n6+io6OlqPP/64Tp06VU/VAgCAhs6p4WbhwoWaOHGipk2bpk2bNik2NlaJiYnKzs4ut/38+fM1adIkTZs2Tbt27dKHH36ohQsX6umnn67Vui6zOdYuh/MDAKiMU8PNm2++qYceekgjR45Up06d9O6778rPz08fffRRue3XrFmj3r176/7771dMTIxuuukmDRky5KKjPVXl6ekpSTpxwkkLZaJKzj5Z2d3d3cmVAAAaIqc9obi4uFgbN27U5MmT7fvc3Nw0YMAArV27ttxjevXqpf/93//V+vXrFRcXp19//VVLlizRsGHDKvyeoqIiFRUV2V/n5+dX2Nbd3V0hISH2kSM/Pz/7CttoGKxWq44cOSI/Pz95ePCAbQBAWU777ZCTk6OSkpIyD2qLiIjQ7t27yz3m/vvvV05Ojvr06SPDMHTmzBmNGTOm0stSM2fO1IwZM6pc19kVqyu6NAbnc3NzU4sWLQieAIByudRffVNTU/XSSy/pr3/9q+Lj4/XLL79o/Pjxev7558s8DfesyZMna+LEifbX+fn5io6OrvA7LBaLoqKi1KRJE50+fbrW+4BL5+XlJTc3p8+FBwA0UE4LN40bN5a7u3uZNY6ysrLsoycXevbZZzVs2DA9+OCDkqQuXbqosLBQo0eP1pQpU8r9heft7S1vb+9q1+fu7s6cDgAAXJDT/vrr5eWlHj16KCUlxb7ParUqJSVFCQkJ5R5z4sSJMgHmbADhDhoAACA5+bLUxIkTlZycrJ49eyouLk6zZs1SYWGhRo4cKUkaPny4mjVrppkzZ0qSBg0apDfffFNXXXWV/bLUs88+q0GDBjHKAgAAJDk53AwePFhHjhzR1KlTlZmZqW7dumnp0qX2ScYHDx50GKl55plnZLFY9Mwzz+jQoUMKDw/XoEGDyqxQDQAALl+sCg4AABo8VgUHAACXLcINAAAwFcINAAAwFcINAAAwFcINAAAwFcINAAAwFcINAAAwFcINAAAwFcINAAAwFcINAAAwFcINAAAwFcINAAAwFcINAAAwFcINAAAwFcINAAAwFcINAAAwFcINAAAwFcINAAAwFcINAAAwFcINAAAwFcINAAAwFcINAAAwFcINAAAwFcINAAAwFcINAAAwFcINAAAwFcINAAAwFcINAAAwFcINAAAwFcINAAAwFcINAAAwFcINAAAwFcINAAAwFcINAAAwFcINAAAwFcINAAAwFcINAAAwFcINAAAwFcINAAAwFcINAAAwFcINAAAwFcINAAAwFcINAAAwFcINAAAwFcINAAAwFcINAAAwFcINAAAwFQ9nF2AamTukf42WLBbbptJ/Wtyq+LPquL1FsqjiNip9XeZnVbC/sp8v9pmX2r66tVXxz8DiJrl5SuHtJZ+gWv4XBABQXwg3teX0CSl7p7OrQG2wuEkRnaUWCVJ0vNTiGim4ubOrAgBUkcUwDMPZRdSn/Px8BQcHKy8vT0FBtfi385O50uFNkmFIMmz/tP9srcLPVWlf+trhZ1Wwv7KfVXGbCr9fVexHVfpUWY1VaVNB+9r4sy4+IR0/XPb8BkefCzotrpGadJLc3Gvj3xwAQBVU5/c3Ize1xTdEan29s6tAbcjPkNLXSQd/lA6ulTK3S3nptm3HZ7Y23kFS86ttQSc6XmreU/Lyd27dAABJjNw4uxy4gqIC6dBG6eA6W+hJ/0kqPu7YxuIuRXV1vJQVGOmcegHAhKrz+5twA1SXtUTK2imll47sHFwn5R8q2y40Roq+RmoRbws9jdtLbtygCAA1QbipBOEGdSI3vTTsrLNtWTtkm9x0Hp+Q0lGdeFvoadZd8vR1RrUA4HIIN5Ug3KBenMqTfvvJNm8nfZ302wbbHXXnc/OUmnYrnbdTOlHZv7FTygWAho5wUwnCDZyi5LRtYvLZeTsHf5QKMsu2a9TG8VJWozbnnukDAJcxwk0lCDdoEAxDOrb/vHk7P0pHdpVt59foXNiJvsY20uPhXd/VAoDTEW4qQbhBg3XiaOmlrNJ5O4c3SWdOObZx97bN1Tl7KSs6TvILc069AFCPCDeVINzAZZwpljK22kZ2zk5WPpFTtl14h9KJygm2EZ7QK7iUBcB0CDeVINzAZRmG9Pu+0jk7pZeyft9btp1/k3NPUo6+xvb8HXfP+q8XAGoR4aYShBuYSmGO4y3ohzdL1tOObTx8bU9Qtl/KulryCXZOvQBQQ4SbShBuYGqnT9kCzvmXsk7lXtDIYlsY9Py1soKjuZQFoEEj3FSCcIPLitUq5fxceimrdDuWVrZdYNPzLmXFSxFXSu4sPQeg4XCpcDNnzhy99tpryszMVGxsrN5++23FxcVV2D43N1dTpkzRv/71Lx09elQtW7bUrFmzdOutt1bp+wg3uOwdz7pgYdBtkvWMYxuvgNJLWQnnFgb1DnROvQAgF1oVfOHChZo4caLeffddxcfHa9asWUpMTNSePXvUpEmTMu2Li4t14403qkmTJvrss8/UrFkzHThwQCEhIfVfPOCqAiOkTrfbNkkqLixdGLT0acrp66WifOnXVNsmSRY3KbLLuScpt7hGCmrqrB4AQKWcOnITHx+vq6++WrNnz5YkWa1WRUdHa9y4cZo0aVKZ9u+++65ee+017d69W56eNbv7g5Eb4CKsJVL2rvPm7fwo5R0s2y64RWnQKX3AYJOOkpt7/dcL4LLgEpeliouL5efnp88++0xJSUn2/cnJycrNzdUXX3xR5phbb71VYWFh8vPz0xdffKHw8HDdf//9euqpp+TuXv7/VIuKilRUVGR/nZ+fr+joaMINUB15hxzn7WTtkAyrYxvvYNudWGfvymrWQ/Lyc069AEzHJS5L5eTkqKSkRBEREQ77IyIitHv37nKP+fXXX/Xtt99q6NChWrJkiX755Rc9+uijOn36tKZNm1buMTNnztSMGTNqvX7gshLcTAq+S7ryLtvrouPnFgY9uNa2MGhRnvTLCtsmSW4eUlSs46WsgLKXmwGgtjlt5Obw4cNq1qyZ1qxZo4SEBPv+J598UitXrtSPP/5Y5ph27drp1KlTSktLs4/UvPnmm3rttdeUkZFR7vcwcgPUg5IzttEc+8Kg66Tj5fw3GXrFuScpR18jNW4nubnVf70AXI5LjNw0btxY7u7uysrKctiflZWlyMjIco+JioqSp6enwyWojh07KjMzU8XFxfLy8ipzjLe3t7y9WWgQqFPuHrZFPZt2k64ZY3uacu5Bx4VBs/9ruw39WJq0db7tON/Qc8/bib5GanqV5OnjzJ4AMAGnhRsvLy/16NFDKSkp9jk3VqtVKSkpGjt2bLnH9O7dW/Pnz5fVapVb6d/2fv75Z0VFRZUbbAA4icUihba0bV3vte07meu4MOihjdLJY9LPS22bJLl72QLO2bWyouMl/0ZO6wYA1+TUu6UWLlyo5ORkvffee4qLi9OsWbO0aNEi7d69WxERERo+fLiaNWummTNnSpLS09PVuXNnJScna9y4cdq7d68eeOAB/b//9/80ZcqUKn0nd0sBDcSZYtszds6/lFV4pGy7Rm3PzdlpkSCFteJpysBlyCUuS0nS4MGDdeTIEU2dOlWZmZnq1q2bli5dap9kfPDgQfsIjSRFR0dr2bJlevzxx9W1a1c1a9ZM48eP11NPPeWsLgCoKQ8v28MBm/eUNNZ2Kevor46XsnL22BYH/X2vtPkftuP8Gl+wMGis7bMAoJTTn1Bc3xi5AVzIiaMXLAy6SSopdmzj4WO77dw+dyfONpcHgKm4xHNunIVwA7iw06ekjC2ll7JKQ8/Joxc0stiepnzFtbatRYLkw3/rgKsj3FSCcAOYiGFIOXsdV0E/us+xjcXdNkn5ir5STF/b6I6Xv3PqBVBjhJtKEG4Ak8vPkPb/IO1fJaWtko7td3zfzdM2zyemr21kp/nV3H4OuADCTSUIN8BlJjdd2v+9LeikfS/l/+b4voePbZ5OzLW20Z2m3ZmgDDRAhJtKEG6Ay9jZO7L2f28LOmmrpMJsxzae/rZLV1eUjuxExtoeUgjAqQg3lSDcALAzDCnn59JRnVW2y1kXTlD2DpJa9rIFnZi+UsSVLBkBOAHhphKEGwAVslpty0SkrbKN7uxfbVsQ9Hy+oVJMn9LLWNdK4e15qCBw1ulTUvZO218cmves1Y8m3FSCcAOgyqwlUsbWc3N2DqyVThc6tvFvYgs7Z2895wnKuFwUHZcyd9j+Gzm7HdktGSVSq37S8C9q9etc5gnFANCgublLzbrbtt7jpZLT0uHNUtpK25yd9B9tc3Z2/su2SVJg09KgUzpnJ6SFc/sA1IYTR23LpZwfZH7fJ6mc8RG/RpJvWL2XeD5GbgCgps4U2RYDTfveNrqTvl6ynnZsE9KyNOhcZ5uzExTlnFqBqjqedV6I2SJlbJPyDpbfNrCpbQkU+9ZVCmpWJ6OXXJaqBOEGQJ0pPmEbzTl7GevQJtsQ/fkatTk3OTmmrxQQ7pxaAcOQ8tIdR2MytkkFmeW3D405F2IiS4NMQJN6K5dwUwnCDYB6U3TcNk9nf+kzdjK2qswwfpNOpQ8U7Cu17C35OXc4HyZltdoeg5Cx5VyQydwmnTxWTmOL1Lid42hMZBenr9lGuKkE4QaA05w8Jh1Yc+4ZO9k7L2jAulioBSVnpJw9jiMymdul4oKybd08pCYdS0NMNymyqxR5ZYNcooRwUwnCDYAGozDH9myds7ee5/zs+D7rYuFiTp+yPb7g/CCT/V/pzKmybT18bM9pOn9EpkknycO7/uuuAcJNJQg3ABos1sVCZYoKpKxybr22ninb1ivQFl7sc2S62i41ufDTtgk3lSDcAHAZrIt1+Tp5zDa51+HW619U7q3XvmEX3LEUK4VeYbonaRNuKkG4AeCSDEM6lnYu6LAulnkUZJ9323VpkMmt6NbrKMcQE9lVCm5+WTw4knBTCcINAFNgXSzXYxhS3m8XTPTdJh3PKL99SMvzgky3er/1uqGp83CTnp4ui8Wi5s2bS5LWr1+v+fPnq1OnTho9enTNqq4nhBsApsS6WA2L1WobaTt/NCZjW9kAKsl263XbC0ZknH/rdUNT5+Gmb9++Gj16tIYNG6bMzEy1b99enTt31t69ezVu3DhNnTq1xsXXNcINgMuCw7pY39tuQWddrLpRcsY2ilbm1uvjZdu6eUjhHR2DTERnyTug/ut2MXUebkJDQ7Vu3Tq1b99eb731lhYuXKjVq1frm2++0ZgxY/Trr7/WuPi6RrgBcFmyr4tVehkr/ceytwuzLtbFnSkqe+t11s5Kbr3u7Bhkwjtyh1sN1fnCmadPn5a3t+2++BUrVui2226TJHXo0EEZGRVcOwQAOI+7p+3Oqug46donyl8X6/hhadsC2yaxLlZxYTmrXu+q4NbrANvk3vODjIvfeu3KajRyEx8fr/79+2vgwIG66aabtG7dOsXGxmrdunW6++679dtvv138Q5yEkRsAKMflvi7WydwLVr3eVvpQxfJuvQ694Nbrbqa89bqhqfPLUqmpqbrjjjuUn5+v5ORkffTRR5Kkp59+Wrt379a//vWvmlVeDwg3AFAFRcelg+uktJXmWxer4IjjrdeZ28o+MPGsgMiyz5C5TG69bmjq5VbwkpIS5efnKzT03Gzu/fv3y8/PT02aNNxb1Qg3AFADrrgulmFI+YfKrnp9/HD57UNaOI7GRHaVAiPqtWRUrM7DzcmTJ2UYhvz8/CRJBw4c0OLFi9WxY0clJibWrOp6QrgBgFrQ0NbFst96fcEzZE78Xk5ji+0S24W3XrvKyNNlqs7DzU033aQ777xTY8aMUW5urjp06CBPT0/l5OTozTff1COPPFLj4usa4QYA6sDxzNLJyfWwLlbJGen3vWVvvS7KL9vW4n7eqtdnb72+kluvXVCdh5vGjRtr5cqV6ty5sz744AO9/fbb2rx5s/7v//5PU6dO1a5du2pcfF0j3ABAPaitdbHOFEnZuxxHYzJ3SGdOlm3r7l321usmnbj12iTq/FbwEydOKDAwUJL0zTff6M4775Sbm5uuueYaHThwoCYfCQAwk5Boqdv9tu3CdbH2fy8VZJ175s53clwXyyugdLLvNluwsZ4u+/leAbZLSWVuvfas756iAapRuGnTpo0+//xz3XHHHVq2bJkef/xxSVJ2djajIQAARxaL7enHYa2kHiMc18U6+wTlk0elfSm27UI+IWVvvQ5rxa3XqFCNws3UqVN1//336/HHH9f111+vhIQESbZRnKuuuqpWCwQAmIzFYlvXKry9FPfQuXWx9n9vm6Rcctq2SKT91utobr1GtdT4VvDMzExlZGQoNjZWbqXpef369QoKClKHDh1qtcjaxJwbAABcT53PuZGkyMhIRUZG2p9G3Lx5c8XFxdX04wAAAGpFjS5YWq1WPffccwoODlbLli3VsmVLhYSE6Pnnn5fVaq3tGgEAAKqsRiM3U6ZM0YcffqiXX35ZvXv3liT98MMPmj59uk6dOqUXX3yxVosEAACoqhrNuWnatKneffdd+2rgZ33xxRd69NFHdejQoVorsLYx5wYAANdTnd/fNbosdfTo0XInDXfo0EFHjx6tyUcCAADUihqFm9jYWM2ePbvM/tmzZ6tr166XXBQAAEBN1WjOzauvvqqBAwdqxYoV9mfcrF27Vunp6VqyZEmtFggAAFAdNRq5ue666/Tzzz/rjjvuUG5urnJzc3XnnXdq586d+sc//lHbNQIAAFRZjR/iV56tW7eqe/fuKikpqa2PrHVMKAYAwPXU+YRiAACAhopwAwAATIVwAwAATKVad0vdeeedlb6fm5t7KbUAAABcsmqFm+Dg4Iu+P3z48EsqCAAA4FJUK9zMnTu3ruoAAACoFcy5AQAApkK4AQAApkK4AQAApkK4AQAApkK4AQAApkK4AQAApkK4AQAApkK4AQAApkK4AQAApkK4AQAApkK4AQAApkK4AQAApkK4AQAApkK4AQAApkK4AQAApkK4AQAAptIgws2cOXMUExMjHx8fxcfHa/369VU6bsGCBbJYLEpKSqrbAgEAgMtwerhZuHChJk6cqGnTpmnTpk2KjY1VYmKisrOzKz1u//79euKJJ9S3b996qhQAALgCp4ebN998Uw899JBGjhypTp066d1335Wfn58++uijCo8pKSnR0KFDNWPGDLVq1aoeqwUAAA2dU8NNcXGxNm7cqAEDBtj3ubm5acCAAVq7dm2Fxz333HNq0qSJRo0addHvKCoqUn5+vsMGAADMy6nhJicnRyUlJYqIiHDYHxERoczMzHKP+eGHH/Thhx/q/fffr9J3zJw5U8HBwfYtOjr6kusGAAANl9MvS1XH8ePHNWzYML3//vtq3LhxlY6ZPHmy8vLy7Ft6enodVwkAAJzJw5lf3rhxY7m7uysrK8thf1ZWliIjI8u037dvn/bv369BgwbZ91mtVkmSh4eH9uzZo9atWzsc4+3tLW9v7zqoHgAANEROHbnx8vJSjx49lJKSYt9ntVqVkpKihISEMu07dOig7du3a8uWLfbttttuU//+/bVlyxYuOQEAAOeO3EjSxIkTlZycrJ49eyouLk6zZs1SYWGhRo4cKUkaPny4mjVrppkzZ8rHx0dXXnmlw/EhISGSVGY/AAC4PDk93AwePFhHjhzR1KlTlZmZqW7dumnp0qX2ScYHDx6Um5tLTQ0CAABOZDEMw3B2EfUpPz9fwcHBysvLU1BQkLPLAQAAVVCd398MiQAAAFMh3AAAAFMh3AAAAFMh3AAAAFMh3AAAAFMh3AAAAFMh3AAAAFMh3AAAAFMh3AAAAFMh3AAAAFMh3AAAAFMh3AAAAFMh3AAAAFMh3AAAAFMh3AAAAFMh3AAAAFMh3AAAAFMh3AAAAFMh3AAAAFMh3AAAAFMh3AAAAFMh3AAAAFMh3AAAAFMh3AAAAFMh3AAAAFMh3AAAAFMh3AAAAFMh3AAAAFMh3AAAAFMh3AAAAFMh3AAAAFMh3AAAAFMh3AAAAFMh3AAAAFMh3AAAAFMh3AAAAFMh3AAAAFMh3AAAAFMh3AAAAFMh3AAAAFMh3AAAAFMh3AAAAFMh3AAAAFMh3AAAAFMh3AAAAFMh3AAAAFMh3AAAAFMh3AAAAFMh3AAAAFMh3AAAAFMh3AAAAFMh3AAAAFMh3AAAAFMh3AAAAFMh3AAAAFMh3AAAAFMh3AAAAFMh3AAAAFMh3AAAAFMh3AAAAFMh3AAAAFMh3AAAAFMh3AAAAFNpEOFmzpw5iomJkY+Pj+Lj47V+/foK277//vvq27evQkNDFRoaqgEDBlTaHgAAXF6cHm4WLlyoiRMnatq0adq0aZNiY2OVmJio7OzsctunpqZqyJAh+u6777R27VpFR0frpptu0qFDh+q5cgAA0BBZDMMwnFlAfHy8rr76as2ePVuSZLVaFR0drXHjxmnSpEkXPb6kpEShoaGaPXu2hg8fftH2+fn5Cg4OVl5enoKCgi65fgAAUPeq8/vbqSM3xcXF2rhxowYMGGDf5+bmpgEDBmjt2rVV+owTJ07o9OnTCgsLK/f9oqIi5efnO2wAAMC8nBpucnJyVFJSooiICIf9ERERyszMrNJnPPXUU2ratKlDQDrfzJkzFRwcbN+io6MvuW4AANBwOX3OzaV4+eWXtWDBAi1evFg+Pj7ltpk8ebLy8vLsW3p6ej1XCQAA6pOHM7+8cePGcnd3V1ZWlsP+rKwsRUZGVnrs66+/rpdfflkrVqxQ165dK2zn7e0tb2/vWqkXAAA0fE4dufHy8lKPHj2UkpJi32e1WpWSkqKEhIQKj3v11Vf1/PPPa+nSperZs2d9lAoAAFyEU0duJGnixIlKTk5Wz549FRcXp1mzZqmwsFAjR46UJA0fPlzNmjXTzJkzJUmvvPKKpk6dqvnz5ysmJsY+NycgIEABAQFO6wcAAGgYnB5uBg8erCNHjmjq1KnKzMxUt27dtHTpUvsk44MHD8rN7dwA0zvvvKPi4mLdfffdDp8zbdo0TZ8+vT5LBwAADZDTn3NT33jODQAArsdlnnMDAABQ2wg3AADAVAg3AADAVAg3AADAVAg3AADAVAg3AADAVAg3AADAVAg3AADAVAg3AADAVAg3AADAVAg3AADAVAg3AADAVAg3AADAVAg3AADAVAg3AADAVAg3AADAVAg3AADAVAg3AADAVAg3AADAVAg3AADAVAg3AADAVAg3AADAVAg3AADAVAg3AADAVAg3AADAVAg3AADAVAg3AADAVAg3AADAVAg3AADAVAg3AADAVAg3AADAVAg3AADAVAg3AADAVAg3AADAVAg3AADAVAg3AADAVAg3AADAVAg3AADAVAg3AADAVAg3AADAVAg3AADAVAg3AADAVAg3AADAVAg3AADAVAg3AADAVAg3AADAVAg3AADAVAg3AADAVAg3AADAVAg3AADAVAg3AADAVAg3AADAVAg3AADAVAg3AADAVAg3AADAVAg3AADAVAg3AADAVAg3AADAVAg3AADAVAg3AADAVAg3AADAVAg3AADAVBpEuJkzZ45iYmLk4+Oj+Ph4rV+/vtL2n376qTp06CAfHx916dJFS5YsqadKAQBAQ+f0cLNw4UJNnDhR06ZN06ZNmxQbG6vExERlZ2eX237NmjUaMmSIRo0apc2bNyspKUlJSUnasWNHPVcOAAAaIothGIYzC4iPj9fVV1+t2bNnS5KsVquio6M1btw4TZo0qUz7wYMHq7CwUP/5z3/s+6655hp169ZN77777kW/Lz8/X8HBwcrLy1NQUFDtdQQAANSZ6vz+durITXFxsTZu3KgBAwbY97m5uWnAgAFau3ZtucesXbvWob0kJSYmVtgeAABcXjyc+eU5OTkqKSlRRESEw/6IiAjt3r273GMyMzPLbZ+ZmVlu+6KiIhUVFdlf5+XlSbIlQAAA4BrO/t6uygUnp4ab+jBz5kzNmDGjzP7o6GgnVAMAAC7F8ePHFRwcXGkbp4abxo0by93dXVlZWQ77s7KyFBkZWe4xkZGR1Wo/efJkTZw40f7aarXq6NGjatSokSwWyyX2wFF+fr6io6OVnp5uyvk8Zu+fZP4+0j/XZ/Y+0j/XV1d9NAxDx48fV9OmTS/a1qnhxsvLSz169FBKSoqSkpIk2cJHSkqKxo4dW+4xCQkJSklJ0YQJE+z7li9froSEhHLbe3t7y9vb22FfSEhIbZRfoaCgINP+SyuZv3+S+ftI/1yf2ftI/1xfXfTxYiM2Zzn9stTEiROVnJysnj17Ki4uTrNmzVJhYaFGjhwpSRo+fLiaNWummTNnSpLGjx+v6667Tm+88YYGDhyoBQsWaMOGDfrb3/7mzG4AAIAGwunhZvDgwTpy5IimTp2qzMxMdevWTUuXLrVPGj548KDc3M7d1NWrVy/Nnz9fzzzzjJ5++mm1bdtWn3/+ua688kpndQEAADQgTg83kjR27NgKL0OlpqaW2XfPPffonnvuqeOqqs/b21vTpk0rcxnMLMzeP8n8faR/rs/sfaR/rq8h9NHpD/EDAACoTU5ffgEAAKA2EW4AAICpEG4AAICpEG4AAICpEG6qaNWqVRo0aJCaNm0qi8Wizz///KLHpKamqnv37vL29labNm00b968Oq/zUlS3j6mpqbJYLGW2itb5craZM2fq6quvVmBgoJo0aaKkpCTt2bPnosd9+umn6tChg3x8fNSlSxctWbKkHqqtvpr0b968eWXOn4+PTz1VXD3vvPOOunbtan8wWEJCgr7++utKj3GVc3dWdfvoSuevPC+//LIsFovDQ1nL42rn8ayq9M/VzuH06dPL1NuhQ4dKj3HG+SPcVFFhYaFiY2M1Z86cKrVPS0vTwIED1b9/f23ZskUTJkzQgw8+qGXLltVxpTVX3T6etWfPHmVkZNi3Jk2a1FGFl2blypV67LHHtG7dOi1fvlynT5/WTTfdpMLCwgqPWbNmjYYMGaJRo0Zp8+bNSkpKUlJSknbs2FGPlVdNTfon2Z4iev75O3DgQD1VXD3NmzfXyy+/rI0bN2rDhg26/vrrdfvtt2vnzp3ltnelc3dWdfsouc75u9BPP/2k9957T127dq20nSueR6nq/ZNc7xx27tzZod4ffvihwrZOO38Gqk2SsXjx4krbPPnkk0bnzp0d9g0ePNhITEysw8pqT1X6+N133xmSjGPHjtVLTbUtOzvbkGSsXLmywjb33nuvMXDgQId98fHxxsMPP1zX5V2yqvRv7ty5RnBwcP0VVctCQ0ONDz74oNz3XPncna+yPrrq+Tt+/LjRtm1bY/ny5cZ1111njB8/vsK2rngeq9M/VzuH06ZNM2JjY6vc3lnnj5GbOrJ27VoNGDDAYV9iYqLWrl3rpIrqTrdu3RQVFaUbb7xRq1evdnY5VZaXlydJCgsLq7CNK5/HqvRPkgoKCtSyZUtFR0dfdJSgoSgpKdGCBQtUWFhY4bpyrnzupKr1UXLN8/fYY49p4MCBZc5PeVzxPFanf5LrncO9e/eqadOmatWqlYYOHaqDBw9W2NZZ569BPKHYjDIzM+1LSJwVERGh/Px8nTx5Ur6+vk6qrPZERUXp3XffVc+ePVVUVKQPPvhA/fr1048//qju3bs7u7xKWa1WTZgwQb1796506Y6KzmNDnVd0VlX71759e3300Ufq2rWr8vLy9Prrr6tXr17auXOnmjdvXo8VV8327duVkJCgU6dOKSAgQIsXL1anTp3Kbeuq5646fXS18ydJCxYs0KZNm/TTTz9Vqb2rncfq9s/VzmF8fLzmzZun9u3bKyMjQzNmzFDfvn21Y8cOBQYGlmnvrPNHuEGNtW/fXu3bt7e/7tWrl/bt26c///nP+sc//uHEyi7uscce044dOyq9VuzKqtq/hIQEh1GBXr16qWPHjnrvvff0/PPP13WZ1da+fXtt2bJFeXl5+uyzz5ScnKyVK1dW+MvfFVWnj652/tLT0zV+/HgtX768QU+arama9M/VzuEtt9xi/7lr166Kj49Xy5YttWjRIo0aNcqJlTki3NSRyMhIZWVlOezLyspSUFCQKUZtKhIXF9fgA8PYsWP1n//8R6tWrbro34wqOo+RkZF1WeIlqU7/LuTp6amrrrpKv/zySx1Vd2m8vLzUpk0bSVKPHj30008/6S9/+Yvee++9Mm1d8dxJ1evjhRr6+du4caOys7MdRnZLSkq0atUqzZ49W0VFRXJ3d3c4xpXOY036d6GGfg4vFBISonbt2lVYr7POH3Nu6khCQoJSUlIc9i1fvrzSa+dmsGXLFkVFRTm7jHIZhqGxY8dq8eLF+vbbb3XFFVdc9BhXOo816d+FSkpKtH379gZ7Di9ktVpVVFRU7nuudO4qU1kfL9TQz98NN9yg7du3a8uWLfatZ8+eGjp0qLZs2VLuL35XOo816d+FGvo5vFBBQYH27dtXYb1OO391Ol3ZRI4fP25s3rzZ2Lx5syHJePPNN43NmzcbBw4cMAzDMCZNmmQMGzbM3v7XX381/Pz8jD/96U/Grl27jDlz5hju7u7G0qVLndWFi6puH//85z8bn3/+ubF3715j+/btxvjx4w03NzdjxYoVzupCpR555BEjODjYSE1NNTIyMuzbiRMn7G2GDRtmTJo0yf569erVhoeHh/H6668bu3btMqZNm2Z4enoa27dvd0YXKlWT/s2YMcNYtmyZsW/fPmPjxo3GfffdZ/j4+Bg7d+50RhcqNWnSJGPlypVGWlqasW3bNmPSpEmGxWIxvvnmG8MwXPvcnVXdPrrS+avIhXcTmeE8nu9i/XO1c/g///M/RmpqqpGWlmasXr3aGDBggNG4cWMjOzvbMIyGc/4IN1V09rbnC7fk5GTDMAwjOTnZuO6668oc061bN8PLy8to1aqVMXfu3Hqvuzqq28dXXnnFaN26teHj42OEhYUZ/fr1M7799lvnFF8F5fVNksN5ue666+z9PWvRokVGu3btDC8vL6Nz587GV199Vb+FV1FN+jdhwgSjRYsWhpeXlxEREWHceuutxqZNm+q/+Cp44IEHjJYtWxpeXl5GeHi4ccMNN9h/6RuGa5+7s6rbR1c6fxW58Je/Gc7j+S7WP1c7h4MHDzaioqIMLy8vo1mzZsbgwYONX375xf5+Qzl/FsMwjLodGwIAAKg/zLkBAACmQrgBAACmQrgBAACmQrgBAACmQrgBAACmQrgBAACmQrgBAACmQrgBcNmzWCz6/PPPnV0GgFpCuAHgVCNGjJDFYimz3Xzzzc4uDYCLYlVwAE538803a+7cuQ77vL29nVQNAFfHyA0Ap/P29lZkZKTDFhoaKsl2yeidd97RLbfcIl9fX7Vq1UqfffaZw/Hbt2/X9ddfL19fXzVq1EijR49WQUGBQ5uPPvpInTt3lre3t6KiojR27FiH93NycnTHHXfIz89Pbdu21Zdfflm3nQZQZwg3ABq8Z599VnfddZe2bt2qoUOH6r777tOuXbskSYWFhUpMTFRoaKh++uknffrpp1qxYoVDeHnnnXf02GOPafTo0dq+fbu+/PJLtWnTxuE7ZsyYoXvvvVfbtm3TrbfeqqFDh+ro0aP12k8AtaTOl+YEgEokJycb7u7uhr+/v8P24osvGoZhW+18zJgxDsfEx8cbjzzyiGEYhvG3v/3NCA0NNQoKCuzvf/XVV4abm5uRmZlpGIZhNG3a1JgyZUqFNUgynnnmGfvrgoICQ5Lx9ddf11o/AdQf5twAcLr+/fvrnXfecdgXFhZm/zkhIcHhvYSEBG3ZskWStGvXLsXGxsrf39/+fu/evWW1WrVnzx5ZLBYdPnxYN9xwQ6U1dO3a1f6zv7+/goKClJ2dXdMuAXAiwg0Ap/P39y9zmai2+Pr6Vqmdp6enw2uLxSKr1VoXJQGoY8y5AdDgrVu3rszrjh07SpI6duyorVu3qrCw0P7+6tWr5ebmpvbt2yswMFAxMTFKSUmp15oBOA8jNwCcrqioSJmZmQ77PDw81LhxY0nSp59+qp49e6pPnz765z//qfXr1+vDDz+UJA0dOlTTpk1TcnKypk+friNHjmjcuHEaNmyYIiIiJEnTp0/XmDFj1KRJE91yyy06fvy4Vq9erXHjxtVvRwHUC8INAKdbunSpoqKiHPa1b99eu3fvlmS7k2nBggV69NFHFRUVpU8++USdOnWSJPn5+WnZsmUaP368rr76avn5+emuu+7Sm2++af+s5ORknTp1Sn/+85/1xBNPqHHjxrr77rvrr4MA6pXFMAzD2UUAQEUsFosWL16spKQkZ5cCwEUw5wYAAJgK4QYAAJgKc24ANGhcOQdQXYzcAAAAUyHcAAAAUyHcAAAAUyHcAAAAUyHcAAAAUyHcAAAAUyHcAAAAUyHcAAAAUyHcAAAAU/n/gu8rf7jOfFwAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plot the training and validation losses\n",
    "\n",
    "#Convert loss results into a dataframe\n",
    "result_preproc = pd.DataFrame({\n",
    "    'Epoch': [i+1 for i in range(len(results[\"loss\"]))], \n",
    "    'Train': results[\"loss\"],\n",
    "    'Validate': results[\"val_loss\"]\n",
    "    })\n",
    "\n",
    "# Convert dataframe from wide to long format\n",
    "df = pd.melt(result_preproc, ['Epoch'])\n",
    "\n",
    "#Make plot\n",
    "g = sns.lineplot(data=df, x='Epoch', y='value', hue='variable')\n",
    "g.set_title(\"Loss Curves\")\n",
    "g.legend_.set_title(\"Loss\")\n",
    "g.set_ylabel('Loss')\n",
    "g.set_ylim(0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9) Predict Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simple simple function to ititialize the test dataset using global variables\n",
    "def init_test_dataset():\n",
    "    return make_dataset(hdf5_file, test_meta_dict, test_pos_isic_id, test_pos_target, batch_size = test_batch_size, apply_hair_removal=apply_hair_removal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total test batches in dataset: 25\n"
     ]
    }
   ],
   "source": [
    "#Test dataset basic size information: nb of samples and batch size\n",
    "nb_test_batches = int(np.ceil(len(test_meta_dict)/test_batch_size))\n",
    "print(\"Total test batches in dataset:\", nb_test_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retrieve real values for the target in the test dataset\n",
    "y_test = []\n",
    "test_dataset = init_test_dataset()\n",
    "for item in test_dataset.take(nb_test_batches):\n",
    "    img_meta, targ = item\n",
    "    y_test.extend(targ.numpy().flatten())\n",
    "#Convert to numpy array (mathematical operations are faster)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 88ms/step\n",
      "Shape of prediction data: (790, 1)\n"
     ]
    }
   ],
   "source": [
    "#Reinitialize the test dataset (necessary to start at beginning)\n",
    "test_dataset = init_test_dataset()\n",
    "#Retrieve predictions\n",
    "predictions = model.predict(test_dataset, steps = nb_test_batches)\n",
    "#Put predictionsin a numpy array\n",
    "y_pred = np.array([round(i) for i  in predictions.flatten()])\n",
    "print(\"Shape of prediction data:\", predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#Import results from file\\nimported_test_results = pd.read_csv(testResPath)\\ny_test = np.array(imported_test_results[\"y_test\"])\\ny_pred = np.array(imported_test_results[\"y_pred\"])\\ndel imported_test_results\\n'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Save test results to file\n",
    "pd.DataFrame({\"y_test\": y_test, \"y_pred\": y_pred}).to_csv(testResPath, index=False)\n",
    "\n",
    "\"\"\"\n",
    "#Import results from file\n",
    "imported_test_results = pd.read_csv(testResPath)\n",
    "y_test = np.array(imported_test_results[\"y_test\"])\n",
    "y_pred = np.array(imported_test_results[\"y_pred\"])\n",
    "del imported_test_results\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the loss\n",
    "loss = sum(abs(y_test - y_pred))/len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Determine true/false positives and negatives\n",
    "pos_indices = y_test == 1\n",
    "neg_indices = y_test == 0\n",
    "\n",
    "#True positives\n",
    "true_pos = sum(abs(y_test[pos_indices] == y_pred[pos_indices]))\n",
    "\n",
    "#False negatives\n",
    "false_neg = sum(abs(y_test[pos_indices] != y_pred[pos_indices]))\n",
    "\n",
    "#True negatives\n",
    "true_neg = sum(abs(y_test[neg_indices] == y_pred[neg_indices]))\n",
    "\n",
    "#False positives\n",
    "false_pos = sum(abs(y_test[neg_indices] != y_pred[neg_indices]))\n",
    "\n",
    "#Precision\n",
    "try:\n",
    "    precision = true_pos / (true_pos + false_pos)\n",
    "except:\n",
    "    precision = np.nan\n",
    "\n",
    "#Recall (sensitivity)\n",
    "try:\n",
    "    recall = true_pos / (true_pos + false_neg)\n",
    "except:\n",
    "    recall = np.nan\n",
    "\n",
    "#Specificity\n",
    "try:\n",
    "    specificity = true_neg / (true_neg + false_pos)\n",
    "except:\n",
    "    specificity = np.nan\n",
    "\n",
    "#F1 Score\n",
    "try:\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "except:\n",
    "    f1_score = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---TEST RESULTS---\n",
      "True positives: 1\n",
      "False positives: 103\n",
      "True negatives: 685\n",
      "False negatives: 1\n",
      "\n",
      "Sensitivity: 0.5\n",
      "Specificity: 0.8692893401015228\n",
      "\n",
      "Precision: 0.009615384615384616\n",
      "Recall: 0.5\n",
      "\n",
      "F1 Score: 0.01886792452830189\n",
      "Loss on test data: 0.13164556962025317\n"
     ]
    }
   ],
   "source": [
    "print(\"---TEST RESULTS---\")\n",
    "print(\"True positives:\", true_pos)\n",
    "print(\"False positives:\", false_pos)\n",
    "print(\"True negatives:\", true_neg)\n",
    "print(\"False negatives:\", false_neg)\n",
    "print()\n",
    "print(\"Sensitivity:\", recall)\n",
    "print(\"Specificity:\", specificity)\n",
    "print()\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print()\n",
    "print(\"F1 Score:\", f1_score)\n",
    "print(\"Loss on test data:\", loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cancer_isic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

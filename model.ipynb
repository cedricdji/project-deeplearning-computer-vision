{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL - IMAGE LOADING & NEURAL NETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras-cv==0.9.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (0.9.0)\n",
      "Requirement already satisfied: packaging in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from keras-cv==0.9.0) (21.3)\n",
      "Requirement already satisfied: absl-py in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from keras-cv==0.9.0) (2.1.0)\n",
      "Requirement already satisfied: regex in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from keras-cv==0.9.0) (2024.9.11)\n",
      "Requirement already satisfied: tensorflow-datasets in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from keras-cv==0.9.0) (4.9.6)\n",
      "Requirement already satisfied: keras-core in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from keras-cv==0.9.0) (0.1.7)\n",
      "Requirement already satisfied: kagglehub in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from keras-cv==0.9.0) (0.3.3)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from kagglehub->keras-cv==0.9.0) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from kagglehub->keras-cv==0.9.0) (4.66.5)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from keras-core->keras-cv==0.9.0) (1.26.4)\n",
      "Requirement already satisfied: rich in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from keras-core->keras-cv==0.9.0) (13.8.1)\n",
      "Requirement already satisfied: namex in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from keras-core->keras-cv==0.9.0) (0.0.8)\n",
      "Requirement already satisfied: h5py in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from keras-core->keras-cv==0.9.0) (3.11.0)\n",
      "Requirement already satisfied: dm-tree in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from keras-core->keras-cv==0.9.0) (0.1.8)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from packaging->keras-cv==0.9.0) (3.1.4)\n",
      "Requirement already satisfied: click in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from tensorflow-datasets->keras-cv==0.9.0) (8.1.7)\n",
      "Requirement already satisfied: immutabledict in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from tensorflow-datasets->keras-cv==0.9.0) (4.2.0)\n",
      "Requirement already satisfied: promise in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from tensorflow-datasets->keras-cv==0.9.0) (2.3)\n",
      "Requirement already satisfied: protobuf>=3.20 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from tensorflow-datasets->keras-cv==0.9.0) (3.20.3)\n",
      "Requirement already satisfied: psutil in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from tensorflow-datasets->keras-cv==0.9.0) (6.0.0)\n",
      "Requirement already satisfied: pyarrow in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from tensorflow-datasets->keras-cv==0.9.0) (17.0.0)\n",
      "Requirement already satisfied: simple-parsing in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from tensorflow-datasets->keras-cv==0.9.0) (0.1.6)\n",
      "Requirement already satisfied: tensorflow-metadata in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from tensorflow-datasets->keras-cv==0.9.0) (1.16.1)\n",
      "Requirement already satisfied: termcolor in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from tensorflow-datasets->keras-cv==0.9.0) (2.4.0)\n",
      "Requirement already satisfied: toml in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from tensorflow-datasets->keras-cv==0.9.0) (0.10.2)\n",
      "Requirement already satisfied: wrapt in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from tensorflow-datasets->keras-cv==0.9.0) (1.16.0)\n",
      "Requirement already satisfied: array-record>=0.5.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from tensorflow-datasets->keras-cv==0.9.0) (0.5.1)\n",
      "Requirement already satisfied: etils>=1.6.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from etils[enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow-datasets->keras-cv==0.9.0) (1.10.0)\n",
      "Requirement already satisfied: typing_extensions in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from etils[enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow-datasets->keras-cv==0.9.0) (4.12.2)\n",
      "Requirement already satisfied: fsspec in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from etils[enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow-datasets->keras-cv==0.9.0) (2024.9.0)\n",
      "Requirement already satisfied: importlib_resources in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from etils[enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow-datasets->keras-cv==0.9.0) (6.4.5)\n",
      "Requirement already satisfied: zipp in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from etils[enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow-datasets->keras-cv==0.9.0) (3.20.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from requests->kagglehub->keras-cv==0.9.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from requests->kagglehub->keras-cv==0.9.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from requests->kagglehub->keras-cv==0.9.0) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from requests->kagglehub->keras-cv==0.9.0) (2024.8.30)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from promise->tensorflow-datasets->keras-cv==0.9.0) (1.16.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from rich->keras-core->keras-cv==0.9.0) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from rich->keras-core->keras-cv==0.9.0) (2.18.0)\n",
      "Requirement already satisfied: docstring-parser<1.0,>=0.15 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from simple-parsing->tensorflow-datasets->keras-cv==0.9.0) (0.16)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras-core->keras-cv==0.9.0) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install keras-cv==0.9.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: absl-py==2.1.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 1)) (2.1.0)\n",
      "Requirement already satisfied: alembic==1.13.3 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 2)) (1.13.3)\n",
      "Requirement already satisfied: annotated-types==0.7.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 3)) (0.7.0)\n",
      "Collecting ansible==10.5.0 (from -r requirements.txt (line 4))\n",
      "  Using cached ansible-10.5.0-py3-none-any.whl.metadata (8.0 kB)\n",
      "Collecting ansible-core==2.17.5 (from -r requirements.txt (line 5))\n",
      "  Using cached ansible_core-2.17.5-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: asttokens==2.4.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 6)) (2.4.1)\n",
      "Requirement already satisfied: astunparse==1.6.3 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 7)) (1.6.3)\n",
      "Requirement already satisfied: attrs==23.2.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 8)) (23.2.0)\n",
      "Requirement already satisfied: beautifulsoup4==4.12.3 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 9)) (4.12.3)\n",
      "Requirement already satisfied: setuptools==58.0.4 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 10)) (58.0.4)\n",
      "Requirement already satisfied: promise==2.3 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 11)) (2.3)\n",
      "Requirement already satisfied: keras-cv==0.9.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 12)) (0.9.0)\n",
      "Requirement already satisfied: bleach==6.1.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 13)) (6.1.0)\n",
      "Requirement already satisfied: blinker==1.8.2 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 14)) (1.8.2)\n",
      "Collecting boto3==1.35.44 (from -r requirements.txt (line 15))\n",
      "  Using cached boto3-1.35.44-py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting botocore==1.35.44 (from -r requirements.txt (line 16))\n",
      "  Using cached botocore-1.35.44-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: cachetools==5.5.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 17)) (5.5.0)\n",
      "Requirement already satisfied: certifi==2024.8.30 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 18)) (2024.8.30)\n",
      "Requirement already satisfied: cffi==1.17.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 19)) (1.17.1)\n",
      "Requirement already satisfied: charset-normalizer==3.3.2 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 20)) (3.3.2)\n",
      "Requirement already satisfied: click==8.1.7 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 21)) (8.1.7)\n",
      "Requirement already satisfied: cloudpickle==2.2.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 22)) (2.2.1)\n",
      "Requirement already satisfied: colorama==0.4.6 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 23)) (0.4.6)\n",
      "Requirement already satisfied: comm==0.2.2 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 24)) (0.2.2)\n",
      "Requirement already satisfied: contourpy==1.3.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 25)) (1.3.0)\n",
      "Collecting cryptography==43.0.3 (from -r requirements.txt (line 26))\n",
      "  Using cached cryptography-43.0.3-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: cycler==0.12.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 27)) (0.12.1)\n",
      "Requirement already satisfied: databricks-sdk==0.35.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 28)) (0.35.0)\n",
      "Collecting debugpy==1.8.5 (from -r requirements.txt (line 29))\n",
      "  Using cached debugpy-1.8.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: decorator==5.1.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 30)) (5.1.1)\n",
      "Requirement already satisfied: defusedxml==0.7.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 31)) (0.7.1)\n",
      "Requirement already satisfied: Deprecated==1.2.14 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 32)) (1.2.14)\n",
      "Collecting dill==0.3.9 (from -r requirements.txt (line 33))\n",
      "  Using cached dill-0.3.9-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: dm-tree==0.1.8 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 34)) (0.1.8)\n",
      "Requirement already satisfied: docker==7.1.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 35)) (7.1.0)\n",
      "Requirement already satisfied: docstring_parser==0.16 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 36)) (0.16)\n",
      "Requirement already satisfied: etils==1.10.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 37)) (1.10.0)\n",
      "Requirement already satisfied: executing==2.1.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 38)) (2.1.0)\n",
      "Requirement already satisfied: fastjsonschema==2.20.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 39)) (2.20.0)\n",
      "Requirement already satisfied: Flask==3.0.3 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 40)) (3.0.3)\n",
      "Requirement already satisfied: flatbuffers==24.3.25 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 41)) (24.3.25)\n",
      "Collecting fonttools==4.53.1 (from -r requirements.txt (line 42))\n",
      "  Using cached fonttools-4.53.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (162 kB)\n",
      "Requirement already satisfied: fsspec==2024.9.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 43)) (2024.9.0)\n",
      "Requirement already satisfied: gast==0.6.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 44)) (0.6.0)\n",
      "Requirement already satisfied: gitdb==4.0.11 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 45)) (4.0.11)\n",
      "Requirement already satisfied: GitPython==3.1.43 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 46)) (3.1.43)\n",
      "Requirement already satisfied: google-auth==2.35.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 47)) (2.35.0)\n",
      "Requirement already satisfied: google-pasta==0.2.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 48)) (0.2.0)\n",
      "Collecting googleapis-common-protos==1.65.0 (from -r requirements.txt (line 49))\n",
      "  Using cached googleapis_common_protos-1.65.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting graphene==3.4 (from -r requirements.txt (line 50))\n",
      "  Using cached graphene-3.4-py2.py3-none-any.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: graphql-core==3.2.5 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 51)) (3.2.5)\n",
      "Requirement already satisfied: graphql-relay==3.2.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 52)) (3.2.0)\n",
      "Requirement already satisfied: greenlet==3.1.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 53)) (3.1.1)\n",
      "Requirement already satisfied: grpcio==1.66.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 54)) (1.66.1)\n",
      "Requirement already satisfied: h5py==3.11.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 55)) (3.11.0)\n",
      "Requirement already satisfied: idna==3.10 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 56)) (3.10)\n",
      "Requirement already satisfied: immutabledict==4.2.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 57)) (4.2.0)\n",
      "Requirement already satisfied: importlib-metadata==6.11.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 58)) (6.11.0)\n",
      "Requirement already satisfied: importlib_resources==6.4.5 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 59)) (6.4.5)\n",
      "Requirement already satisfied: ipykernel==6.29.5 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 60)) (6.29.5)\n",
      "Requirement already satisfied: ipython==8.27.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 61)) (8.27.0)\n",
      "Requirement already satisfied: itsdangerous==2.2.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 62)) (2.2.0)\n",
      "Requirement already satisfied: jedi==0.19.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 63)) (0.19.1)\n",
      "Requirement already satisfied: Jinja2==3.1.4 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 64)) (3.1.4)\n",
      "Requirement already satisfied: jmespath==1.0.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 65)) (1.0.1)\n",
      "Requirement already satisfied: joblib==1.4.2 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 66)) (1.4.2)\n",
      "Requirement already satisfied: jsonschema==4.23.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 67)) (4.23.0)\n",
      "Requirement already satisfied: jsonschema-specifications==2023.12.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 68)) (2023.12.1)\n",
      "Requirement already satisfied: jupyter_client==8.6.3 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 69)) (8.6.3)\n",
      "Requirement already satisfied: jupyter_core==5.7.2 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 70)) (5.7.2)\n",
      "Requirement already satisfied: jupyterlab_pygments==0.3.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 71)) (0.3.0)\n",
      "Requirement already satisfied: kagglehub==0.3.3 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 72)) (0.3.3)\n",
      "Requirement already satisfied: keras==3.5.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 73)) (3.5.0)\n",
      "Requirement already satisfied: keras-core==0.1.7 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 74)) (0.1.7)\n",
      "Requirement already satisfied: kiwisolver==1.4.7 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 75)) (1.4.7)\n",
      "Requirement already satisfied: libclang==18.1.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 76)) (18.1.1)\n",
      "Requirement already satisfied: Mako==1.3.5 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 77)) (1.3.5)\n",
      "Requirement already satisfied: Markdown==3.7 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 78)) (3.7)\n",
      "Requirement already satisfied: markdown-it-py==3.0.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 79)) (3.0.0)\n",
      "Requirement already satisfied: MarkupSafe==2.1.5 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 80)) (2.1.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib==3.8.4 (from -r requirements.txt (line 81))\n",
      "  Using cached matplotlib-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: matplotlib-inline==0.1.7 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 82)) (0.1.7)\n",
      "Requirement already satisfied: mdurl==0.1.2 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 83)) (0.1.2)\n",
      "Requirement already satisfied: mistune==3.0.2 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 84)) (3.0.2)\n",
      "Collecting ml-dtypes==0.4.1 (from -r requirements.txt (line 85))\n",
      "  Using cached ml_dtypes-0.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: mlflow==2.17.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 86)) (2.17.0)\n",
      "Requirement already satisfied: mlflow-skinny==2.17.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 87)) (2.17.0)\n",
      "Requirement already satisfied: mock==4.0.3 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 88)) (4.0.3)\n",
      "Collecting multiprocess==0.70.17 (from -r requirements.txt (line 89))\n",
      "  Using cached multiprocess-0.70.17-py310-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: namex==0.0.8 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 90)) (0.0.8)\n",
      "Requirement already satisfied: nbclient==0.10.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 91)) (0.10.0)\n",
      "Requirement already satisfied: nbconvert==7.16.4 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 92)) (7.16.4)\n",
      "Requirement already satisfied: nbformat==5.10.4 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 93)) (5.10.4)\n",
      "Requirement already satisfied: nest-asyncio==1.6.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 94)) (1.6.0)\n",
      "Requirement already satisfied: numpy==1.26.4 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 95)) (1.26.4)\n",
      "Requirement already satisfied: opencv-python==4.10.0.84 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 96)) (4.10.0.84)\n",
      "Requirement already satisfied: opentelemetry-api==1.27.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 97)) (1.27.0)\n",
      "Requirement already satisfied: opentelemetry-sdk==1.27.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 98)) (1.27.0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.48b0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 99)) (0.48b0)\n",
      "Collecting opt-einsum==3.3.0 (from -r requirements.txt (line 100))\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: optree==0.12.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 101)) (0.12.1)\n",
      "Collecting packaging==24.1 (from -r requirements.txt (line 102))\n",
      "  Using cached packaging-24.1-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting pandas==2.2.2 (from -r requirements.txt (line 103))\n",
      "  Using cached pandas-2.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
      "Collecting pandocfilters==1.5.1 (from -r requirements.txt (line 104))\n",
      "  Using cached pandocfilters-1.5.1-py2.py3-none-any.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: parso==0.8.4 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 105)) (0.8.4)\n",
      "Collecting pathos==0.3.3 (from -r requirements.txt (line 106))\n",
      "  Using cached pathos-0.3.3-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: pillow==10.4.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 107)) (10.4.0)\n",
      "Requirement already satisfied: platformdirs==4.3.6 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 108)) (4.3.6)\n",
      "Collecting pox==0.3.5 (from -r requirements.txt (line 109))\n",
      "  Using cached pox-0.3.5-py3-none-any.whl.metadata (8.0 kB)\n",
      "Collecting ppft==1.7.6.9 (from -r requirements.txt (line 110))\n",
      "  Using cached ppft-1.7.6.9-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting prompt_toolkit==3.0.47 (from -r requirements.txt (line 111))\n",
      "  Using cached prompt_toolkit-3.0.47-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting protobuf==4.25.5 (from -r requirements.txt (line 112))\n",
      "  Using cached protobuf-4.25.5-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
      "Requirement already satisfied: psutil==6.0.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 113)) (6.0.0)\n",
      "Requirement already satisfied: pure_eval==0.2.3 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 114)) (0.2.3)\n",
      "Requirement already satisfied: pyarrow==17.0.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 115)) (17.0.0)\n",
      "Requirement already satisfied: pyasn1==0.6.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 116)) (0.6.1)\n",
      "Requirement already satisfied: pyasn1_modules==0.4.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 117)) (0.4.1)\n",
      "Requirement already satisfied: pycparser==2.22 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 118)) (2.22)\n",
      "Requirement already satisfied: pydantic==2.9.2 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 119)) (2.9.2)\n",
      "Requirement already satisfied: pydantic_core==2.23.4 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 120)) (2.23.4)\n",
      "Requirement already satisfied: Pygments==2.18.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 121)) (2.18.0)\n",
      "Requirement already satisfied: pyparsing==3.1.4 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 122)) (3.1.4)\n",
      "Collecting python-dateutil==2.9.0.post0 (from -r requirements.txt (line 123))\n",
      "  Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: pytz==2024.2 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 124)) (2024.2)\n",
      "\u001b[31mERROR: Ignored the following versions that require a different python version: 11.0.0a1 Requires-Python >=3.11; 11.0.0a2 Requires-Python >=3.11; 2.18.0b1 Requires-Python >=3.11; 2.18.0rc1 Requires-Python >=3.11\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement pywin32==306 (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for pywin32==306\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-25 11:45:19.866589: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-25 11:45:19.891538: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-25 11:45:19.891575: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-25 11:45:19.906820: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-25 11:45:20.974676: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend\n"
     ]
    }
   ],
   "source": [
    "#Import libraries\n",
    "import boto3\n",
    "import sagemaker\n",
    "import gc\n",
    "import csv\n",
    "import keras_cv\n",
    "import os\n",
    "import io\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import random\n",
    "from collections.abc import Generator\n",
    "import itertools\n",
    "import datetime\n",
    "\n",
    "TF_ENABLE_ONEDNN_OPTS=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) HYPERPARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fraction of data to use (CEDRIC can reduce to speed up tests... go to 0.02!)\n",
    "train_frac_to_use = 0.03   #Reduce training data to this fraction. Set to \"1\" to use all data.\n",
    "val_frac_to_use = 0.03     #Reduce validation data to this fraction. Set to \"1\" to use all data.\n",
    "test_frac_to_use = 0.03     #Reduce validation data to this fraction. Set to \"1\" to use all data.\n",
    "\n",
    "#Memory management (Speeds up model and currently works with a machine with 16GB of memory)\n",
    "save_val_in_memory = False  #Place all validation directly in memory to accelerate the validation step in the model\n",
    "\n",
    "#Splitting of train-validate-test\n",
    "split_seed = 88                 #Seed to use for all train-validate-test splits, including the split of reserved Target=1 data\n",
    "reserve_frac = 0.1              #Fraction of total original data of Target = 1 (reserved for use in validation data)\n",
    "test_frac = 0.2                 #Fraction of total original data, excluding the reserved fraction, to use as the test data\n",
    "nb_of_augments = 100            #Number of augments to perform on Target = 1 images in train-validate sets\n",
    "val_frac = 0.33                 #Fraction of augmented train-validate list to use as the validation data. The rest becomes the training data.\n",
    "nb_of_augments_reserved = 15    #Number of augmentations to perform on reserved validation fraction (Target = 1). Note: this is added to the validation data.\n",
    "reduce_frac = 0.8               #Fraction of Target = 0 samples to remove from the validation data (improves balance)\n",
    "\n",
    "#Image resizing: all images are adjusted to this size so the the CNN receives same number of data points each time\n",
    "imgSize = 100\n",
    "\n",
    "#Hair removal (applies to all images)\n",
    "apply_hair_removal = False\n",
    "\n",
    "#Batch sizes\n",
    "train_batch_size = 32\n",
    "val_batch_size = 32\n",
    "test_batch_size = 32\n",
    "\n",
    "#Neural Network Parameters\n",
    "nb_neurons_hidden_layers = 36  #Number of neurons in each hidden layer\n",
    "dropout = 0.1                  #Fraction of neurons to drop\n",
    "\n",
    "#Optimizer Parameters\n",
    "learning_rate = 0.01           #Initial learning rate for the Adam optimizer\n",
    "\n",
    "#Epoch management (CEDRIC... reduce number of epochs if needed)\n",
    "nb_epochs = 5\n",
    "early_break = False #End early in case of increasing validation loss\n",
    "wt_save_freq = 1 #Frequency of weights saving to file (epochs/save)\n",
    "\n",
    "#Debugging\n",
    "cheat = False #add the target to the metadata so the model can precisely learn the correct response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) IMPORT DATA & DECLARE SAVES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 - Declare file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le fichier /home/ec2-user/SageMaker/train-image.hdf5 existe déjà. Téléchargement sauté.\n"
     ]
    }
   ],
   "source": [
    "# Configuration S3 et chemin local\n",
    "session = sagemaker.Session()\n",
    "bucket = 'images-projet-deep-learning'\n",
    "hdf5_key = 'train-image.hdf5'\n",
    "local_hdf5_path = os.path.join(os.getcwd(), 'train-image.hdf5')\n",
    "\n",
    "# Vérifie si le fichier est déjà présent\n",
    "if os.path.exists(local_hdf5_path):\n",
    "    print(f\"Le fichier {local_hdf5_path} existe déjà. Téléchargement sauté.\")\n",
    "else:\n",
    "    # Télécharger le fichier HDF5 depuis S3\n",
    "    print(f\"Téléchargement de {hdf5_key} depuis S3 vers {local_hdf5_path}...\")\n",
    "    s3 = boto3.client('s3')\n",
    "    s3.download_file(bucket, hdf5_key, local_hdf5_path)\n",
    "    print(\"Téléchargement terminé.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORT FILES\n",
    "\n",
    "\n",
    "dataPath = \"s3://images-projet-deep-learning/\" #slash required at end\n",
    "#Metadata file paths\n",
    "metaPath = dataPath + \"cleaned-metadata.csv\"\n",
    "#Image file path\n",
    "hdf5_file = \"train-image.hdf5\"\n",
    "\n",
    "#SAVE FILES\n",
    "#Directory for saved files \n",
    "savePath = \"s3://dsti-a23-deep-learning-outputs/\" #slash required at end\n",
    "#Model results\n",
    "modelResPath = savePath + \"model_results_save.csv\"\n",
    "#Test results (y_test, y_pred)\n",
    "testResPath = savePath + \"test_results_save.csv\"\n",
    "\n",
    "\n",
    "#ALTERNATIVE 1 QUI MARCHE PARFOIS\n",
    "#base_path = \"C:/Users/admin/Documents/DSTI/DeepLearning/Project/Computer_Vision/isic-2024-challenge\"\n",
    "#hdf5_file = os.path.join(base_path, \"sampleclaire-image.hdf5\")\n",
    "#metadata = pd.read_csv(metaPath, sep=\",\")\n",
    "\n",
    "#ALTERNATIVE 2 QUI MARCHE PARFOIS\n",
    "#base_path = \"C:/Users/admin/Documents/DSTI/DeepLearning/Project/Computer_Vision/isic-2024-challenge\"\n",
    "#metadata = pd.read_csv(os.path.join(base_path, \"train-metadata.csv\"))\n",
    "#hdf5_file = os.path.join(base_path, \"train-image.hdf5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "savePath = \"s3://dsti-a23-deep-learning-outputs/\" #slash required at end\n",
    "#Model results\n",
    "modelResPath = savePath + \"model_results_save.csv\"\n",
    "#Test results (y_test, y_pred)\n",
    "testResPath = savePath + \"test_results_save.csv\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 - Load metadata from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/fsspec/registry.py:279: UserWarning: Your installed version of s3fs is very old and known to cause\n",
      "severe performance issues, see also https://github.com/dask/dask/issues/10276\n",
      "\n",
      "To fix, you should specify a lower version bound on s3fs, or\n",
      "update the current installation.\n",
      "\n",
      "  warnings.warn(s3_msg)\n",
      "/tmp/ipykernel_25872/327758908.py:2: DtypeWarning: Columns (51,52) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  metadata = pd.read_csv(metaPath, sep=\",\")\n"
     ]
    }
   ],
   "source": [
    "#Import metadata from file\n",
    "metadata = pd.read_csv(metaPath, sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- X_meta NA counts --\n",
      "isic_id                           0\n",
      "age_approx                     2798\n",
      "target                            0\n",
      "clin_size_long_diam_mm            0\n",
      "tbp_lv_areaMM2                    0\n",
      "tbp_lv_area_perim_ratio           0\n",
      "tbp_lv_eccentricity               0\n",
      "tbp_lv_minorAxisMM                0\n",
      "tbp_lv_color_std_mean             0\n",
      "tbp_lv_deltaLBnorm                0\n",
      "tbp_lv_radial_color_std_max       0\n",
      "tbp_lv_location                   0\n",
      "dtype: int64\n",
      "Number of unknown for tbp_lv_location 5756\n"
     ]
    }
   ],
   "source": [
    "#METADATA: color and size features having no NAs\n",
    "metadata = metadata[[\"isic_id\",\n",
    "                     \"age_approx\",\n",
    "                     \"target\",\n",
    "                     \"clin_size_long_diam_mm\",\n",
    "                     \"tbp_lv_areaMM2\",\n",
    "                     \"tbp_lv_area_perim_ratio\",\n",
    "                     \"tbp_lv_eccentricity\",\n",
    "                     \"tbp_lv_minorAxisMM\",\n",
    "                     \"tbp_lv_color_std_mean\",\n",
    "                     \"tbp_lv_deltaLBnorm\",\n",
    "                     \"tbp_lv_radial_color_std_max\",\n",
    "                     \"tbp_lv_location\"]]\n",
    "\n",
    "#Verify that there are no NAs\n",
    "print(\"-- X_meta NA counts --\")\n",
    "print(metadata.isna().sum())\n",
    "\n",
    "#Check number of Unknoxn for tbp_lv_location\n",
    "loc_unknown=metadata[metadata[\"tbp_lv_location\"]==\"Unknown\"]\n",
    "print(\"Number of unknown for tbp_lv_location\", len(loc_unknown))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Activate for debugging of the predict function\n",
    "if cheat:\n",
    "    metadata[\"target_cheat\"] = metadata[\"target\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 - Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata=metadata[metadata[\"tbp_lv_location\"]!=\"Unknown\"]\n",
    "\n",
    "loc_unknown2=metadata[metadata[\"tbp_lv_location\"]==\"Unknown\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply One-hot encoding for location\n",
    "location=pd.get_dummies(metadata[\"tbp_lv_location\"],prefix='category')\n",
    "location = location.astype(int)\n",
    "metadata = pd.concat([metadata, location], axis=1)\n",
    "metadata=metadata.drop(\"tbp_lv_location\",axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- X_meta NA counts --\n",
      "isic_id                             0\n",
      "age_approx                          0\n",
      "target                              0\n",
      "clin_size_long_diam_mm              0\n",
      "tbp_lv_areaMM2                      0\n",
      "tbp_lv_area_perim_ratio             0\n",
      "tbp_lv_eccentricity                 0\n",
      "tbp_lv_minorAxisMM                  0\n",
      "tbp_lv_color_std_mean               0\n",
      "tbp_lv_deltaLBnorm                  0\n",
      "tbp_lv_radial_color_std_max         0\n",
      "category_Head & Neck                0\n",
      "category_Left Arm                   0\n",
      "category_Left Arm - Lower           0\n",
      "category_Left Arm - Upper           0\n",
      "category_Left Leg                   0\n",
      "category_Left Leg - Lower           0\n",
      "category_Left Leg - Upper           0\n",
      "category_Right Arm                  0\n",
      "category_Right Arm - Lower          0\n",
      "category_Right Arm - Upper          0\n",
      "category_Right Leg                  0\n",
      "category_Right Leg - Lower          0\n",
      "category_Right Leg - Upper          0\n",
      "category_Torso Back                 0\n",
      "category_Torso Back Bottom Third    0\n",
      "category_Torso Back Middle Third    0\n",
      "category_Torso Back Top Third       0\n",
      "category_Torso Front                0\n",
      "category_Torso Front Bottom Half    0\n",
      "category_Torso Front Top Half       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Calculate the mean of age_approx for each target group\n",
    "mean_age_malign = metadata.loc[metadata[\"target\"] == 1, \"age_approx\"].mean()\n",
    "mean_age_benign = metadata.loc[metadata[\"target\"] == 0, \"age_approx\"].mean()\n",
    "\n",
    "# Define a function to fill NA based on the target value\n",
    "def fill_na_by_target(row):\n",
    "    if pd.isna(row['age_approx']):\n",
    "        if row['target'] == 1:\n",
    "            return mean_age_malign\n",
    "        elif row['target'] == 0:\n",
    "            return mean_age_benign\n",
    "    return row['age_approx']\n",
    "\n",
    "# Apply the function to the age_approx column\n",
    "metadata['age_approx'] = metadata.apply(fill_na_by_target, axis=1)\n",
    "\n",
    "#Verify that there are no NAs\n",
    "print(\"-- X_meta NA counts --\")\n",
    "print(metadata.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Train, Validate, Test Split + Preparation of Data Augmentation\n",
    "1. Make two separate lists of isic_ids for target=0 and target=1. Transform into tuples (isic_id, target, mod toggle). Base data has mod toggle = 0, meaning no adjustment will be made.\n",
    "2. Reserve 10% of target = 1 for validate\n",
    "3. Split into train-validate & test on both lists (0 and 1). Take the test lists (0 and 1), concatenate and shuffle them\n",
    "4. Create augmentation preparation function for target = 1: mod toggle = strictly positive integer (this adds more isic_ids to the list, with mod toggle non zero))\n",
    "5. Run augmentation preparation function on train-validate and the reserved validation data: mod toggle = strictly positive integer\n",
    "6. Split train-validate on both lists (0 and 1)\n",
    "7. Reduce the validation data on target = 0 by value specified in reduce_frac\n",
    "8. Concatenate and shuffle the train lists and validation lists\n",
    "9. Limit training and validation data to speed up training (take only fraction of prepared lists)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 - Make two separate lists of isic_ids for target=0 (mod_toggle = -1) and target=1 (mod_toggle = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total ids with target = 0: 394910\n",
      "Total ids with target = 1: 393\n"
     ]
    }
   ],
   "source": [
    "#Make a list of isic_ids for each target value (0 and 1)\n",
    "isic_id_target_0 = metadata[metadata['target'] == 0]['isic_id'].tolist()\n",
    "isic_id_target_1 = metadata[metadata['target'] == 1]['isic_id'].tolist()\n",
    "\n",
    "#Retrieve dataframe with isic id and target\n",
    "temp_0 = metadata[metadata[\"isic_id\"].isin(isic_id_target_0)].loc[:,[\"isic_id\",\"target\"]]\n",
    "temp_1 = metadata[metadata[\"isic_id\"].isin(isic_id_target_1)].loc[:,[\"isic_id\",\"target\"]]\n",
    "\n",
    "#Convert into list of tuples... this makes it compatible with data augmentations\n",
    "#Form: (isic_id, target, mod toggle)\n",
    "isic_id_target_0 = list(zip(temp_0.iloc[:,0], temp_0.iloc[:,1], [-1]*len(temp_0)))\n",
    "isic_id_target_1 = list(zip(temp_1.iloc[:,0], temp_1.iloc[:,1], [0]*len(temp_1)))\n",
    "\n",
    "#Delete temporary dataframes (the original metadata dataframe is untouched)\n",
    "del temp_0\n",
    "del temp_1\n",
    "\n",
    "#Count the number of occurrences for each target value\n",
    "print(\"Total ids with target = 0:\", len(isic_id_target_0))\n",
    "print(\"Total ids with target = 1:\", len(isic_id_target_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 - Reserve 10% of target = 1 for validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total ids with target = 0: 394910\n",
      "Total ids with target = 1: 353\n",
      "Total reserved target = 1: 40\n"
     ]
    }
   ],
   "source": [
    "#Keep 10% of isic_Id of target=1 without duplication\n",
    "isic_id_target_1, isic_id_target_1_reserved = train_test_split(isic_id_target_1, test_size = reserve_frac, random_state=split_seed, shuffle=False)\n",
    "\n",
    "#Count the number of occurrences for each target value (AFTER RESERVATION)\n",
    "print(\"Total ids with target = 0:\", len(isic_id_target_0))\n",
    "print(\"Total ids with target = 1:\", len(isic_id_target_1))\n",
    "print(\"Total reserved target = 1:\", len(isic_id_target_1_reserved))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 - Split into train-validate & test on both lists (0 and 1). Take the test lists (0 and 1), concatenate and shuffle them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split out the test ids\n",
    "trainval_0, test_0 = train_test_split(isic_id_target_0, test_size = test_frac, random_state=split_seed, shuffle=True)\n",
    "trainval_1, test_1 = train_test_split(isic_id_target_1, test_size = test_frac, random_state=split_seed, shuffle=True)\n",
    "\n",
    "test_ids = test_0 + test_1\n",
    "np.random.shuffle(test_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 - Create augmentation preparation function for target = 1: mod toggle = strictly positive integer\n",
    "(this adds more isic_ids to the list, with mod toggle non zero))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate a list containing augmentation toggles. Apply only to training and validation sets.\n",
    "def augment_prep(tuple_list, nb_of_augments = 30, shuffle_seed=None):\n",
    "    augment_list = []\n",
    "    \n",
    "    #If augmentation is desired, then the mod toggle is a strictly positive integer\n",
    "    augment_list = [(item[0], item[1], i) for item in tuple_list for i in range(1, nb_of_augments + 1)]\n",
    "    \n",
    "    #Shuffle the list\n",
    "    augment_list.extend(tuple_list)\n",
    "    np.random.seed(shuffle_seed)\n",
    "    np.random.shuffle(augment_list)\n",
    "\n",
    "    return augment_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 - Run augmentation preparation function on train-validate and the reserved validation data: mod toggle = strictly positive integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Augment the training and validation list\n",
    "trainval_1 = augment_prep(trainval_1, nb_of_augments=nb_of_augments, shuffle_seed=50)\n",
    "\n",
    "#Duplicate the reserved training data\n",
    "reserved_1 = augment_prep(isic_id_target_1_reserved, nb_of_augments=nb_of_augments_reserved, shuffle_seed=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 - Split train-validate on both lists (0 and 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split training and validation lists\n",
    "train_0, val_0 = train_test_split(trainval_0, test_size = val_frac, random_state=split_seed, shuffle=True)\n",
    "train_1, val_1 = train_test_split(trainval_1, test_size = val_frac, random_state=split_seed, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7 - Reduce the validation data on target = 0 by value specified in reduce_frac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reduce the validation data of type Target = 0\n",
    "nb_samples = int((1 - reduce_frac) * len(val_0))\n",
    "val_0 = random.sample(val_0, nb_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.8 - Concatenate and shuffle the train and validation lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concatenate\n",
    "train_ids=train_0.copy()\n",
    "train_ids.extend(train_1)\n",
    "val_ids=list(itertools.chain(val_0, val_1, reserved_1))\n",
    "\n",
    "#Shuffle\n",
    "np.random.seed(60)\n",
    "np.random.shuffle(train_ids)\n",
    "np.random.shuffle(val_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Validate/Test Counts: 230753 / 30891 / 79053\n",
      "Train/Validate/Test Fractions: 0.68 / 0.09 / 0.23\n",
      "Proportion of Target = 1 in training data: 0.08269448284529345\n",
      "Proportion of Target = 1 in validation data: 0.3250137580525072\n",
      "Proportion of Target = 1 in test data: 0.0008981316332081009\n"
     ]
    }
   ],
   "source": [
    "#Calculate the proportaion of Target=1 in each set (training, validation, test)\n",
    "def calc_frac_target1(ids):\n",
    "    return sum([item[1] for item in ids]) / len(ids)\n",
    "\n",
    "tot_samples = len(train_ids) + len(val_ids) + len(test_ids)\n",
    "\n",
    "print(\"Train/Validate/Test Counts:\", len(train_ids), \"/\", len(val_ids), \"/\", len(test_ids))\n",
    "print(\"Train/Validate/Test Fractions:\", round(len(train_ids)/tot_samples,2), \"/\", round(len(val_ids)/tot_samples,2), \"/\", round(len(test_ids)/tot_samples,2))\n",
    "print(\"Proportion of Target = 1 in training data:\", calc_frac_target1(train_ids))\n",
    "print(\"Proportion of Target = 1 in validation data:\", calc_frac_target1(val_ids))\n",
    "print(\"Proportion of Target = 1 in test data:\", calc_frac_target1(test_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.9 - Limit training and validation data to speed up training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train ids length before: 230753\n",
      "Train ids length after: 6922\n",
      "Validate ids length before: 30891\n",
      "Validate ids length after: 926\n",
      "Test ids length before: 79053\n",
      "Test ids length after: 2371\n"
     ]
    }
   ],
   "source": [
    "#Choose a portion of TRAINING ids to load into memory\n",
    "def take_fewer_samples(ids, frac_to_use, seed):\n",
    "    if frac_to_use < 1 and frac_to_use > 0:\n",
    "        random.seed(seed)\n",
    "        k = int(frac_to_use * len(ids))\n",
    "        ids_short = random.choices(ids, k=k)\n",
    "        return ids_short\n",
    "    return ids\n",
    "\n",
    "print(\"Train ids length before:\", len(train_ids))\n",
    "train_ids = take_fewer_samples(train_ids, train_frac_to_use, seed=12)\n",
    "print(\"Train ids length after:\", len(train_ids))\n",
    "\n",
    "print(\"Validate ids length before:\", len(val_ids))\n",
    "val_ids = take_fewer_samples(val_ids, val_frac_to_use, seed=12)\n",
    "print(\"Validate ids length after:\", len(val_ids))\n",
    "\n",
    "print(\"Test ids length before:\", len(test_ids))\n",
    "test_ids = take_fewer_samples(test_ids, test_frac_to_use, seed=12)\n",
    "print(\"Test ids length after:\", len(test_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Augmentation functions (used during dataset creation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hair_removal(image, crop_pixels=10):\n",
    "    height_pixels = len(image)  # Image rows\n",
    "    width_pixels = len(image[0])  # Image columns\n",
    "\n",
    "    # Image cropping\n",
    "    height = [crop_pixels, height_pixels - crop_pixels]\n",
    "    width = [crop_pixels, width_pixels - crop_pixels]\n",
    "    img = image[height[0]:height[1], width[0]:width[1]]\n",
    "\n",
    "    # Gray scale\n",
    "    grayScale = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    # Black hat filter\n",
    "    kernel = cv2.getStructuringElement(1, (9, 9))\n",
    "    blackhat = cv2.morphologyEx(grayScale, cv2.MORPH_BLACKHAT, kernel)\n",
    "    # Gaussian filter\n",
    "    bhg = cv2.GaussianBlur(blackhat, (3, 3), cv2.BORDER_DEFAULT)\n",
    "    # Binary thresholding (MASK)\n",
    "    ret, mask = cv2.threshold(bhg, 10, 255, cv2.THRESH_BINARY)\n",
    "    # Replace pixels of the mask\n",
    "    dst = cv2.inpaint(img, mask, 6, cv2.INPAINT_TELEA)\n",
    "\n",
    "    return dst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Define the augmentation function\\ndef augment_image(image, is_training=False):\\n    #Apply a series of augmentations to create diverse variations of the input image.\\n    #Includes random flips, rotations, brightness adjustments, and other transformations.\\n    # Apply various augmentations\\n\\n    # parameter\\n    if is_training==True:\\n        height_factor_cut=(0.02, 0.06),\\n        width_factor_cut=(0.02, 0.06),\\n        max_delta_brigth=0.25,\\n        lower_sat=0.7, \\n        upper_sat=1.8,\\n        lower_cont=0.7, \\n        upper_cont=1.8,\\n        minval_rot=0,\\n        maxvalue_rot=4\\n    else:\\n        height_factor_cut=(0, 0),\\n        width_factor_cut=(0, 0),\\n        max_delta_brigth=0.15,\\n        lower_sat=0.8, \\n        upper_sat=1.2,\\n        lower_cont=0.8, \\n        upper_cont=1.2,\\n        minval_rot=0,\\n        maxvalue_rot=0\\n        \\n    # RandomCutout initialization\\n    cutout_layer = keras_cv.layers.RandomCutout(height_factor=height_factor_cut, width_factor=width_factor_cut)\\n    \\n    # List of augmentations\\n    augmentations = [\\n        tf.image.random_flip_left_right,  \\n        tf.image.random_flip_up_down,   \\n        tf.image.random_brightness,      \\n        tf.image.random_contrast,         \\n        tf.image.random_saturation,\\n        lambda img: tf.image.rot90(img, tf.random.uniform(shape=[], minval=minval_rot, maxval=maxvalue_rot, dtype=tf.int32)),\\n        lambda img: cutout_layer(img) \\n    ]\\n\\n    # Shuffle and pick one augmentation\\n    augmentation = augmentations[tf.random.uniform(shape=[], minval=0, maxval=len(augmentations), dtype=tf.int32)]\\n    \\n    # Apply augmentation with 95% probability\\n    #if tf.random.uniform([]) < 0.95:\\n    # Apply augmentation \\n    if augmentation in [tf.image.random_flip_left_right, tf.image.random_flip_up_down]:\\n        image = augmentation(image) \\n    elif augmentation == tf.image.random_brightness:\\n        image = augmentation(image, max_delta=max_delta_brigth)  \\n    elif augmentation == tf.image.random_contrast:\\n        image = augmentation(image, lower_cont, upper_cont)\\n    elif augmentation == tf.image.random_saturation:\\n        image = augmentation(image, lower_sat, upper_sat)  \\n    else:\\n        image = augmentation(image)\\n    \\n    return image\\n'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Define the augmentation function\n",
    "def augment_image(image, is_training=False):\n",
    "    #Apply a series of augmentations to create diverse variations of the input image.\n",
    "    #Includes random flips, rotations, brightness adjustments, and other transformations.\n",
    "    # Apply various augmentations\n",
    "\n",
    "    # parameter\n",
    "    if is_training==True:\n",
    "        height_factor_cut=(0.02, 0.06),\n",
    "        width_factor_cut=(0.02, 0.06),\n",
    "        max_delta_brigth=0.25,\n",
    "        lower_sat=0.7, \n",
    "        upper_sat=1.8,\n",
    "        lower_cont=0.7, \n",
    "        upper_cont=1.8,\n",
    "        minval_rot=0,\n",
    "        maxvalue_rot=4\n",
    "    else:\n",
    "        height_factor_cut=(0, 0),\n",
    "        width_factor_cut=(0, 0),\n",
    "        max_delta_brigth=0.15,\n",
    "        lower_sat=0.8, \n",
    "        upper_sat=1.2,\n",
    "        lower_cont=0.8, \n",
    "        upper_cont=1.2,\n",
    "        minval_rot=0,\n",
    "        maxvalue_rot=0\n",
    "        \n",
    "    # RandomCutout initialization\n",
    "    cutout_layer = keras_cv.layers.RandomCutout(height_factor=height_factor_cut, width_factor=width_factor_cut)\n",
    "    \n",
    "    # List of augmentations\n",
    "    augmentations = [\n",
    "        tf.image.random_flip_left_right,  \n",
    "        tf.image.random_flip_up_down,   \n",
    "        tf.image.random_brightness,      \n",
    "        tf.image.random_contrast,         \n",
    "        tf.image.random_saturation,\n",
    "        lambda img: tf.image.rot90(img, tf.random.uniform(shape=[], minval=minval_rot, maxval=maxvalue_rot, dtype=tf.int32)),\n",
    "        lambda img: cutout_layer(img) \n",
    "    ]\n",
    "\n",
    "    # Shuffle and pick one augmentation\n",
    "    augmentation = augmentations[tf.random.uniform(shape=[], minval=0, maxval=len(augmentations), dtype=tf.int32)]\n",
    "    \n",
    "    # Apply augmentation with 95% probability\n",
    "    #if tf.random.uniform([]) < 0.95:\n",
    "    # Apply augmentation \n",
    "    if augmentation in [tf.image.random_flip_left_right, tf.image.random_flip_up_down]:\n",
    "        image = augmentation(image) \n",
    "    elif augmentation == tf.image.random_brightness:\n",
    "        image = augmentation(image, max_delta=max_delta_brigth)  \n",
    "    elif augmentation == tf.image.random_contrast:\n",
    "        image = augmentation(image, lower_cont, upper_cont)\n",
    "    elif augmentation == tf.image.random_saturation:\n",
    "        image = augmentation(image, lower_sat, upper_sat)  \n",
    "    else:\n",
    "        image = augmentation(image)\n",
    "    \n",
    "    return image\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the augmentation function\n",
    "def augment_image(image, mod_toggle, is_training=False):\n",
    "    \"\"\"\n",
    "    Apply a series of augmentations to create diverse variations of the input image.\n",
    "    Includes random flips, rotations, brightness adjustments, and other transformations.\n",
    "    \"\"\"\n",
    "\n",
    "    # Parameters\n",
    "    if is_training==True:\n",
    "        #Cutout values\n",
    "        height_factor_cut=(0.02, 0.06)\n",
    "        width_factor_cut=(0.02, 0.06)\n",
    "        random_cutout = keras_cv.layers.RandomCutout(height_factor=height_factor_cut, width_factor=width_factor_cut)\n",
    "        #Brightness values\n",
    "        min_delta_bright=0.05\n",
    "        max_delta_bright=0.25\n",
    "        #Saturation values\n",
    "        weak_sat_low=0.7\n",
    "        weak_sat_high=0.95\n",
    "        strong_sat_low=1.05\n",
    "        strong_sat_high=1.8\n",
    "        #Contrast values\n",
    "        weak_cont_low=0.7\n",
    "        weak_cont_high=0.95\n",
    "        strong_cont_low=1.05\n",
    "        strong_cont_high=1.8\n",
    "\n",
    "    else:\n",
    "        #Brightness values\n",
    "        min_delta_bright=0.05\n",
    "        max_delta_bright=0.15\n",
    "        #Saturation values\n",
    "        weak_sat_low=0.8\n",
    "        weak_sat_high=0.95\n",
    "        strong_sat_low=1.05\n",
    "        strong_sat_high=1.2\n",
    "        #Contrast values\n",
    "        weak_cont_low=0.8\n",
    "        weak_cont_high=0.95\n",
    "        strong_cont_low=1.05\n",
    "        strong_cont_high=1.2\n",
    "\n",
    "    #List of base augmentations\n",
    "    base_augments = [\n",
    "        lambda img: tf.image.flip_left_right(img),\n",
    "        lambda img: tf.image.flip_up_down(img),\n",
    "        lambda img: tf.image.rot90(img, k=1),\n",
    "        lambda img: tf.image.rot90(img, k=2),\n",
    "        lambda img: tf.image.rot90(img, k=3),\n",
    "        lambda img: random_cutout(img)\n",
    "    ]\n",
    "\n",
    "    #List of other augmentations that can be performed multiple times\n",
    "    other_augments = [\n",
    "        lambda img: tf.image.adjust_brightness(img, -random.uniform(min_delta_bright, max_delta_bright)),\n",
    "        lambda img: tf.image.adjust_brightness(img, random.uniform(min_delta_bright, max_delta_bright)),\n",
    "        lambda img: tf.image.random_contrast(img, lower=weak_cont_low, upper=weak_cont_high),\n",
    "        lambda img: tf.image.random_contrast(img, lower=strong_cont_low, upper=strong_cont_high),\n",
    "        lambda img: tf.image.random_saturation(img, lower=weak_sat_low, upper=weak_sat_high),\n",
    "        lambda img: tf.image.random_saturation(img, lower=strong_sat_low, upper=strong_sat_high)\n",
    "    ]\n",
    "\n",
    "    #Select augmentations to use based on whether it is training data or not\n",
    "    if is_training:\n",
    "        base_augments = base_augments\n",
    "        other_augments = other_augments \n",
    "    else:\n",
    "        base_selection = [0,1]  #Positions of the augmentations to use in the base_augments list\n",
    "        base_augments = [base_augments[i] for i in base_selection]\n",
    "        other_augments = other_augments\n",
    "\n",
    "    #Engage the augment based on the mod_toggle. The base_augments are always done first.\n",
    "    nb_base_aug = len(base_augments)\n",
    "    \n",
    "    if mod_toggle <= nb_base_aug and mod_toggle > 0:\n",
    "        augmentation = base_augments[mod_toggle - 1] #Mod toggles start at 1\n",
    "        image = augmentation(image)\n",
    "    else:\n",
    "        augmentation = random.choice(other_augments)\n",
    "        image = augmentation(image)\n",
    "        #Apply a second transformation with a certain probability\n",
    "        if tf.random.uniform([]) < 0.85:\n",
    "            augmentation2 = random.choice(base_augments)\n",
    "            image = augmentation2(image)\n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nid_try = \"ISIC_5675460\"\\nwith h5py.File(hdf5_file, \\'r\\') as h5file:\\n    img_try = np.array(Image.open(io.BytesIO(h5file[id_try][()])))\\n\\nfor mod_toggle in range(500):\\n    img = hair_removal(img_try)\\n    img = cv2.resize(img, (100, 100), interpolation= cv2.INTER_AREA)\\n    augment_image(img, mod_toggle, is_training=True)\\n'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TESTING OF AUGMENT_IMAGE FUNCTION\n",
    "\"\"\"\n",
    "id_try = \"ISIC_5675460\"\n",
    "with h5py.File(hdf5_file, 'r') as h5file:\n",
    "    img_try = np.array(Image.open(io.BytesIO(h5file[id_try][()])))\n",
    "\n",
    "for mod_toggle in range(500):\n",
    "    img = hair_removal(img_try)\n",
    "    img = cv2.resize(img, (100, 100), interpolation= cv2.INTER_AREA)\n",
    "    augment_image(img, mod_toggle, is_training=True)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nid_try = \"ISIC_5675460\"\\nwith h5py.File(hdf5_file, \\'r\\') as h5file:\\n    img_try = np.array(Image.open(io.BytesIO(h5file[id_try][()])))\\n\\nheight_factor_cut=(0.02, 0.06)\\nwidth_factor_cut=(0.02, 0.06)\\nrandom_cutout = keras_cv.layers.RandomCutout(height_factor=height_factor_cut, width_factor=width_factor_cut)\\n\\nbase_augments = [\\n    lambda img: img,\\n    lambda img: tf.image.flip_left_right(img),\\n    lambda img: tf.image.flip_up_down(img),\\n    lambda img: tf.image.rot90(img, k=1),\\n    lambda img: tf.image.rot90(img, k=2),\\n    lambda img: tf.image.rot90(img, k=3),\\n    lambda img: random_cutout(img).numpy().astype(int),\\n]\\n\\nimages = []\\nfor augmentation in base_augments:\\n    image = augmentation(img_try)\\n    images.append(image)\\n\\n#Show the images\\nfor image in images:\\n    plt.imshow(image, interpolation=None)\\n    plt.grid(None)\\n    plt.show()\\n'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TESTING OF BASE AUGMENTS\n",
    "\"\"\"\n",
    "id_try = \"ISIC_5675460\"\n",
    "with h5py.File(hdf5_file, 'r') as h5file:\n",
    "    img_try = np.array(Image.open(io.BytesIO(h5file[id_try][()])))\n",
    "\n",
    "height_factor_cut=(0.02, 0.06)\n",
    "width_factor_cut=(0.02, 0.06)\n",
    "random_cutout = keras_cv.layers.RandomCutout(height_factor=height_factor_cut, width_factor=width_factor_cut)\n",
    "\n",
    "base_augments = [\n",
    "    lambda img: img,\n",
    "    lambda img: tf.image.flip_left_right(img),\n",
    "    lambda img: tf.image.flip_up_down(img),\n",
    "    lambda img: tf.image.rot90(img, k=1),\n",
    "    lambda img: tf.image.rot90(img, k=2),\n",
    "    lambda img: tf.image.rot90(img, k=3),\n",
    "    lambda img: random_cutout(img).numpy().astype(int),\n",
    "]\n",
    "\n",
    "images = []\n",
    "for augmentation in base_augments:\n",
    "    image = augmentation(img_try)\n",
    "    images.append(image)\n",
    "\n",
    "#Show the images\n",
    "for image in images:\n",
    "    plt.imshow(image, interpolation=None)\n",
    "    plt.grid(None)\n",
    "    plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nid_try = \"ISIC_5675460\"\\nwith h5py.File(hdf5_file, \\'r\\') as h5file:\\n    img_try = np.array(Image.open(io.BytesIO(h5file[id_try][()])))\\n\\n#Brightness values\\nmin_delta_bright=0.05\\nmax_delta_bright=0.25\\n#Saturation values\\nweak_sat_low=0.7\\nweak_sat_high=0.95\\nstrong_sat_low=1.05\\nstrong_sat_high=1.8\\n#Contrast values\\nweak_cont_low=0.7\\nweak_cont_high=0.95\\nstrong_cont_low=1.05\\nstrong_cont_high=1.8\\n\\n#List of other augmentations that can be performed multiple times\\nother_augments = [\\n    lambda img: tf.image.adjust_brightness(img, -random.uniform(min_delta_bright, max_delta_bright)),\\n    lambda img: tf.image.adjust_brightness(img, random.uniform(min_delta_bright, max_delta_bright)),\\n    lambda img: tf.image.random_contrast(img, lower=weak_cont_low, upper=weak_cont_high),\\n    lambda img: tf.image.random_contrast(img, lower=strong_cont_low, upper=strong_cont_high),\\n    lambda img: tf.image.random_saturation(img, lower=weak_sat_low, upper=weak_sat_high),\\n    lambda img: tf.image.random_saturation(img, lower=strong_sat_low, upper=strong_sat_high)\\n]\\n\\nimages = []\\nfor augmentation in other_augments:\\n    image = augmentation(img_try)\\n    if tf.random.uniform([]) < 1:\\n        augmentation2 = random.choice(base_augments)\\n        image = augmentation2(image)\\n    images.append(image)\\n\\n#Show the images\\nfor image in images:\\n    plt.imshow(image, interpolation=None)\\n    plt.grid(None)\\n    plt.show()\\n'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TESTING OF OTHER AUGMENTS\n",
    "\"\"\"\n",
    "id_try = \"ISIC_5675460\"\n",
    "with h5py.File(hdf5_file, 'r') as h5file:\n",
    "    img_try = np.array(Image.open(io.BytesIO(h5file[id_try][()])))\n",
    "\n",
    "#Brightness values\n",
    "min_delta_bright=0.05\n",
    "max_delta_bright=0.25\n",
    "#Saturation values\n",
    "weak_sat_low=0.7\n",
    "weak_sat_high=0.95\n",
    "strong_sat_low=1.05\n",
    "strong_sat_high=1.8\n",
    "#Contrast values\n",
    "weak_cont_low=0.7\n",
    "weak_cont_high=0.95\n",
    "strong_cont_low=1.05\n",
    "strong_cont_high=1.8\n",
    "\n",
    "#List of other augmentations that can be performed multiple times\n",
    "other_augments = [\n",
    "    lambda img: tf.image.adjust_brightness(img, -random.uniform(min_delta_bright, max_delta_bright)),\n",
    "    lambda img: tf.image.adjust_brightness(img, random.uniform(min_delta_bright, max_delta_bright)),\n",
    "    lambda img: tf.image.random_contrast(img, lower=weak_cont_low, upper=weak_cont_high),\n",
    "    lambda img: tf.image.random_contrast(img, lower=strong_cont_low, upper=strong_cont_high),\n",
    "    lambda img: tf.image.random_saturation(img, lower=weak_sat_low, upper=weak_sat_high),\n",
    "    lambda img: tf.image.random_saturation(img, lower=strong_sat_low, upper=strong_sat_high)\n",
    "]\n",
    "\n",
    "images = []\n",
    "for augmentation in other_augments:\n",
    "    image = augmentation(img_try)\n",
    "    if tf.random.uniform([]) < 1:\n",
    "        augmentation2 = random.choice(base_augments)\n",
    "        image = augmentation2(image)\n",
    "    images.append(image)\n",
    "\n",
    "#Show the images\n",
    "for image in images:\n",
    "    plt.imshow(image, interpolation=None)\n",
    "    plt.grid(None)\n",
    "    plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Metadata Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a metadata dictionary for efficient lookup\n",
    "#Structure: {index: value} where value is (mod_toggle, metadata array)\n",
    "\n",
    "#Objective: We need \"train_ids\" to efficiently retrieve and augment images from the HDF5 file. This already exists.\n",
    "#           We need to accesss metadata through a dictionary to improve speed. A common reference is needed for both.\n",
    "\n",
    "#Idea:      Since train_ids is a LIST of tuples (isic_id, target, mod toggle), it is accessed via indices (ex. train_ids[10]).\n",
    "#           We need to make a dictionary that, for each train_ids index, lists all the metadata associated to the isic_id.\n",
    "#           Thus, when train_ids[10] is called, we call the dictionary and request key=10 to get the metadata.\n",
    "#           Result: very fast data retrieval\n",
    "\n",
    "def make_meta_dict(metadata, isic_ids_tuple):\n",
    "    #Reindex. The metadata must be contiguously indexed. Holes in index numbering will not work.\n",
    "    metadata = metadata.reset_index(drop=True)\n",
    "\n",
    "    #Get the column number for \"isic_id\" and \"target\" in the metadata dataframe.\n",
    "    #This allows us to know where these items are located in the metadata retrieved for each item.\n",
    "    #This is used later to filter out these items from the metadata.\n",
    "    col_num_id = metadata.columns.get_loc(\"isic_id\")\n",
    "    col_num_target = metadata.columns.get_loc(\"target\")\n",
    "\n",
    "    #The metadata contains all unique isic_ids. Since the dataframe is reindexed, it is possible to make\n",
    "    #a dictionary of (isic_id: row number) for fast retrieval of all metadata associated to an isic_id.\n",
    "\n",
    "    #Take the metadata dataframe and create a dictionary that stores (isic_id, row number).\n",
    "    metadata_index_dict = metadata[\"isic_id\"].to_dict()\n",
    "    metadata_index_dict = dict((v, k) for k, v in metadata_index_dict.items())\n",
    "\n",
    "    #Make a dictionary of (index: (mod toggle, metadata)), where index is the position of a sample in train_ids and metadata is the metadata\n",
    "    #associated to the sample's isic_id. We thus create a dict_of_meta that is a mirror image of the \"isic_ids_tuple\" list.\n",
    "    #We must be careful to not shuffle \"isic_ids_tuple\".\n",
    "    dict_of_meta = {}\n",
    "    for pos, tup in enumerate(isic_ids_tuple):\n",
    "        # Use the lookup table to directly find the index\n",
    "        index = metadata_index_dict.get(tup[0], -1)  # -1 if not found\n",
    "\n",
    "        if index != -1:\n",
    "            # Access the row directly without masking\n",
    "            dict_of_meta.update({pos: (tup[2], np.array(metadata.iloc[index].values))})\n",
    "            # Process the row as needed\n",
    "        else:\n",
    "            raise Exception(\"isic_id values are not all unique\")\n",
    "    return dict_of_meta, col_num_id, col_num_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the metadata dictionaries for train-validate-test\n",
    "train_meta_dict, train_pos_isic_id, train_pos_target = make_meta_dict(metadata, train_ids)\n",
    "val_meta_dict, val_pos_isic_id, val_pos_target = make_meta_dict(metadata, val_ids)\n",
    "test_meta_dict, test_pos_isic_id, test_pos_target = make_meta_dict(metadata, test_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAMPLE OF ITEMS FROM VALIDATION META DICTIONARY\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: (-1,\n",
       "  array(['ISIC_0851677', 45.0, 0, 3.0, 3.92193657346594, 16.7191919083428,\n",
       "         0.821134678634129, 1.70969231487954, 0.626906759659996,\n",
       "         7.25332777230176, 0.936470909657748, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=object)),\n",
       " 1: (-1,\n",
       "  array(['ISIC_0183824', 50.0, 0, 2.51, 5.1792081065866, 13.9462460963262,\n",
       "         0.420267058181425, 2.38253763277237, 1.16035751908145,\n",
       "         11.4659236433193, 0.929215060837378, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=object)),\n",
       " 2: (-1,\n",
       "  array(['ISIC_8757697', 65.0, 0, 2.71, 5.08538187277163, 14.9251034237249,\n",
       "         0.623615671101967, 2.22787047085697, 1.56762346408759,\n",
       "         10.4929069616258, 1.42721249668427, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 1, 0, 0, 0], dtype=object))}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Look some dictionary elements to understand the structure {index, (mod_toggle, metadata)}\n",
    "print(\"SAMPLE OF ITEMS FROM VALIDATION META DICTIONARY\")\n",
    "dict(filter(lambda item: item[0] in {0, 1, 2}, val_meta_dict.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Dataset generation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMAGE generator in a class\n",
    "class hdf5_generator_all_included(Generator):\n",
    "    def __init__(self, file, meta_dict, dict_pos_isic_id, dict_pos_target, num_features, imgSize, is_training=False, shuffle_seed=None, apply_hair_removal=False):\n",
    "        self.file = file\n",
    "        self.meta_dict = meta_dict\n",
    "        self.pos_mod_toggle = 0                     #Position of 'mod_toggle' within dictionary: structure {key: value} where value is (mod_toggle, metadata array)\n",
    "        self.pos_metadata_array = 1                 #Position of 'metadata array' within dictionary: structure {key: value} where value is (mod_toggle, metadata array)\n",
    "        self.dict_pos_isic_id = dict_pos_isic_id    #Set position of 'isic_id\" within the metadata array: structure [var0, var1, var2, var3, ...]\n",
    "        self.dict_pos_target = dict_pos_target      #Set position of 'target\" within the metadata array: structure [var0, var1, var2, var3, ...]\n",
    "        self.num_features = num_features\n",
    "        self.imgSize = imgSize\n",
    "        self.is_training = is_training\n",
    "        self.shuffle_seed = shuffle_seed\n",
    "        self.apply_hair_removal = apply_hair_removal\n",
    "        self.len = len(meta_dict)\n",
    "        self.start = 0\n",
    "        self.stop = self.len\n",
    "        self.i = self.start\n",
    "        self.error_check()\n",
    "        self.open_hdf5()\n",
    "        self.order_and_shuffle()\n",
    "        \n",
    "    def send(self, value):\n",
    "        if self.i < self.stop:\n",
    "            if self.i == self.start:\n",
    "                self.open_hdf5()\n",
    "\n",
    "            #Retrieve index of isic_id according to the shuffled order\n",
    "            index = self.order[self.i]\n",
    "\n",
    "            #Retrieve target... remember that each item of the the meta_dict is a tuple of (mod toggle, metadata)\n",
    "            target = self.meta_dict[index][self.pos_metadata_array][self.dict_pos_target]\n",
    "            target = np.reshape(target, (1,1))\n",
    "            target = tf.cast(target, dtype=tf.int32)\n",
    "\n",
    "            #Retrieve metadata... remember that each item of the the meta_dict is a tuple of (mod toggle, metadata)\n",
    "            meta = np.delete(self.meta_dict[index][self.pos_metadata_array], [self.dict_pos_isic_id, self.dict_pos_target], 0)\n",
    "            meta = meta.astype(dtype=float)\n",
    "            meta = tf.cast(meta, dtype=tf.float32)\n",
    "            meta = tf.reshape(meta, shape=(1, self.num_features))\n",
    "\n",
    "            try:\n",
    "                #Retrieve isic_id\n",
    "                img_name = self.meta_dict[index][self.pos_metadata_array][self.dict_pos_isic_id]\n",
    "                \n",
    "                # Load image data from HDF5\n",
    "                img = np.array(Image.open(io.BytesIO(self.h5file[img_name][()])))\n",
    "\n",
    "                # Clean image\n",
    "                if self.apply_hair_removal:\n",
    "                    img = hair_removal(img)\n",
    "\n",
    "                # Resize the image\n",
    "                img = cv2.resize(img, (self.imgSize, self.imgSize), interpolation= cv2.INTER_AREA)\n",
    "\n",
    "                # Apply augmentations if needed\n",
    "                mod_toggle = self.meta_dict[index][self.pos_mod_toggle]\n",
    "                if mod_toggle > 0:\n",
    "                    img=augment_image(img, mod_toggle, self.is_training)\n",
    "                    \n",
    "                # Standardize and return as TensorFlow constant\n",
    "                img = tf.constant(img / 255, dtype=tf.float32)\n",
    "                \n",
    "                #Augment counter\n",
    "                self.i = self.i + 1\n",
    "                return (img, meta), target\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error loading image {img_name}: {e}\")\n",
    "                # log the error to a file for later analysis\n",
    "                with open('image_errors.log', 'a') as f:\n",
    "                    f.write(f\"Error loading image {img_name}: {e}\\n\")\n",
    "\n",
    "            if self.i == self.stop:\n",
    "                self.h5file.close()\n",
    "        raise StopIteration\n",
    "\n",
    "    def throw(self, typ, val=None, tb=None):\n",
    "        #Close HDF5 file and terminate generator\n",
    "        try:\n",
    "            self.h5file.close()\n",
    "            super().throw(typ, val, tb)\n",
    "        except:\n",
    "            super().throw(typ, val, tb)\n",
    "\n",
    "    def error_check(self):\n",
    "        #Seed type check\n",
    "        try:\n",
    "            int(self.shuffle_seed) == self.shuffle_seed\n",
    "        except:\n",
    "            if self.shuffle_seed != None:\n",
    "                raise Exception(\"Seed must either be an integer or None\")\n",
    "\n",
    "    def order_and_shuffle(self):\n",
    "        np.random.seed(self.shuffle_seed)\n",
    "        self.order = np.array(list(self.meta_dict.keys()), dtype=int)\n",
    "        if self.shuffle_seed != None:\n",
    "            np.random.shuffle(self.order)\n",
    "\n",
    "    def open_hdf5(self):\n",
    "        self.h5file = h5py.File(self.file, 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(hdf5_file, meta_dict, dict_pos_isic_id, dict_pos_target, imgSize=100, batch_size=32, is_training=False, shuffle_seed = None, apply_hair_removal=False):\n",
    "    num_features = len(val_meta_dict[0][1]) - 2 #Subtract isic_id and target\n",
    "\n",
    "    combined_generator = hdf5_generator_all_included(hdf5_file, meta_dict, dict_pos_isic_id, dict_pos_target, num_features, imgSize, is_training, shuffle_seed, apply_hair_removal)\n",
    "\n",
    "    # Generate image dataset\n",
    "    element_spec = ((tf.TensorSpec(shape=(imgSize, imgSize, 3), dtype=tf.float32),\n",
    "                    tf.TensorSpec(shape=(1, num_features), dtype=tf.float32)),\n",
    "                    tf.TensorSpec(shape=(1, 1), dtype=tf.int32))\n",
    "\n",
    "    img_dataset = tf.data.Dataset.from_generator(\n",
    "        lambda: combined_generator,\n",
    "        output_signature=element_spec\n",
    "    )\n",
    "\n",
    "    return img_dataset.batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) Alternative dataset function to load all in memory (not a generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_in_memory_dataset(hdf5_file, meta_dict, dict_pos_isic_id, dict_pos_target, imgSize=100, batch_size=32, is_training=False, shuffle_seed=None, apply_hair_removal=False):\n",
    "    #Position of elements in the value part of the dictionary (it is a tuple with multiple elements)\n",
    "    pos_mod_toggle = 0\n",
    "    pos_metadata_array = 1\n",
    "\n",
    "    num_features = len(meta_dict[0][pos_metadata_array]) - 2  # Subtract isic_id and target columns\n",
    "\n",
    "    # Initialize lists to hold images, metadata, and targets\n",
    "    images = []\n",
    "    metas = []\n",
    "    targets = []\n",
    "\n",
    "    np.random.seed(shuffle_seed)\n",
    "    order = np.array(list(meta_dict.keys()), dtype=int)\n",
    "    if shuffle_seed is not None:\n",
    "        np.random.shuffle(order)\n",
    "\n",
    "    with h5py.File(hdf5_file, 'r') as h5file:\n",
    "        for i in range(len(meta_dict)):\n",
    "            index = order[i]\n",
    "            \n",
    "            # Retrieve target\n",
    "            target = meta_dict[index][pos_metadata_array][dict_pos_target]\n",
    "            target = np.reshape(target, (1, 1))\n",
    "            target = tf.cast(target, dtype=tf.int32)\n",
    "\n",
    "            # Retrieve metadata\n",
    "            meta = np.delete(meta_dict[index][pos_metadata_array], [dict_pos_isic_id, dict_pos_target], 0)\n",
    "            meta = meta.astype(dtype=float)\n",
    "            meta = tf.cast(meta, dtype=tf.float32)\n",
    "            meta = tf.reshape(meta, shape=(1, num_features))\n",
    "\n",
    "            try:\n",
    "                # Retrieve isic_id and load image\n",
    "                img_name = meta_dict[index][pos_metadata_array][dict_pos_isic_id]\n",
    "                \n",
    "                # Load image data from HDF5\n",
    "                img = np.array(Image.open(io.BytesIO(h5file[img_name][()])))\n",
    "\n",
    "                # Clean image\n",
    "                if apply_hair_removal:\n",
    "                    img = hair_removal(img)\n",
    "\n",
    "                # Resize the image\n",
    "                img = cv2.resize(img, (imgSize, imgSize), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "                # Apply augmentations if needed\n",
    "                mod_toggle = meta_dict[index][pos_mod_toggle]\n",
    "                if mod_toggle > 0:\n",
    "                    img=augment_image(img, mod_toggle, is_training)\n",
    "\n",
    "                # Normalize the image\n",
    "                img = tf.constant(img / 255, dtype=tf.float32)\n",
    "\n",
    "                # Add processed data to lists\n",
    "                images.append(img)\n",
    "                metas.append(meta)\n",
    "                targets.append(target)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading image {img_name}: {e}\")\n",
    "                with open('image_errors.log', 'a') as f:\n",
    "                    f.write(f\"Error loading image {img_name}: {e}\\n\")\n",
    "                continue\n",
    "\n",
    "    # Convert lists to tensors\n",
    "    images_tensor = tf.stack(images, axis=0)\n",
    "    metas_tensor = tf.stack(metas, axis=0)\n",
    "    targets_tensor = tf.stack(targets, axis=0)\n",
    "\n",
    "    # Combine the images, metadata, and targets into a tf.data.Dataset\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(((images_tensor, metas_tensor), targets_tensor))\n",
    "\n",
    "    # Batch and prefetch the dataset\n",
    "    return dataset.batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8) CNN MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 - Model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simple CNN model using only images and target (FOR TESTING)\n",
    "class CNN_model(tf.keras.Model):\n",
    "    def __init__(self, neurons = 8, activ = 'leaky_relu', img_size = 100, img_channels=3):\n",
    "        #Run the constructor of the parent class\n",
    "        super().__init__()\n",
    "\n",
    "        #Weight and bias initializers\n",
    "        kernel_initializer = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.05, seed=None)\n",
    "        bias_initializer = tf.keras.initializers.RandomUniform(minval=-0.05, maxval=0.05, seed=None)\n",
    "        \n",
    "        #Image size declaration\n",
    "        self.img_size = img_size\n",
    "        self.img_channels = img_channels\n",
    "\n",
    "        #Layers\n",
    "        self.conv1 = tf.keras.layers.Conv2D(filters=16, kernel_size=5, strides=(1, 1), activation='relu', padding='same', input_shape=(img_size, img_size, img_channels),\n",
    "                                            kernel_initializer=kernel_initializer, bias_initializer=bias_initializer)\n",
    "        self.pool1 = tf.keras.layers.MaxPool2D(pool_size=(2,2))\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.dense1 = tf.keras.layers.Dense(neurons, activation = activ, kernel_initializer=kernel_initializer, bias_initializer=bias_initializer)\n",
    "        self.dense2 = tf.keras.layers.Dense(1, activation='sigmoid', kernel_initializer=kernel_initializer, bias_initializer=bias_initializer)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x_image, x_meta = inputs\n",
    "\n",
    "        # Convolutions\n",
    "        x1 = self.conv1(x_image)\n",
    "        x1 = self.pool1(x1)\n",
    "\n",
    "        # Flattening of images for input layer\n",
    "        x1 = self.flatten(x1)\n",
    "\n",
    "        # Hidden layers of neural network\n",
    "        x1 = self.dense1(x1)\n",
    "\n",
    "        # Output layer of neural network\n",
    "        output = self.dense2(x1)\n",
    "\n",
    "        return output \n",
    "\n",
    "#Metadata Neural Network (FOR TESTING)\n",
    "class Meta_model(tf.keras.Model):\n",
    "    def __init__(self, neurons = 8, activ = 'tanh',**kwargs):\n",
    "        if kwargs:  \n",
    "            self.name=kwargs['name'] \n",
    "            \n",
    "        #Run the constructor of the parent class\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        #Weight and bias initializers\n",
    "        kernel_initializer = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.05, seed=None)\n",
    "        bias_initializer = tf.keras.initializers.RandomUniform(minval=-0.05, maxval=0.05, seed=None)\n",
    "\n",
    "        #Layers\n",
    "        self.dense1 = tf.keras.layers.Dense(neurons, activation = activ, kernel_initializer=kernel_initializer, bias_initializer=bias_initializer)\n",
    "        self.dense2 = tf.keras.layers.Dense(neurons, activation = activ, kernel_initializer=kernel_initializer, bias_initializer=bias_initializer)\n",
    "        self.dense3 = tf.keras.layers.Dense(1, activation='sigmoid', kernel_initializer=kernel_initializer, bias_initializer=bias_initializer)\n",
    "        self.dropout = tf.keras.layers.Dropout(0.25)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        x_image, x_meta = inputs\n",
    "        x_all = tf.reshape(x_meta, (tf.shape(x_meta)[0], x_meta.shape[-1]))\n",
    "        # Neural Network\n",
    "        x_all = self.dense1(x_all)\n",
    "        x_all = self.dense2(x_all)\n",
    "        if training:\n",
    "            x_all = self.dropout(x_all, training=training)\n",
    "        output = self.dense3(x_all)\n",
    "        return output\n",
    "\n",
    "#Hybrid CNN model taking metadata (FULL MODEL)\n",
    "@tf.keras.utils.register_keras_serializable(package=\"MyLayers\", name=\"KernelMult\")\n",
    "class Hybrid_model(tf.keras.Model):\n",
    "    def __init__(self, neurons = 8, dropout = 0, activ = 'leaky_relu', img_size = 100, img_channels = 3, **kwargs):\n",
    "        #Run the constructor of the parent class\n",
    "        super(). __init__(**kwargs)\n",
    "        \n",
    "        #Save inputs\n",
    "        self.neurons = neurons\n",
    "        self.dropout = dropout\n",
    "        self.activ = activ\n",
    "        self.img_size = img_size\n",
    "        self.img_channels = img_channels\n",
    "\n",
    "        #Weight and bias initializers\n",
    "        self.kernel_initializer = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.05, seed=None)\n",
    "        self.bias_initializer = tf.keras.initializers.RandomUniform(minval=-0.05, maxval=0.05, seed=None)\n",
    "\n",
    "        #Layers\n",
    "        self.conv1 = tf.keras.layers.Conv2D(filters=32, kernel_size=5, strides=(1, 1), activation='relu', padding='same', input_shape=(self.img_size, self.img_size, self.img_channels),\n",
    "                                            kernel_initializer=self.kernel_initializer, bias_initializer=self.bias_initializer)\n",
    "        self.conv2 = tf.keras.layers.Conv2D(64, 5, activation='relu', kernel_initializer=self.kernel_initializer, bias_initializer=self.bias_initializer)\n",
    "        self.pool = tf.keras.layers.MaxPool2D(pool_size=(2,2))\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.dense1 = tf.keras.layers.Dense(self.neurons, activation = self.activ, kernel_initializer=self.kernel_initializer, bias_initializer=self.bias_initializer)\n",
    "        self.dropout1 = tf.keras.layers.Dropout(self.dropout)\n",
    "        self.dense2 = tf.keras.layers.Dense(self.neurons, activation = self.activ, kernel_initializer=self.kernel_initializer, bias_initializer=self.bias_initializer)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(self.dropout)\n",
    "        self.dense3 = tf.keras.layers.Dense(1, activation='sigmoid', kernel_initializer=self.kernel_initializer, bias_initializer=self.bias_initializer)\n",
    "        self.concatenate = keras.layers.Concatenate(axis=1)\n",
    "        \n",
    "    def call(self, inputs, training=False):\n",
    "        flattened_inputs = tf.nest.flatten(inputs)\n",
    "        x_image, x_meta = flattened_inputs\n",
    "        # Convolutions\n",
    "        x = self.conv1(x_image)\n",
    "        x = self.pool(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.pool(x)\n",
    "        # Flattening of images and concatenation with other data\n",
    "        x = self.flatten(x)\n",
    "        # Reshape metadata to match dimensions\n",
    "        x_meta = tf.reshape(x_meta, (tf.shape(x_meta)[0], x_meta.shape[-1]))\n",
    "        x_all = self.concatenate([x, x_meta])\n",
    "        # Neural Network\n",
    "        x_all = self.dense1(x_all)\n",
    "        if training:\n",
    "            x_all = self.dropout1(x_all, training=training)\n",
    "        x_all = self.dense2(x_all)\n",
    "        if training:\n",
    "            x_all = self.dropout2(x_all, training=training)\n",
    "        output = self.dense3(x_all)\n",
    "        return output\n",
    "           \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            'conv1' : self.conv1,\n",
    "            'conv2' : self.conv2,\n",
    "            'pool' : self.pool,\n",
    "            'flatten' : self.flatten,\n",
    "            'dense1' : self.dense1,\n",
    "            'dropout1' : self.dropout1,\n",
    "            'dense2' : self.dense2,\n",
    "            'dropout2' : self.dropout2,\n",
    "            'dense3' : self.dense3,\n",
    "            'concatenate' : self.concatenate,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        config[\"conv1\"] = keras.layers.deserialize(config[\"conv1\"])\n",
    "        config[\"conv2\"] = keras.layers.deserialize(config[\"conv2\"])\n",
    "        config[\"pool\"] = keras.layers.deserialize(config[\"pool\"])\n",
    "        config[\"flatten\"] = keras.layers.deserialize(config[\"flatten\"])\n",
    "        config[\"dense1\"] = keras.layers.deserialize(config[\"dense1\"])\n",
    "        config[\"dropout1\"] = keras.layers.deserialize(config[\"dropout1\"])\n",
    "        config[\"dense2\"] = keras.layers.deserialize(config[\"dense2\"])\n",
    "        config[\"dropout2\"] = keras.layers.deserialize(config[\"dropout2\"])\n",
    "        config[\"dense3\"] = keras.layers.deserialize(config[\"dense3\"])\n",
    "        config[\"concatenate\"] = keras.layers.deserialize(config[\"concatenate\"])\n",
    "        return cls(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n#MANUAL DEFINITION OF LAYERS - ATTEMPT TO BE ABLE TO EXPORT MODEL - SLOWER!!!\\nfrom keras import Model\\nkernel_initializer = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.05, seed=None)\\nbias_initializer = tf.keras.initializers.RandomUniform(minval=-0.05, maxval=0.05, seed=None)\\n\\ninput_img = keras.layers.Input(shape=(100,100,3))\\nx = keras.layers.Conv2D(filters=32, kernel_size=5, strides=(1, 1), activation='relu', padding='same', input_shape=(100, 100, 3), kernel_initializer=kernel_initializer, bias_initializer=bias_initializer)(input_img)\\nx = keras.layers.Conv2D(64, 5, activation='relu', kernel_initializer=kernel_initializer, bias_initializer=bias_initializer)(x)\\nx = keras.layers.MaxPool2D(pool_size=(2,2))(x)\\nx = keras.layers.Flatten()(x)\\nx = Model(inputs=input_img, outputs=x)\\n\\ninput_meta = keras.layers.Input(shape=(1,29))\\nx_meta = keras.layers.Reshape(target_shape=([29]))(input_meta)\\n#x_meta = tf.reshape(input_meta, (tf.shape(input_meta)[0], input_meta.shape[-1]))\\nx_meta = Model(inputs=input_meta, outputs=x_meta)\\n\\ncombined = keras.layers.Concatenate(axis=1)([x.output, x_meta.output])\\n\\nx_all = keras.layers.Dense(8, activation = 'leaky_relu', kernel_initializer=kernel_initializer, bias_initializer=bias_initializer)(combined)\\nx_all = tf.keras.layers.Dropout(0.1)(x_all)\\nx_all = tf.keras.layers.Dense(8, activation = 'leaky_relu', kernel_initializer=kernel_initializer, bias_initializer=bias_initializer)(x_all)\\nx_all = tf.keras.layers.Dropout(0.1)(x_all)\\nx_all = tf.keras.layers.Dense(1, activation='sigmoid', kernel_initializer=kernel_initializer, bias_initializer=bias_initializer)(x_all)\\nmodel = Model(inputs=[input_img, input_meta], outputs=x_all)\\n\""
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "#MANUAL DEFINITION OF LAYERS - ATTEMPT TO BE ABLE TO EXPORT MODEL - SLOWER!!!\n",
    "from keras import Model\n",
    "kernel_initializer = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.05, seed=None)\n",
    "bias_initializer = tf.keras.initializers.RandomUniform(minval=-0.05, maxval=0.05, seed=None)\n",
    "\n",
    "input_img = keras.layers.Input(shape=(100,100,3))\n",
    "x = keras.layers.Conv2D(filters=32, kernel_size=5, strides=(1, 1), activation='relu', padding='same', input_shape=(100, 100, 3), kernel_initializer=kernel_initializer, bias_initializer=bias_initializer)(input_img)\n",
    "x = keras.layers.Conv2D(64, 5, activation='relu', kernel_initializer=kernel_initializer, bias_initializer=bias_initializer)(x)\n",
    "x = keras.layers.MaxPool2D(pool_size=(2,2))(x)\n",
    "x = keras.layers.Flatten()(x)\n",
    "x = Model(inputs=input_img, outputs=x)\n",
    "\n",
    "input_meta = keras.layers.Input(shape=(1,29))\n",
    "x_meta = keras.layers.Reshape(target_shape=([29]))(input_meta)\n",
    "#x_meta = tf.reshape(input_meta, (tf.shape(input_meta)[0], input_meta.shape[-1]))\n",
    "x_meta = Model(inputs=input_meta, outputs=x_meta)\n",
    "\n",
    "combined = keras.layers.Concatenate(axis=1)([x.output, x_meta.output])\n",
    "\n",
    "x_all = keras.layers.Dense(8, activation = 'leaky_relu', kernel_initializer=kernel_initializer, bias_initializer=bias_initializer)(combined)\n",
    "x_all = tf.keras.layers.Dropout(0.1)(x_all)\n",
    "x_all = tf.keras.layers.Dense(8, activation = 'leaky_relu', kernel_initializer=kernel_initializer, bias_initializer=bias_initializer)(x_all)\n",
    "x_all = tf.keras.layers.Dropout(0.1)(x_all)\n",
    "x_all = tf.keras.layers.Dense(1, activation='sigmoid', kernel_initializer=kernel_initializer, bias_initializer=bias_initializer)(x_all)\n",
    "model = Model(inputs=[input_img, input_meta], outputs=x_all)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 - Model compiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-25 11:45:53.490005: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:282] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
     ]
    }
   ],
   "source": [
    "#Set seed\n",
    "tf.random.set_seed(71)\n",
    "\n",
    "#Initialize model - the full model is Hybrid_model. The others are only for testing\n",
    "#model = CNN_model(neurons=8, activ='tanh')\n",
    "model = Hybrid_model(neurons=nb_neurons_hidden_layers, dropout=dropout, activ='leaky_relu')\n",
    "#model = Meta_model(neurons=18, activ='tanh')\n",
    "\n",
    "#Define optimizer and loss function\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "loss = tf.keras.losses.BinaryCrossentropy(from_logits=False,\n",
    "                                          label_smoothing=0.0,\n",
    "                                          axis=-1,\n",
    "                                          reduction='sum_over_batch_size',\n",
    "                                          name='binary_crossentropy')\n",
    "\n",
    "#Compile the model with loss, optimizer, and metrics\n",
    "model.compile(loss = loss,\n",
    "              optimizer = optimizer,\n",
    "              metrics = [\n",
    "                  tf.keras.metrics.BinaryAccuracy(),\n",
    "                  tf.keras.metrics.FalseNegatives(),\n",
    "                  tf.keras.metrics.FalsePositives(),\n",
    "                  tf.keras.metrics.TrueNegatives(),\n",
    "                  tf.keras.metrics.TruePositives()\n",
    "                  ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3 - Model loss function weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define function to calculate weights to use in loss function\n",
    "def compute_class_weights(meta_dict, pos_target):\n",
    "    # Initialize counters for target=0 and target=1\n",
    "    target_0_count = 0\n",
    "    target_1_count = 0\n",
    "\n",
    "    # Calculate total number of images\n",
    "    total = len(meta_dict)\n",
    "    # Calculate number of target = 1 by summing the target value for each dict item\n",
    "    target_1_count = sum([meta_dict[key][1][pos_target] for key in range(total)])\n",
    "    # Calculate number of target = 1\n",
    "    target_0_count = total - target_1_count\n",
    "\n",
    "    # Calculate class weights based on the counts, avoid division by zero\n",
    "    if target_1_count > 0 :\n",
    "        if target_1_count < target_0_count:\n",
    "            weight_for_0 = 1\n",
    "            weight_for_1 = target_0_count/target_1_count\n",
    "        elif target_0_count > 0:\n",
    "            weight_for_0 = target_1_count/target_0_count\n",
    "            weight_for_1 = 1\n",
    "        else:\n",
    "            weight_for_0 = 0\n",
    "            weight_for_1=target_1_count\n",
    "    else:\n",
    "        weight_for_0 = target_0_count\n",
    "        weight_for_1 = 0\n",
    "        \n",
    "\n",
    "    return weight_for_0, weight_for_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get weights for training\n",
    "weight_for_0, weight_for_1 = compute_class_weights(train_meta_dict, train_pos_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.4 - Model Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training batches in dataset: 217\n",
      "Total validate batches in dataset: 29\n"
     ]
    }
   ],
   "source": [
    "#Determine the number of batches (includes last incomplete batch)\n",
    "nb_training_batches = int(np.ceil(len(train_meta_dict)/train_batch_size))\n",
    "nb_validate_batches = int(np.ceil(len(val_meta_dict)/val_batch_size))\n",
    "\n",
    "#Print results\n",
    "print(\"Total training batches in dataset:\", nb_training_batches)\n",
    "print(\"Total validate batches in dataset:\", nb_validate_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nimport boto3\\nimport h5py\\nimport io\\n\\n# Fonction pour charger le fichier HDF5 directement depuis S3 sans le télécharger localement\\ndef load_hdf5_from_s3(bucket_name, s3_key):\\n    s3 = boto3.client('s3')\\n    # Télécharger le fichier en mémoire\\n    obj = s3.get_object(Bucket=bucket_name, Key=s3_key)\\n    bytestream = io.BytesIO(obj['Body'].read())  # Lire le fichier en mémoire\\n    return bytestream  # Retourner le flux de mémoire\\n\\n# Utilisation des paramètres du bucket S3\\nbucket_name = 'images-projet-deep-learning'\\nhdf5_s3_key = 'train-image.hdf5'\\n\\n# Charger le fichier HDF5 directement depuis S3\\nhdf5_file_stream = load_hdf5_from_s3(bucket_name, hdf5_s3_key)\\n\\n# Charger le dataset de validation dans la mémoire (si nécessaire)\\nif save_val_in_memory:\\n    # Utiliser directement l'objet `h5file` dans `make_in_memory_dataset` sans le rouvrir\\n    val_in_memory = make_in_memory_dataset(hdf5_file_stream, val_meta_dict, val_pos_isic_id, val_pos_target, apply_hair_removal=apply_hair_removal)\\n    \\n\""
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import boto3\n",
    "import h5py\n",
    "import io\n",
    "\n",
    "# Fonction pour charger le fichier HDF5 directement depuis S3 sans le télécharger localement\n",
    "def load_hdf5_from_s3(bucket_name, s3_key):\n",
    "    s3 = boto3.client('s3')\n",
    "    # Télécharger le fichier en mémoire\n",
    "    obj = s3.get_object(Bucket=bucket_name, Key=s3_key)\n",
    "    bytestream = io.BytesIO(obj['Body'].read())  # Lire le fichier en mémoire\n",
    "    return bytestream  # Retourner le flux de mémoire\n",
    "\n",
    "# Utilisation des paramètres du bucket S3\n",
    "bucket_name = 'images-projet-deep-learning'\n",
    "hdf5_s3_key = 'train-image.hdf5'\n",
    "\n",
    "# Charger le fichier HDF5 directement depuis S3\n",
    "hdf5_file_stream = load_hdf5_from_s3(bucket_name, hdf5_s3_key)\n",
    "\n",
    "# Charger le dataset de validation dans la mémoire (si nécessaire)\n",
    "if save_val_in_memory:\n",
    "    # Utiliser directement l'objet `h5file` dans `make_in_memory_dataset` sans le rouvrir\n",
    "    val_in_memory = make_in_memory_dataset(hdf5_file_stream, val_meta_dict, val_pos_isic_id, val_pos_target, apply_hair_removal=apply_hair_removal)\n",
    "    \n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load validation dataset into memory to speed up the model val_loss calc\n",
    "if save_val_in_memory:\n",
    "    val_in_memory = make_in_memory_dataset(hdf5_file, val_meta_dict, val_pos_isic_id, val_pos_target, apply_hair_removal=apply_hair_removal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clear the memory leak in Keras\n",
    "class CustomCallback(tf.keras.callbacks.Callback):\n",
    "  def on_epoch_end(self, epoch, logs=None):\n",
    "    gc.collect()\n",
    "    #print(f\"Epoch {epoch+1} finished. Validation loss: {logs['val_loss']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/tf_keras/src/initializers/initializers.py:121: UserWarning: The initializer RandomNormal is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/tf_keras/src/initializers/initializers.py:121: UserWarning: The initializer RandomUniform is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/numpy/core/getlimits.py:549: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
      "  setattr(self, word, getattr(machar, word).flat[0])\n",
      "/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
      "  return self._float_to_str(self.smallest_subnormal)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "217/217 [==============================] - 92s 323ms/step - loss: 1.2770 - binary_accuracy: 0.7229 - false_negatives: 295.0000 - false_positives: 1623.0000 - true_negatives: 4711.0000 - true_positives: 293.0000 - val_loss: 0.5774 - val_binary_accuracy: 0.7214 - val_false_negatives: 200.0000 - val_false_positives: 58.0000 - val_true_negatives: 527.0000 - val_true_positives: 141.0000\n",
      "EPOCH 2\n",
      "217/217 [==============================] - 70s 321ms/step - loss: 1.1102 - binary_accuracy: 0.7670 - false_negatives: 231.0000 - false_positives: 1382.0000 - true_negatives: 4952.0000 - true_positives: 357.0000 - val_loss: 0.4680 - val_binary_accuracy: 0.8013 - val_false_negatives: 146.0000 - val_false_positives: 38.0000 - val_true_negatives: 547.0000 - val_true_positives: 195.0000\n",
      "EPOCH 3\n",
      "217/217 [==============================] - 71s 327ms/step - loss: 0.9797 - binary_accuracy: 0.8002 - false_negatives: 178.0000 - false_positives: 1205.0000 - true_negatives: 5129.0000 - true_positives: 410.0000 - val_loss: 0.4720 - val_binary_accuracy: 0.8153 - val_false_negatives: 60.0000 - val_false_positives: 111.0000 - val_true_negatives: 474.0000 - val_true_positives: 281.0000\n",
      "EPOCH 4\n",
      "217/217 [==============================] - 70s 320ms/step - loss: 0.9337 - binary_accuracy: 0.8057 - false_negatives: 162.0000 - false_positives: 1183.0000 - true_negatives: 5151.0000 - true_positives: 426.0000 - val_loss: 0.4290 - val_binary_accuracy: 0.8305 - val_false_negatives: 72.0000 - val_false_positives: 85.0000 - val_true_negatives: 500.0000 - val_true_positives: 269.0000\n",
      "EPOCH 5\n",
      "217/217 [==============================] - 71s 326ms/step - loss: 0.9181 - binary_accuracy: 0.8021 - false_negatives: 156.0000 - false_positives: 1214.0000 - true_negatives: 5120.0000 - true_positives: 432.0000 - val_loss: 0.5332 - val_binary_accuracy: 0.7808 - val_false_negatives: 49.0000 - val_false_positives: 154.0000 - val_true_negatives: 431.0000 - val_true_positives: 292.0000\n"
     ]
    }
   ],
   "source": [
    "#Run the model through epochs\n",
    "for epoch in range(1, nb_epochs + 1):\n",
    "    #Make datasets\n",
    "    print(\"EPOCH\", epoch)\n",
    "\n",
    "    #Reinitialize the training dataset with a new shuffle each time\n",
    "    shuffle_seed = 8 + epoch #Next initialization of datasets will have a different shuffle\n",
    "    train_dataset = make_dataset(hdf5_file, train_meta_dict, train_pos_isic_id, train_pos_target, batch_size = train_batch_size, is_training=True, shuffle_seed=shuffle_seed, apply_hair_removal=apply_hair_removal)\n",
    "\n",
    "    #Fit the model, using validation data either stored in memory or on the hard disk\n",
    "    if save_val_in_memory:\n",
    "        mod = model.fit(train_dataset, epochs=1, steps_per_epoch = nb_training_batches, validation_data = val_in_memory, callbacks = [CustomCallback()],\n",
    "                        class_weight={0: weight_for_0, 1: weight_for_1})\n",
    "    else:\n",
    "        val_dataset = make_dataset(hdf5_file, val_meta_dict, val_pos_isic_id, val_pos_target, batch_size = val_batch_size, is_training=False, shuffle_seed=shuffle_seed, apply_hair_removal=apply_hair_removal)\n",
    "        mod = model.fit(train_dataset, epochs=1, steps_per_epoch = nb_training_batches, validation_data = val_dataset, validation_steps = nb_validate_batches, callbacks = [CustomCallback()],\n",
    "                        class_weight={0: weight_for_0, 1: weight_for_1})\n",
    "    \n",
    "    #Save results\n",
    "    if epoch == 1:\n",
    "        results = mod.history\n",
    "    else:\n",
    "        for key in mod.history:   \n",
    "            results[key] += mod.history[key]\n",
    "            \n",
    "    #Export model structure and weights (json and H5) - NOT WORKING\n",
    "    '''    \n",
    "    #Save occasionally\n",
    "    if epoch == 5:\n",
    "        model_json = model.to_json()\n",
    "        with open(savePath + \"model.json\", \"w\") as json_file:\n",
    "            json_file.write(model_json)\n",
    "\n",
    "    if (epoch % wt_save_freq == 0):\n",
    "        now = datetime.datetime.now()\n",
    "\n",
    "        if apply_hair_removal:\n",
    "            modifier = \"with_hair_removal_\"\n",
    "        else:\n",
    "            modifier = \"no_hair_removal_\"\n",
    "        #filename = \"Model_\" + modifier + \"Epoch_\" + str(epoch) + \"_\" + now.strftime(\"%Y-%m-%d_%Hh%Mm%Ss\") + \".weights.h5\"\n",
    "        filename = \"Model\" + \".weights.h5\"\n",
    "        model.save_weights(savePath + filename)\n",
    "    '''    \n",
    "\n",
    "    #Clean memory after use\n",
    "    del mod\n",
    "    del train_dataset\n",
    "    #If save_val_in_memory is not true, we have a generator. In this case, we want to delete the generator.\n",
    "    if save_val_in_memory != True:\n",
    "        del val_dataset\n",
    "    tf.keras.backend.clear_session()\n",
    "    gc.collect()\n",
    "\n",
    "    #Early termination (check after 15 epochs)\n",
    "    if epoch >= 15 and early_break == True:\n",
    "        #Calculate previous three changes, if positive, then loss is increasing\n",
    "        change1 = results[\"val_loss\"][-1] - results[\"val_loss\"][-2]\n",
    "        change2 = results[\"val_loss\"][-2] - results[\"val_loss\"][-3]\n",
    "        change3 = results[\"val_loss\"][-3] - results[\"val_loss\"][-4]\n",
    "\n",
    "        #Three consecutive increases in validation loss will stop the model\n",
    "        if change1 > 0 and change2 > 0 and change3 > 0:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"hybrid_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             multiple                  2432      \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           multiple                  51264     \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  multiple                  0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " flatten (Flatten)           multiple                  0         \n",
      "                                                                 \n",
      " dense (Dense)               multiple                  1219896   \n",
      "                                                                 \n",
      " dropout (Dropout)           multiple                  0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             multiple                  1332      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         multiple                  0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             multiple                  37        \n",
      "                                                                 \n",
      " concatenate (Concatenate)   multiple                  0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1274961 (4.86 MB)\n",
      "Trainable params: 1274961 (4.86 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_ = tf.keras.models.load_model(\"save1.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n#Import results from file\\nimported_results = pd.read_csv(modelResPath).to_dict(orient='list')\\n\""
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Save results to file\n",
    "pd.DataFrame.from_dict(results).to_csv(modelResPath, index=False)\n",
    "\n",
    "\"\"\"\n",
    "#Import results from file\n",
    "imported_results = pd.read_csv(modelResPath).to_dict(orient='list')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'hybrid_model/conv2d/kernel:0' shape=(5, 5, 3, 32) dtype=float32, numpy=\n",
       " array([[[[-6.50972724e-02, -3.29118147e-02,  3.54769491e-02, ...,\n",
       "           -2.76434720e-02, -1.22326307e-01, -8.40713903e-02],\n",
       "          [-3.21318656e-02, -7.10730255e-02, -1.91045273e-02, ...,\n",
       "           -8.80925506e-02, -9.27662179e-02, -2.69741602e-02],\n",
       "          [-1.04082756e-01, -1.26705781e-01, -8.77464935e-02, ...,\n",
       "           -2.17576753e-02, -6.65811524e-02,  6.70795189e-03]],\n",
       " \n",
       "         [[-1.24591619e-01, -8.41553062e-02,  6.06087595e-03, ...,\n",
       "           -5.85049763e-02,  3.20576914e-02, -4.46815677e-02],\n",
       "          [-1.46839306e-01,  6.73104152e-02, -3.02820094e-02, ...,\n",
       "           -7.75230080e-02, -4.44932170e-02,  7.37336790e-03],\n",
       "          [-3.35734934e-02, -3.41610163e-02,  8.03634226e-02, ...,\n",
       "            1.30789494e-02, -9.76392254e-02, -4.35819775e-02]],\n",
       " \n",
       "         [[-4.36801538e-02, -7.88492558e-04, -1.40936658e-01, ...,\n",
       "           -5.38865989e-03,  1.36507209e-02,  3.61908460e-04],\n",
       "          [-1.76023386e-04, -8.47140700e-02, -7.76573569e-02, ...,\n",
       "           -1.29207298e-01, -4.48299572e-02, -1.17895462e-01],\n",
       "          [-6.30862489e-02, -3.07951998e-02, -3.60961929e-02, ...,\n",
       "           -4.49800082e-02, -7.09977932e-04, -6.15009591e-02]],\n",
       " \n",
       "         [[-5.13792001e-02, -1.90897640e-02, -6.53625205e-02, ...,\n",
       "           -4.94095683e-02, -5.23825623e-02, -7.66028475e-04],\n",
       "          [-6.45891428e-02, -2.15091240e-02, -4.59379591e-02, ...,\n",
       "           -2.30108965e-02, -8.98559093e-02, -6.48579150e-02],\n",
       "          [-7.95169994e-02, -3.37340459e-02, -1.47048254e-02, ...,\n",
       "           -1.82015263e-02, -1.06485210e-01, -5.69895841e-02]],\n",
       " \n",
       "         [[-1.07479645e-02, -6.99682801e-04, -1.65973287e-02, ...,\n",
       "           -8.85087475e-02, -1.16902083e-01, -2.99663516e-03],\n",
       "          [-2.31645647e-02, -2.18588263e-02,  9.01345685e-02, ...,\n",
       "           -2.10664123e-02, -6.47852048e-02, -5.44058941e-02],\n",
       "          [ 5.06241992e-02, -1.15855359e-01, -6.88551143e-02, ...,\n",
       "           -5.49575016e-02, -2.53206585e-03, -7.82175586e-02]]],\n",
       " \n",
       " \n",
       "        [[[-5.47907539e-02, -1.06154174e-01,  7.24455044e-02, ...,\n",
       "           -8.79678577e-02, -8.98740217e-02,  8.94561526e-04],\n",
       "          [-3.28228883e-02, -3.68016660e-02, -3.90564539e-02, ...,\n",
       "           -1.59572940e-02, -7.83407092e-02, -8.98504853e-02],\n",
       "          [-1.39806092e-01, -1.77446306e-01,  2.15154979e-02, ...,\n",
       "           -2.94678118e-02,  5.01251360e-03, -1.30524695e-01]],\n",
       " \n",
       "         [[-6.09929822e-02, -5.68829775e-02, -4.12040763e-02, ...,\n",
       "           -4.13136259e-02, -1.22467652e-01, -5.06795682e-02],\n",
       "          [-2.32993681e-02, -9.92747471e-02, -6.98214397e-02, ...,\n",
       "           -4.10110317e-02,  1.04011977e-02, -1.34704718e-02],\n",
       "          [ 3.15326564e-02, -8.37283880e-02, -9.31447297e-02, ...,\n",
       "           -8.48773029e-03, -6.15040585e-02,  3.80178057e-02]],\n",
       " \n",
       "         [[-1.13142468e-02,  4.25473861e-02,  1.13091851e-02, ...,\n",
       "           -5.23456335e-02, -2.68608462e-02, -1.75712090e-02],\n",
       "          [-1.25094131e-01, -6.66617900e-02,  1.20984474e-02, ...,\n",
       "           -1.60646409e-01, -7.56330490e-02, -6.26100376e-02],\n",
       "          [-6.72832355e-02, -6.10329844e-02, -7.86650255e-02, ...,\n",
       "           -1.19677715e-01, -7.35729262e-02, -4.86314334e-02]],\n",
       " \n",
       "         [[-2.48994194e-02, -2.20589973e-02, -1.84448604e-02, ...,\n",
       "            5.45360520e-02, -3.31112631e-02,  6.29425570e-02],\n",
       "          [-1.15373068e-01, -6.15735203e-02,  1.62578821e-02, ...,\n",
       "           -5.26405424e-02, -8.76759291e-02, -2.25232709e-02],\n",
       "          [-6.33686334e-02, -1.45854518e-01, -1.55666070e-02, ...,\n",
       "           -5.06045148e-02,  6.91787153e-02, -4.21097316e-02]],\n",
       " \n",
       "         [[-1.75789371e-02, -8.17141682e-02, -9.32746753e-02, ...,\n",
       "           -9.64270234e-02, -9.46747884e-02, -3.99243161e-02],\n",
       "          [-4.47183959e-02, -1.27460450e-01, -9.36844498e-02, ...,\n",
       "            4.13580947e-02, -2.35934556e-02,  1.06877834e-02],\n",
       "          [-4.06892747e-02, -2.61379816e-02,  2.94453558e-03, ...,\n",
       "           -6.30558804e-02, -3.67358178e-02, -5.13794981e-02]]],\n",
       " \n",
       " \n",
       "        [[[-9.45899710e-02, -3.95706370e-02, -9.29014161e-02, ...,\n",
       "            1.30442539e-02, -4.64963727e-02, -3.90498117e-02],\n",
       "          [-2.56372429e-02, -4.50195819e-02, -4.15596180e-03, ...,\n",
       "           -3.70638743e-02, -1.65166438e-01, -1.34001583e-01],\n",
       "          [-4.22858335e-02, -6.35983869e-02, -3.42961587e-02, ...,\n",
       "           -6.68474436e-02, -1.78310405e-02, -1.76799763e-02]],\n",
       " \n",
       "         [[ 8.42196576e-04, -2.26695072e-02, -1.14357024e-02, ...,\n",
       "           -3.23394239e-02, -4.10727821e-02, -8.32467973e-02],\n",
       "          [-8.93169940e-02, -9.17563364e-02,  2.94329971e-02, ...,\n",
       "           -9.53245237e-02,  4.61913040e-03, -1.71171099e-01],\n",
       "          [-4.50818129e-02, -1.56195601e-02, -3.23302820e-02, ...,\n",
       "           -1.83221281e-01, -7.06239492e-02, -8.14127773e-02]],\n",
       " \n",
       "         [[-9.23633277e-02, -5.03388122e-02, -4.50341217e-02, ...,\n",
       "           -1.47407547e-01, -7.00986432e-03, -3.74414809e-02],\n",
       "          [-1.11480437e-01, -9.64066759e-02, -3.73693034e-02, ...,\n",
       "           -7.38716871e-02, -8.17541704e-02, -1.86309859e-01],\n",
       "          [-4.68308814e-02, -4.09650877e-02, -2.59372573e-02, ...,\n",
       "           -2.65065450e-02,  5.72181121e-02, -1.27171531e-01]],\n",
       " \n",
       "         [[ 3.56384590e-02, -8.52136966e-03, -3.84829546e-05, ...,\n",
       "           -7.20903948e-02, -1.12599686e-01, -6.14744499e-02],\n",
       "          [-3.67386616e-03, -3.86325233e-02, -1.89858060e-02, ...,\n",
       "           -4.42391410e-02, -9.08928216e-02, -1.07550658e-01],\n",
       "          [-6.60068616e-02, -8.22114572e-02, -5.90438098e-02, ...,\n",
       "            3.03553399e-02, -5.66829778e-02, -4.64338548e-02]],\n",
       " \n",
       "         [[-5.87250739e-02, -2.70442050e-02, -1.12333121e-02, ...,\n",
       "           -1.42807402e-02, -2.03119591e-01, -6.00861795e-02],\n",
       "          [-6.47254894e-03, -1.01588301e-01, -5.59617914e-02, ...,\n",
       "           -4.15832885e-02,  4.10292782e-02, -8.11814815e-02],\n",
       "          [-5.09646460e-02, -9.15757790e-02,  2.62898020e-02, ...,\n",
       "           -1.78510398e-02, -6.82189837e-02, -1.33670285e-01]]],\n",
       " \n",
       " \n",
       "        [[[ 7.29669333e-02, -1.09567799e-01, -1.07375095e-02, ...,\n",
       "           -1.64801031e-02, -6.64507002e-02, -4.63331752e-02],\n",
       "          [-2.44928934e-02, -9.26617160e-02,  3.02917603e-03, ...,\n",
       "           -3.67179178e-02, -1.34948296e-02, -8.75252560e-02],\n",
       "          [-2.49589980e-02, -3.98258008e-02, -4.63309698e-02, ...,\n",
       "           -9.53789055e-02, -5.57975210e-02, -5.30242436e-02]],\n",
       " \n",
       "         [[ 1.25877440e-01, -1.88342631e-01, -6.12697862e-02, ...,\n",
       "            6.54036878e-03, -3.68648656e-02, -6.96297437e-02],\n",
       "          [ 2.20549051e-02, -2.15090159e-02, -3.91569100e-02, ...,\n",
       "           -1.56086106e-02, -1.90702127e-03, -1.51984379e-01],\n",
       "          [ 5.65605871e-02,  1.72569528e-02, -1.80158913e-02, ...,\n",
       "            4.40924764e-02, -7.11663514e-02, -1.00789778e-01]],\n",
       " \n",
       "         [[ 1.30026974e-02, -1.44915417e-01, -4.99773435e-02, ...,\n",
       "           -9.83181000e-02, -3.77368368e-02, -1.26296729e-01],\n",
       "          [ 1.10730063e-02,  1.57584846e-02, -1.19205341e-01, ...,\n",
       "           -4.08753492e-02,  4.44760807e-02, -3.01893800e-02],\n",
       "          [-1.98879525e-01, -7.11596757e-02, -1.57617405e-02, ...,\n",
       "           -1.03097238e-01, -9.96789783e-02, -9.48317274e-02]],\n",
       " \n",
       "         [[-4.09292094e-02,  9.80379432e-03,  4.01739366e-02, ...,\n",
       "           -4.78684306e-02, -1.26490980e-01, -1.50884211e-01],\n",
       "          [-1.27178758e-01, -2.22134870e-02,  4.42502871e-02, ...,\n",
       "           -1.31643027e-01, -1.03508435e-01, -9.89558920e-02],\n",
       "          [ 4.69144583e-02, -4.12625670e-02, -4.33356538e-02, ...,\n",
       "           -5.86407669e-02, -1.65506005e-01, -9.28761289e-02]],\n",
       " \n",
       "         [[-8.88614506e-02,  5.92600442e-02, -2.00249031e-02, ...,\n",
       "           -3.88523452e-02, -1.15111567e-01, -7.97637254e-02],\n",
       "          [-1.23683892e-01, -9.89770442e-02,  7.43411630e-02, ...,\n",
       "           -3.38571519e-02, -1.26992717e-01, -1.19001850e-01],\n",
       "          [-3.50057445e-02, -5.93406670e-02, -3.74034084e-02, ...,\n",
       "            1.45335859e-02, -3.97235714e-02, -8.92660916e-02]]],\n",
       " \n",
       " \n",
       "        [[[ 1.32980039e-02, -1.15461431e-01, -5.07998317e-02, ...,\n",
       "           -9.50617902e-03, -2.58362368e-02, -3.95294353e-02],\n",
       "          [-6.32593930e-02, -8.24194103e-02, -1.03542246e-01, ...,\n",
       "            2.54789162e-02, -1.17829151e-01, -6.35712445e-02],\n",
       "          [ 3.98640484e-02, -9.70335528e-02, -6.89379051e-02, ...,\n",
       "           -4.05638292e-02, -8.13955292e-02, -6.18776865e-02]],\n",
       " \n",
       "         [[-3.88081595e-02, -6.83301091e-02,  1.23580508e-01, ...,\n",
       "           -2.41762027e-02, -4.72558886e-02, -1.40404776e-02],\n",
       "          [-9.00009871e-02, -6.74946085e-02, -1.59018859e-02, ...,\n",
       "           -1.21188529e-01,  1.20639801e-01, -6.95966482e-02],\n",
       "          [-1.70951143e-01, -2.01948043e-02,  4.55628373e-02, ...,\n",
       "           -3.62913646e-02,  2.87688095e-02, -7.01355860e-02]],\n",
       " \n",
       "         [[-3.67681198e-02,  4.03182134e-02, -4.33059633e-02, ...,\n",
       "           -1.63454693e-02, -3.30690667e-02,  8.05363338e-03],\n",
       "          [-2.48303153e-02,  3.94167230e-02, -5.37789539e-02, ...,\n",
       "           -6.21236004e-02, -2.19160337e-02, -4.30711359e-02],\n",
       "          [-2.53830906e-02, -6.46212697e-02,  9.88251157e-03, ...,\n",
       "           -6.38183998e-03, -7.75620416e-02, -3.83622758e-02]],\n",
       " \n",
       "         [[-1.84379946e-02, -8.51159915e-02, -3.02119050e-02, ...,\n",
       "            4.70592231e-02, -1.36750638e-01, -2.83928961e-02],\n",
       "          [-5.54466248e-02, -9.83628444e-03,  7.36230286e-03, ...,\n",
       "           -1.03790462e-01, -6.85622022e-02, -1.07173719e-01],\n",
       "          [ 7.13953599e-02, -1.03767840e-02,  1.06188573e-03, ...,\n",
       "           -4.02301364e-02, -5.52175306e-02, -1.22643970e-01]],\n",
       " \n",
       "         [[-5.55149987e-02, -1.13108210e-01, -3.87528092e-02, ...,\n",
       "           -3.00368201e-02, -8.00711736e-02, -5.60247293e-03],\n",
       "          [-3.72919068e-02,  2.19342802e-02, -8.93456787e-02, ...,\n",
       "           -1.36641283e-02, -4.19337526e-02, -1.15280859e-01],\n",
       "          [-4.45227884e-02, -4.27788645e-02, -1.09154038e-01, ...,\n",
       "            3.36243063e-02, -2.37316210e-02, -1.43119141e-01]]]],\n",
       "       dtype=float32)>,\n",
       " <tf.Variable 'hybrid_model/conv2d/bias:0' shape=(32,) dtype=float32, numpy=\n",
       " array([-0.12351941, -0.03985039, -0.05573995, -0.09868482, -0.01662686,\n",
       "        -0.09514494, -0.01624856, -0.05350455, -0.09392361, -0.06050676,\n",
       "        -0.03416575, -0.05367588, -0.09744797, -0.00383766, -0.06484006,\n",
       "        -0.0466109 , -0.03289818, -0.08796017,  0.01034966,  0.05568675,\n",
       "        -0.0546278 , -0.08467758,  0.06196766, -0.02711223, -0.08901956,\n",
       "        -0.05025209, -0.12469756, -0.01108924, -0.04155798, -0.06077271,\n",
       "        -0.09006701,  0.02799689], dtype=float32)>,\n",
       " <tf.Variable 'hybrid_model/conv2d_1/kernel:0' shape=(5, 5, 32, 64) dtype=float32, numpy=\n",
       " array([[[[-6.10189885e-03, -2.24214457e-02,  2.66777277e-02, ...,\n",
       "           -4.06719930e-02, -3.33349966e-02,  5.46492776e-03],\n",
       "          [-4.58253473e-02, -1.18322529e-01, -1.23633206e-01, ...,\n",
       "           -5.59781641e-02,  3.84664536e-02, -1.23943605e-01],\n",
       "          [-8.83022696e-02,  7.38883391e-02, -6.49057478e-02, ...,\n",
       "            1.42417429e-02, -8.32782611e-02, -9.94876400e-02],\n",
       "          ...,\n",
       "          [-7.22985491e-02, -9.14591998e-02,  3.91734578e-02, ...,\n",
       "            1.48355458e-02, -1.40911378e-02, -4.10669595e-02],\n",
       "          [ 5.65936230e-02, -1.08940236e-01,  3.70105729e-02, ...,\n",
       "            1.29187793e-01, -9.52328071e-02, -1.96631085e-02],\n",
       "          [ 1.09442659e-01, -1.11825161e-01, -5.23260087e-02, ...,\n",
       "           -2.54872926e-02,  1.30792130e-02,  9.06806812e-02]],\n",
       " \n",
       "         [[-4.90533896e-02, -1.84199901e-03, -2.19162200e-02, ...,\n",
       "            1.00449296e-02,  8.90754983e-02,  4.39917818e-02],\n",
       "          [-2.84193885e-02,  4.78641279e-02, -7.92667642e-02, ...,\n",
       "           -5.91612756e-02, -1.37066431e-02, -5.75635629e-03],\n",
       "          [-2.53830906e-02, -5.28867058e-02, -2.57767960e-02, ...,\n",
       "            4.74052243e-02, -1.20872058e-01, -2.86258291e-02],\n",
       "          ...,\n",
       "          [ 9.15675089e-02, -4.75165201e-03,  1.66802686e-02, ...,\n",
       "           -1.59062937e-01, -5.13008535e-02,  2.19939649e-02],\n",
       "          [ 3.76033597e-02, -7.81675950e-02,  7.13478820e-03, ...,\n",
       "            1.20037019e-01, -2.35896166e-02, -1.45425126e-01],\n",
       "          [ 3.44823599e-02, -5.14079556e-02,  1.43245971e-02, ...,\n",
       "           -1.56180253e-02, -1.80561747e-02,  9.47124697e-03]],\n",
       " \n",
       "         [[ 3.13407667e-02,  2.73868144e-02, -1.39821991e-02, ...,\n",
       "           -8.78997147e-02, -3.54938842e-02, -5.60384281e-02],\n",
       "          [ 5.62822027e-03, -5.75499125e-02, -5.91069646e-02, ...,\n",
       "           -2.08172426e-02, -5.40092327e-02, -7.18704015e-02],\n",
       "          [-4.78649624e-02, -1.07851923e-01, -1.25569433e-01, ...,\n",
       "           -5.51114045e-03, -1.06465250e-01, -5.08023202e-02],\n",
       "          ...,\n",
       "          [-4.11378853e-02,  5.76879233e-02, -3.33903134e-02, ...,\n",
       "           -7.95047209e-02, -4.76001278e-02, -1.04146428e-01],\n",
       "          [-2.28102449e-02, -4.61330786e-02,  8.44829064e-03, ...,\n",
       "            2.44213119e-02,  3.64773571e-02, -1.55991495e-01],\n",
       "          [ 5.64606003e-02, -9.05793607e-02,  3.65829282e-02, ...,\n",
       "           -8.31219926e-02, -1.54404780e-02,  2.31144931e-02]],\n",
       " \n",
       "         [[-2.14193054e-02,  8.54626577e-03, -1.40635967e-02, ...,\n",
       "           -2.23061219e-02,  4.86236028e-02,  1.07793286e-01],\n",
       "          [ 9.85387526e-03, -3.30154113e-02, -9.99261066e-03, ...,\n",
       "           -6.61112443e-02, -1.13652296e-01, -6.93584653e-03],\n",
       "          [ 9.00430083e-02,  2.75019333e-02, -1.06853142e-01, ...,\n",
       "           -4.81856205e-02, -2.89664362e-02,  1.88507736e-02],\n",
       "          ...,\n",
       "          [-7.58023700e-03, -1.06305815e-01, -5.81542477e-02, ...,\n",
       "           -8.47505108e-02,  2.32589804e-02, -7.86811486e-02],\n",
       "          [-1.86226759e-02, -7.30452091e-02,  7.84683004e-02, ...,\n",
       "            6.31297193e-03, -7.16143399e-02, -2.76313871e-02],\n",
       "          [-2.05111969e-02, -5.26312701e-02, -8.93407781e-03, ...,\n",
       "            2.39660162e-02,  3.13784778e-02,  5.42913489e-02]],\n",
       " \n",
       "         [[ 7.87096098e-02, -8.20279773e-03, -5.01505807e-02, ...,\n",
       "           -8.07144493e-02,  5.11260815e-02, -1.68289319e-02],\n",
       "          [-2.58479454e-02, -8.06449428e-02, -3.60595584e-02, ...,\n",
       "           -4.51734150e-03, -4.69960682e-02, -1.15929633e-01],\n",
       "          [-1.04412995e-02, -9.59642082e-02, -8.81070346e-02, ...,\n",
       "           -6.03751689e-02,  5.05021168e-03, -2.37708334e-02],\n",
       "          ...,\n",
       "          [-2.51341201e-02, -8.52560997e-03, -3.29174958e-02, ...,\n",
       "           -8.98034796e-02, -4.24802899e-02,  6.58347562e-04],\n",
       "          [-1.78838335e-02,  3.49630453e-02,  1.34668518e-02, ...,\n",
       "            1.52127584e-02,  3.71660069e-02, -4.90544103e-02],\n",
       "          [ 5.46417758e-02,  5.41808568e-02, -3.28016579e-02, ...,\n",
       "           -4.94656749e-02, -1.69590544e-02,  2.66379584e-02]]],\n",
       " \n",
       " \n",
       "        [[[ 3.15762199e-02, -2.54947133e-02, -3.49379927e-02, ...,\n",
       "           -3.53840850e-02,  4.84852418e-02,  3.15413512e-02],\n",
       "          [ 7.03392550e-02, -4.63941768e-02, -2.04342883e-02, ...,\n",
       "            1.05569689e-02,  2.66926978e-02, -7.77887031e-02],\n",
       "          [-2.83801202e-02,  2.74479948e-03, -7.70556703e-02, ...,\n",
       "           -4.84369472e-02,  4.23399433e-02,  7.00295204e-04],\n",
       "          ...,\n",
       "          [-7.35066831e-03, -4.68609408e-02, -1.00455068e-01, ...,\n",
       "            2.95065455e-02, -7.60536939e-02, -1.31906778e-01],\n",
       "          [-4.18930277e-02, -5.27990190e-03,  8.71869922e-02, ...,\n",
       "            1.01055704e-01, -1.64700616e-02, -7.26238936e-02],\n",
       "          [-3.96664217e-02, -5.00851460e-02,  5.24012968e-02, ...,\n",
       "           -2.17851307e-02,  4.29072939e-02,  1.06821619e-01]],\n",
       " \n",
       "         [[ 4.67834854e-03,  5.50615750e-02, -3.33763547e-02, ...,\n",
       "            5.73960645e-03,  7.17942864e-02,  6.31276518e-02],\n",
       "          [ 7.51850680e-02, -1.24438561e-01, -7.44141415e-02, ...,\n",
       "           -1.26929116e-02, -3.19453585e-03,  1.32328114e-02],\n",
       "          [-6.68797195e-02, -3.72871459e-02, -4.35707569e-02, ...,\n",
       "           -1.84466422e-03, -2.39861626e-02, -4.15173499e-03],\n",
       "          ...,\n",
       "          [-3.02206613e-02, -6.90937564e-02, -4.74605933e-02, ...,\n",
       "            6.12720698e-02, -1.30496845e-01, -1.20165922e-01],\n",
       "          [ 2.47782394e-02, -1.37646757e-02,  1.75021328e-02, ...,\n",
       "            1.20009378e-01, -8.30964670e-02, -7.22247288e-02],\n",
       "          [ 1.99058633e-02,  7.93278441e-02, -1.19796529e-01, ...,\n",
       "           -9.72631201e-02, -2.29362976e-02,  3.23604755e-02]],\n",
       " \n",
       "         [[-4.58425023e-02,  1.56095130e-02, -3.05092987e-02, ...,\n",
       "            2.02815309e-02, -5.09737097e-02,  2.68434566e-02],\n",
       "          [-3.31913084e-02, -9.87594500e-02, -1.19971447e-01, ...,\n",
       "            2.18506262e-04, -3.58016007e-02, -1.43015776e-02],\n",
       "          [ 6.55012205e-02, -1.35549530e-01, -9.13089961e-02, ...,\n",
       "           -5.68568818e-02, -4.40790616e-02, -8.92865136e-02],\n",
       "          ...,\n",
       "          [-5.27736135e-02,  4.43705507e-02,  7.70055428e-02, ...,\n",
       "           -5.58869541e-02, -3.98100577e-02, -7.22750276e-02],\n",
       "          [ 6.92598009e-03, -1.60525106e-02, -7.42032658e-03, ...,\n",
       "            3.48469429e-02, -1.42846510e-01, -6.11549355e-02],\n",
       "          [-2.66016182e-02, -8.21289886e-03, -6.63316995e-02, ...,\n",
       "            2.75331438e-02, -2.51998845e-02, -2.88277138e-02]],\n",
       " \n",
       "         [[-7.13656768e-02,  7.53051639e-02, -1.22670196e-02, ...,\n",
       "           -3.14847799e-03,  2.71472167e-02,  9.41390023e-02],\n",
       "          [-3.05594858e-02, -1.27569839e-01, -1.01741411e-01, ...,\n",
       "           -5.48439249e-02, -3.73503193e-02, -4.44142707e-02],\n",
       "          [-1.32030398e-02,  1.04764551e-02, -1.75134167e-02, ...,\n",
       "           -1.11018583e-01, -7.43877217e-02, -6.43888786e-02],\n",
       "          ...,\n",
       "          [-5.53740077e-02, -6.95311725e-02, -9.73908510e-03, ...,\n",
       "           -8.43318030e-02,  6.75937301e-03, -2.07717922e-02],\n",
       "          [-8.93848464e-02,  2.73079472e-03,  4.68872152e-02, ...,\n",
       "            1.34285539e-01,  3.36376093e-02,  4.54042777e-02],\n",
       "          [ 1.15729887e-02, -3.88520211e-03, -4.42749485e-02, ...,\n",
       "           -8.22108909e-02,  3.19548175e-02, -2.64371466e-02]],\n",
       " \n",
       "         [[ 2.49385578e-03,  5.19846790e-02, -1.11428685e-02, ...,\n",
       "           -1.18928231e-01,  8.31990782e-03, -7.97053277e-02],\n",
       "          [ 2.60068160e-02, -5.79367951e-02, -1.03220358e-01, ...,\n",
       "            2.03035194e-02, -1.44728094e-01, -9.96944401e-03],\n",
       "          [ 6.74425764e-03,  8.37054178e-02, -1.02643184e-01, ...,\n",
       "           -4.85655591e-02, -6.55557811e-02, -9.01567638e-02],\n",
       "          ...,\n",
       "          [ 1.88214034e-02, -6.64177537e-03, -6.76988512e-02, ...,\n",
       "            2.28838734e-02, -6.00710548e-02, -1.94054544e-02],\n",
       "          [ 1.11128502e-01,  3.47836651e-02,  5.60521111e-02, ...,\n",
       "            1.63153082e-03,  5.59822917e-02,  1.40678529e-02],\n",
       "          [-1.81620717e-02, -5.60785830e-02, -3.90655547e-02, ...,\n",
       "           -2.67404281e-02,  1.10694002e-02,  9.84052569e-02]]],\n",
       " \n",
       " \n",
       "        [[[-1.63517315e-02, -2.77834963e-02,  8.04122835e-02, ...,\n",
       "           -5.88538051e-02,  2.05116980e-02,  2.76357029e-02],\n",
       "          [ 8.41465499e-03, -7.13780895e-02, -1.42267480e-01, ...,\n",
       "           -1.20831847e-01, -1.19189389e-01, -1.13574527e-01],\n",
       "          [ 5.06229885e-02, -5.25750890e-02,  1.51296249e-02, ...,\n",
       "           -4.01629359e-02, -8.80074278e-02, -1.58954293e-01],\n",
       "          ...,\n",
       "          [ 6.80762604e-02, -7.39037693e-02, -9.96274278e-02, ...,\n",
       "           -1.54738709e-01, -9.43845510e-02, -3.04820631e-02],\n",
       "          [ 1.46075180e-02, -4.08533625e-02, -3.77926864e-02, ...,\n",
       "            3.70691121e-02,  3.52258869e-02, -4.79761325e-02],\n",
       "          [-8.00704118e-03, -6.83005378e-02, -4.40303423e-02, ...,\n",
       "           -5.90986460e-02, -2.70590987e-02,  8.19463283e-02]],\n",
       " \n",
       "         [[-3.32535580e-02, -1.45374108e-02, -1.55808507e-02, ...,\n",
       "           -1.08547166e-01, -4.27592061e-02,  1.18742464e-03],\n",
       "          [-1.89511590e-02, -9.74560380e-02, -6.27609193e-02, ...,\n",
       "           -9.74248722e-02, -9.40529779e-02,  3.74371260e-02],\n",
       "          [ 3.88176553e-02, -6.99767396e-02, -4.51240055e-02, ...,\n",
       "           -5.79026155e-02, -6.42057508e-02, -7.26133212e-02],\n",
       "          ...,\n",
       "          [ 1.31492745e-02, -6.03056215e-02,  3.67676653e-02, ...,\n",
       "           -7.33150989e-02, -5.25522493e-02,  1.93036422e-02],\n",
       "          [ 1.56191709e-02, -1.56103342e-03,  9.33610019e-04, ...,\n",
       "            3.31511982e-02, -6.87215030e-02, -1.58785149e-01],\n",
       "          [-2.33547296e-02, -7.42094442e-02,  3.23692262e-02, ...,\n",
       "           -1.58363562e-02,  3.52391675e-02,  3.11959442e-02]],\n",
       " \n",
       "         [[ 3.52000855e-02,  2.96148863e-02, -3.02031338e-02, ...,\n",
       "           -5.97365908e-02,  7.99463540e-02,  5.07366955e-02],\n",
       "          [-5.26118390e-02, -3.79795656e-02,  1.48153603e-02, ...,\n",
       "            1.83899570e-02, -1.23204298e-01, -9.18345377e-02],\n",
       "          [-2.05147490e-02, -1.70675125e-02, -3.08428854e-02, ...,\n",
       "           -2.79658698e-02, -4.33640629e-02, -1.27518773e-01],\n",
       "          ...,\n",
       "          [-4.74398881e-02,  1.59403756e-02, -1.92834094e-01, ...,\n",
       "           -1.08878113e-01, -8.61832350e-02, -1.00703500e-01],\n",
       "          [-5.99591620e-02, -3.60758826e-02,  7.28168562e-02, ...,\n",
       "            4.07189094e-02,  1.97733566e-02,  1.49493357e-02],\n",
       "          [ 3.12128980e-02, -1.08454280e-01,  1.18008023e-02, ...,\n",
       "            1.14632472e-02, -2.57394966e-02, -9.22511052e-03]],\n",
       " \n",
       "         [[ 2.56076548e-02, -3.94607112e-02, -4.18272354e-02, ...,\n",
       "            3.87775153e-02,  5.14031239e-02, -2.38582846e-02],\n",
       "          [ 1.19531296e-01, -4.91514802e-02,  4.00119200e-02, ...,\n",
       "            6.70024240e-03, -4.21580710e-02, -1.20773755e-01],\n",
       "          [-1.56506803e-02, -9.07808468e-02, -2.46711727e-02, ...,\n",
       "           -7.88240358e-02, -6.47805110e-02,  2.20058300e-03],\n",
       "          ...,\n",
       "          [ 4.06635702e-02, -1.03641655e-02, -1.09489551e-02, ...,\n",
       "            8.14095885e-02, -5.99398874e-02, -9.45840105e-02],\n",
       "          [ 4.04332839e-02, -8.21413696e-02,  2.14527436e-02, ...,\n",
       "            1.45623147e-01, -9.06744301e-02, -3.33943740e-02],\n",
       "          [-5.53521737e-02, -8.79385099e-02, -1.02021843e-01, ...,\n",
       "           -8.10357705e-02, -2.29304004e-02,  3.81005593e-02]],\n",
       " \n",
       "         [[-1.99614577e-02,  6.83129653e-02,  8.15236568e-03, ...,\n",
       "           -5.32675460e-02,  4.91126031e-02,  1.42469751e-02],\n",
       "          [-2.61146314e-02, -8.96373913e-02, -1.69182736e-02, ...,\n",
       "            1.38811274e-02, -2.43653115e-02, -2.57218201e-02],\n",
       "          [ 4.41065840e-02,  1.16903298e-02, -4.06816602e-02, ...,\n",
       "           -3.20092193e-03,  5.98662253e-03, -1.04476877e-01],\n",
       "          ...,\n",
       "          [ 9.30086002e-02,  5.94938695e-02, -1.72376595e-02, ...,\n",
       "           -8.56298283e-02, -1.68775283e-02, -1.24129455e-03],\n",
       "          [ 2.26303842e-02, -4.23777774e-02,  1.22241601e-01, ...,\n",
       "            1.70499329e-02, -2.92847690e-05, -1.21093445e-01],\n",
       "          [-1.25595003e-01, -5.82044870e-02, -9.25280806e-03, ...,\n",
       "           -5.22984788e-02,  5.04737627e-03,  4.91667539e-02]]],\n",
       " \n",
       " \n",
       "        [[[ 4.06886041e-02,  2.33721919e-02,  5.81024960e-02, ...,\n",
       "           -1.12393722e-02, -2.53152493e-02, -1.80208012e-02],\n",
       "          [-3.66188362e-02, -6.28893748e-02, -1.26551136e-01, ...,\n",
       "           -1.05643176e-01, -4.42961454e-02, -1.17558785e-01],\n",
       "          [-3.16723883e-02, -1.54375760e-02, -2.70184129e-02, ...,\n",
       "           -1.85994748e-02,  2.36291881e-03, -9.96207669e-02],\n",
       "          ...,\n",
       "          [-2.62166150e-02, -3.18838321e-02, -4.74897511e-02, ...,\n",
       "           -7.67498836e-02, -7.20801428e-02, -1.63807124e-02],\n",
       "          [-3.86890694e-02, -1.56604826e-01,  1.56766161e-01, ...,\n",
       "            6.92886710e-02, -6.16867580e-02, -6.23885095e-02],\n",
       "          [ 9.92144197e-02, -4.78516798e-03,  4.15699072e-02, ...,\n",
       "           -8.51034671e-02, -1.98898949e-02,  5.37626930e-02]],\n",
       " \n",
       "         [[-7.40344301e-02,  2.68135909e-02,  1.04054116e-01, ...,\n",
       "           -2.41792649e-02,  1.97580643e-03,  1.83335859e-02],\n",
       "          [-3.03855818e-02, -2.23491546e-02, -7.26147220e-02, ...,\n",
       "           -2.90325545e-02, -1.28434496e-02, -6.07924163e-02],\n",
       "          [-1.11778677e-01, -6.60480708e-02,  5.32835908e-02, ...,\n",
       "           -1.15720503e-01, -1.00642949e-01, -2.19037645e-02],\n",
       "          ...,\n",
       "          [-1.28402179e-02, -5.91157936e-02,  5.58404028e-02, ...,\n",
       "           -8.99912417e-02, -6.31666854e-02, -3.36854383e-02],\n",
       "          [ 2.38965787e-02, -1.28762815e-02,  6.70290142e-02, ...,\n",
       "            9.18663219e-02, -2.35276371e-02, -6.55658990e-02],\n",
       "          [ 2.03733481e-02, -1.13385983e-01, -8.83903205e-02, ...,\n",
       "           -6.31039515e-02,  6.16743639e-02, -7.90207647e-03]],\n",
       " \n",
       "         [[ 1.14393355e-02,  7.26512894e-02,  1.36105092e-02, ...,\n",
       "            3.73233226e-03, -6.69448078e-02, -3.23363207e-02],\n",
       "          [ 1.90785376e-03, -1.86358169e-01, -3.16670313e-02, ...,\n",
       "           -7.99946487e-02, -1.38939977e-01, -7.61129856e-02],\n",
       "          [ 4.14086096e-02, -2.52257977e-02, -6.89346716e-02, ...,\n",
       "           -2.92303320e-02,  1.51410680e-02, -5.70980422e-02],\n",
       "          ...,\n",
       "          [-2.91720126e-02,  1.14576682e-01, -4.90414612e-02, ...,\n",
       "           -1.04542285e-01, -1.94142554e-02, -9.90434438e-02],\n",
       "          [ 3.97956818e-02, -8.10041428e-02,  8.66097808e-02, ...,\n",
       "            5.63340187e-02, -2.95337737e-02, -1.00440584e-01],\n",
       "          [ 5.18010519e-02, -1.54267833e-01, -6.18070886e-02, ...,\n",
       "           -6.59362078e-02, -7.34513551e-02, -5.72695844e-02]],\n",
       " \n",
       "         [[-3.98309305e-02,  8.71762261e-02,  7.87379369e-02, ...,\n",
       "            5.72929271e-02, -3.33145484e-02,  4.68951948e-02],\n",
       "          [-9.62877087e-03, -4.20282930e-02, -2.96742022e-02, ...,\n",
       "           -6.34427518e-02, -2.47509833e-02, -3.92662734e-02],\n",
       "          [-8.38883873e-03, -3.72450054e-02, -1.23231485e-01, ...,\n",
       "           -5.09795807e-02, -4.32722159e-02, -1.11031532e-01],\n",
       "          ...,\n",
       "          [ 2.86855400e-02, -3.57968360e-02, -1.48597851e-01, ...,\n",
       "           -8.47484767e-02, -3.17605771e-02, -1.03341728e-01],\n",
       "          [-3.55495773e-02, -2.23315158e-03, -3.33369300e-02, ...,\n",
       "            2.85301078e-02, -2.94246562e-02, -1.44768238e-01],\n",
       "          [-5.87467961e-02,  1.21808946e-02, -1.13571189e-01, ...,\n",
       "           -3.30257639e-02,  4.73394319e-02, -3.54585727e-03]],\n",
       " \n",
       "         [[-1.44605208e-02, -6.37579784e-02, -2.72927713e-02, ...,\n",
       "           -5.93043044e-02,  4.95610386e-02,  8.60899538e-02],\n",
       "          [-2.36538369e-02, -1.29747361e-01, -4.15168442e-02, ...,\n",
       "            1.55607332e-02, -1.76090673e-02, -5.38386367e-02],\n",
       "          [-1.85981970e-02, -3.75519060e-02, -8.21507052e-02, ...,\n",
       "           -1.48951367e-01, -1.85229741e-02,  4.59279157e-02],\n",
       "          ...,\n",
       "          [-3.86294886e-03, -8.10896307e-02, -7.59081766e-02, ...,\n",
       "           -6.15342967e-02, -7.29218274e-02, -7.22575113e-02],\n",
       "          [ 3.12402286e-02, -2.10033190e-02,  4.35182787e-02, ...,\n",
       "            1.73121914e-02, -1.73444476e-03, -5.25558963e-02],\n",
       "          [-5.53570390e-02,  1.38504431e-01, -3.05690914e-02, ...,\n",
       "           -2.08876040e-02,  5.07709645e-02,  1.04390033e-01]]],\n",
       " \n",
       " \n",
       "        [[[-7.25252274e-03, -2.97839139e-02,  3.06959040e-02, ...,\n",
       "            2.96616405e-02,  9.40844417e-03,  8.01889151e-02],\n",
       "          [-5.09623364e-02, -9.42019150e-02,  2.35821195e-02, ...,\n",
       "           -1.56192610e-03, -5.30244932e-02, -2.22270396e-02],\n",
       "          [ 2.83789914e-02, -1.06382765e-01, -8.03584531e-02, ...,\n",
       "           -3.81407104e-02, -6.11991324e-02, -8.46590251e-02],\n",
       "          ...,\n",
       "          [-4.15732004e-02, -1.68099552e-01, -7.51353754e-03, ...,\n",
       "           -1.12254536e-02, -3.86654772e-03, -3.58514450e-02],\n",
       "          [ 8.03102681e-04,  4.58383150e-02,  8.72874185e-02, ...,\n",
       "            1.10516340e-01,  4.15814184e-02, -1.63859710e-01],\n",
       "          [-4.31494527e-02, -4.09775274e-03,  3.84809189e-02, ...,\n",
       "           -8.68666396e-02,  1.18768476e-01,  1.39056798e-02]],\n",
       " \n",
       "         [[-1.01849094e-01,  7.93561190e-02, -3.31691466e-03, ...,\n",
       "           -8.58353078e-02, -1.07180914e-02,  1.75672192e-02],\n",
       "          [-3.15993056e-02,  2.36653276e-02,  1.71018187e-02, ...,\n",
       "           -4.00082916e-02, -1.49884140e-02, -2.75838412e-02],\n",
       "          [-7.45692179e-02, -1.77745014e-01, -3.94156165e-02, ...,\n",
       "           -3.18277813e-02, -3.88243087e-02, -1.76054716e-01],\n",
       "          ...,\n",
       "          [-2.51181666e-02, -6.74809441e-02, -1.24583982e-01, ...,\n",
       "           -8.97439793e-02,  3.41049209e-02, -5.31890653e-02],\n",
       "          [-2.33919732e-02, -1.21026978e-01,  3.09325065e-02, ...,\n",
       "           -3.25744972e-02, -1.55528663e-02, -9.78165120e-02],\n",
       "          [ 7.77563602e-02, -3.14787147e-03, -1.32477850e-01, ...,\n",
       "           -2.41203755e-02, -4.03114483e-02, -2.79656798e-02]],\n",
       " \n",
       "         [[-5.05652539e-02,  3.74426506e-02,  9.05382168e-03, ...,\n",
       "           -3.06422450e-02,  2.49915384e-02,  1.26179010e-02],\n",
       "          [-3.58155719e-03, -5.26137799e-02, -1.12497665e-01, ...,\n",
       "           -2.88185254e-02,  1.48354238e-02,  8.37550778e-03],\n",
       "          [ 2.01501790e-02, -2.32537724e-02, -4.57747988e-02, ...,\n",
       "           -6.21450879e-02, -1.03297219e-01, -1.77189335e-01],\n",
       "          ...,\n",
       "          [ 1.29129037e-01, -7.97126591e-02,  9.69093945e-03, ...,\n",
       "           -4.24279794e-02,  4.91228327e-02, -7.32845142e-02],\n",
       "          [-4.89205495e-03, -5.28262043e-03, -1.15253618e-02, ...,\n",
       "            1.44008696e-01,  1.13268368e-01,  9.91218258e-03],\n",
       "          [ 6.23465478e-02, -7.46931955e-02, -4.33376916e-02, ...,\n",
       "           -7.01204538e-02, -4.04448099e-02,  1.45271020e-02]],\n",
       " \n",
       "         [[ 8.46237317e-02, -1.78535338e-02,  5.99033833e-02, ...,\n",
       "           -3.05969752e-02, -2.96866950e-02,  3.08695110e-03],\n",
       "          [-6.20344989e-02, -2.07651518e-02, -1.40987635e-01, ...,\n",
       "           -9.24620852e-02, -4.55400608e-02,  1.28260860e-02],\n",
       "          [ 7.29291290e-02, -8.48843977e-02,  1.91336740e-02, ...,\n",
       "           -4.09571603e-02, -3.10938284e-02, -1.62003674e-02],\n",
       "          ...,\n",
       "          [ 2.10519973e-02, -1.60820819e-02,  1.67588680e-03, ...,\n",
       "           -9.22714770e-02, -1.20610498e-01, -6.07333109e-02],\n",
       "          [-2.59175245e-02, -1.65415496e-01,  1.27334967e-01, ...,\n",
       "            4.74947952e-02, -8.93804803e-02, -7.38159269e-02],\n",
       "          [-9.37811956e-02, -1.47281721e-01, -4.48086374e-02, ...,\n",
       "            4.83572371e-02,  7.75106326e-02, -2.93182768e-03]],\n",
       " \n",
       "         [[-7.11072832e-02, -1.77977346e-02,  3.11274510e-02, ...,\n",
       "            1.68497283e-02,  6.69690967e-03, -4.41596434e-02],\n",
       "          [-1.55706694e-02, -1.32490629e-02,  6.23204112e-02, ...,\n",
       "            3.28265619e-03, -9.10132453e-02, -2.27117934e-03],\n",
       "          [-5.31514250e-02,  5.32421246e-02, -9.74612013e-02, ...,\n",
       "           -1.78367924e-02, -5.91993108e-02,  2.68937554e-02],\n",
       "          ...,\n",
       "          [-1.99056622e-02, -1.90714486e-02, -2.58158725e-02, ...,\n",
       "           -7.24047422e-02, -4.75934148e-03, -7.02357069e-02],\n",
       "          [-8.00544173e-02, -2.65072137e-02,  5.90024740e-02, ...,\n",
       "           -2.25158725e-02, -9.17669237e-02, -1.71283409e-02],\n",
       "          [-8.31471756e-02, -4.07155864e-02, -4.91293930e-02, ...,\n",
       "            4.15490717e-02,  7.51166269e-02,  4.58200164e-02]]]],\n",
       "       dtype=float32)>,\n",
       " <tf.Variable 'hybrid_model/conv2d_1/bias:0' shape=(64,) dtype=float32, numpy=\n",
       " array([-0.07650089, -0.04868462, -0.08660688, -0.09863301, -0.0403885 ,\n",
       "        -0.08661521, -0.02627097, -0.01688926, -0.08434399,  0.00013662,\n",
       "        -0.03896813, -0.08003557, -0.07313508, -0.03299552, -0.06993809,\n",
       "        -0.04635326, -0.06271584, -0.06546568, -0.12099931, -0.06120215,\n",
       "        -0.06829292, -0.08563919, -0.04855213, -0.09140653, -0.07407222,\n",
       "        -0.01042198, -0.11881701, -0.06232876, -0.05968145, -0.0749795 ,\n",
       "        -0.10213227, -0.0365685 , -0.10255515, -0.08806495, -0.06298633,\n",
       "        -0.07002247, -0.00312454, -0.02229505, -0.08470831, -0.02357928,\n",
       "        -0.0195094 , -0.05410148, -0.05850514,  0.01618859, -0.06413887,\n",
       "        -0.05408771, -0.11502121, -0.07816375, -0.09080601, -0.09512917,\n",
       "        -0.0952202 , -0.09025197, -0.03283621, -0.09040923, -0.03292564,\n",
       "        -0.0619041 , -0.0460606 , -0.09006973, -0.05100092, -0.07195891,\n",
       "        -0.01959544, -0.09421758, -0.08194685, -0.09389275], dtype=float32)>,\n",
       " <tf.Variable 'hybrid_model/dense/kernel:0' shape=(33885, 36) dtype=float32, numpy=\n",
       " array([[-0.05120369,  0.01114901,  0.08784641, ..., -0.05435285,\n",
       "          0.02432481,  0.10942683],\n",
       "        [ 0.07082868,  0.10057291, -0.1340387 , ..., -0.09108943,\n",
       "         -0.02181953,  0.00539622],\n",
       "        [-0.0138909 , -0.10338986, -0.08628289, ...,  0.03957371,\n",
       "          0.08186345,  0.02478349],\n",
       "        ...,\n",
       "        [-0.11978644, -0.5023168 ,  0.39659768, ..., -0.7058263 ,\n",
       "         -0.6780899 , -0.34996164],\n",
       "        [-0.5448075 , -1.5315242 ,  0.9173926 , ..., -1.9359573 ,\n",
       "         -2.2003756 , -0.8254116 ],\n",
       "        [ 0.19153373,  0.44154912, -0.07406721, ..., -0.36887637,\n",
       "          0.1887306 ,  0.00473666]], dtype=float32)>,\n",
       " <tf.Variable 'hybrid_model/dense/bias:0' shape=(36,) dtype=float32, numpy=\n",
       " array([-0.5222198 , -0.31408295, -0.20213628, -0.8675961 ,  0.63761514,\n",
       "        -0.40651026,  0.00651906,  0.57426393, -0.09630975, -0.50236696,\n",
       "        -0.13574316,  0.14029893, -0.34264728, -0.01471308,  0.14516516,\n",
       "        -0.9414429 ,  0.02497836, -0.35678187, -0.37642813,  0.3660446 ,\n",
       "        -0.30562997, -0.25198427, -0.07065959,  0.56535804,  0.88908046,\n",
       "         0.4289234 ,  0.15160023, -0.17493255,  0.16241013,  0.30447844,\n",
       "        -0.29109505,  0.18722121,  0.5356437 , -1.0272295 , -0.39083362,\n",
       "        -0.30094537], dtype=float32)>,\n",
       " <tf.Variable 'hybrid_model/dense_1/kernel:0' shape=(36, 36) dtype=float32, numpy=\n",
       " array([[ 0.09013706, -0.10198941, -0.03028517, ...,  0.09072016,\n",
       "         -0.00347504,  0.01897454],\n",
       "        [ 0.03375793, -0.18171526, -0.07429837, ...,  0.01982632,\n",
       "          0.25564215,  0.02171252],\n",
       "        [ 0.02055381, -0.24386503,  0.03127381, ...,  0.00151259,\n",
       "          0.01078643,  0.01831144],\n",
       "        ...,\n",
       "        [ 0.06652158, -0.66892433, -0.02588817, ..., -0.00960598,\n",
       "          0.11629474,  0.11744542],\n",
       "        [ 0.00225906,  0.03516334,  0.0584405 , ...,  0.1776728 ,\n",
       "          0.24231231, -0.00898965],\n",
       "        [ 0.00418316, -0.05145825,  0.11054669, ...,  0.22602078,\n",
       "          0.14172156,  0.08463123]], dtype=float32)>,\n",
       " <tf.Variable 'hybrid_model/dense_1/bias:0' shape=(36,) dtype=float32, numpy=\n",
       " array([-0.11432936,  0.6768216 , -0.5833875 , -0.6188218 ,  0.30853668,\n",
       "        -0.15285341, -0.29115924, -0.35466024, -0.39459047, -0.4480726 ,\n",
       "         0.4778072 ,  0.02810386, -0.04669596, -0.02377507,  0.31097603,\n",
       "        -0.11823986, -0.14022675,  0.5432828 , -0.11123982, -0.06707212,\n",
       "        -0.3097132 , -0.909822  , -0.46353716, -0.4343704 , -0.11186122,\n",
       "         0.08829147, -0.41443086, -0.4635193 ,  0.8858518 , -0.002661  ,\n",
       "        -0.38596123, -0.11086298, -0.36804098, -0.8569047 , -0.5571411 ,\n",
       "        -0.18399812], dtype=float32)>,\n",
       " <tf.Variable 'hybrid_model/dense_2/kernel:0' shape=(36, 1) dtype=float32, numpy=\n",
       " array([[-0.00433611],\n",
       "        [-0.12654008],\n",
       "        [-0.00627023],\n",
       "        [ 0.00199472],\n",
       "        [ 0.05946863],\n",
       "        [ 0.00657374],\n",
       "        [ 0.00771001],\n",
       "        [ 0.0068281 ],\n",
       "        [-0.04043735],\n",
       "        [ 0.02467226],\n",
       "        [-0.12754156],\n",
       "        [-0.01196769],\n",
       "        [-0.01930798],\n",
       "        [-0.00030132],\n",
       "        [-0.11230536],\n",
       "        [ 0.0495398 ],\n",
       "        [ 0.09610768],\n",
       "        [-0.05226199],\n",
       "        [ 0.12833275],\n",
       "        [-0.03387545],\n",
       "        [-0.0137278 ],\n",
       "        [ 0.07467341],\n",
       "        [-0.00037005],\n",
       "        [ 0.03547451],\n",
       "        [ 0.03956771],\n",
       "        [-0.03934124],\n",
       "        [-0.0555947 ],\n",
       "        [ 0.03839656],\n",
       "        [-0.13056444],\n",
       "        [ 0.01087565],\n",
       "        [ 0.00099642],\n",
       "        [-0.01173701],\n",
       "        [-0.09616131],\n",
       "        [ 0.06085362],\n",
       "        [-0.00398409],\n",
       "        [ 0.03634429]], dtype=float32)>,\n",
       " <tf.Variable 'hybrid_model/dense_2/bias:0' shape=(1,) dtype=float32, numpy=array([-0.5262495], dtype=float32)>]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Examine the weights objects\n",
    "model.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"hybrid_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             multiple                  2432      \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           multiple                  51264     \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  multiple                  0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " flatten (Flatten)           multiple                  0         \n",
      "                                                                 \n",
      " dense (Dense)               multiple                  1219896   \n",
      "                                                                 \n",
      " dropout (Dropout)           multiple                  0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             multiple                  1332      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         multiple                  0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             multiple                  37        \n",
      "                                                                 \n",
      " concatenate (Concatenate)   multiple                  0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1274961 (4.86 MB)\n",
      "Trainable params: 1274961 (4.86 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try importing hdf5 and json for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimported_weights = model.load_weights(savePath + \"Model.weights.h5\", skip_mismatch=False)\\n\\n#Taken from https://machinelearningmastery.com/save-load-keras-deep-learning-models/\\n# load json and create model\\njson_file = open(savePath + \\'model.json\\', \\'r\\')\\nloaded_model_json = json_file.read()\\njson_file.close()\\n#loaded_model = tf.keras.models.model_from_json(loaded_model_json)\\n# load weights into new model\\n#loaded_model.load_weights(\"model.h5\")\\n#print(\"Loaded model from disk\")\\n'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "imported_weights = model.load_weights(savePath + \"Model.weights.h5\", skip_mismatch=False)\n",
    "\n",
    "#Taken from https://machinelearningmastery.com/save-load-keras-deep-learning-models/\n",
    "# load json and create model\n",
    "json_file = open(savePath + 'model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "#loaded_model = tf.keras.models.model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "#loaded_model.load_weights(\"model.h5\")\n",
    "#print(\"Loaded model from disk\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5 - Plot the losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 2.0)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plot the training and validation losses\n",
    "\n",
    "#Convert loss results into a dataframe\n",
    "result_preproc = pd.DataFrame({\n",
    "    'Epoch': [i+1 for i in range(len(results[\"loss\"]))], \n",
    "    'Train': results[\"loss\"],\n",
    "    'Validate': results[\"val_loss\"]\n",
    "    })\n",
    "\n",
    "# Convert dataframe from wide to long format\n",
    "df = pd.melt(result_preproc, ['Epoch'])\n",
    "\n",
    "#Make plot\n",
    "g = sns.lineplot(data=df, x='Epoch', y='value', hue='variable')\n",
    "g.set_title(\"Loss Curves\")\n",
    "g.legend_.set_title(\"Loss\")\n",
    "g.set_ylabel('Loss')\n",
    "g.set_ylim(0,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9) Predict Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simple simple function to ititialize the test dataset using global variables\n",
    "def init_test_dataset():\n",
    "    return make_dataset(hdf5_file, test_meta_dict, test_pos_isic_id, test_pos_target, batch_size = test_batch_size, apply_hair_removal=apply_hair_removal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total test batches in dataset: 75\n"
     ]
    }
   ],
   "source": [
    "#Test dataset basic size information: nb of samples and batch size\n",
    "nb_test_batches = int(np.ceil(len(test_meta_dict)/test_batch_size))\n",
    "print(\"Total test batches in dataset:\", nb_test_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-25 11:52:15.459815: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "#Retrieve real values for the target in the test dataset\n",
    "y_test = []\n",
    "test_dataset = init_test_dataset()\n",
    "for item in test_dataset.take(nb_test_batches):\n",
    "    img_meta, targ = item\n",
    "    y_test.extend(targ.numpy().flatten())\n",
    "#Convert to numpy array (mathematical operations are faster)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75/75 [==============================] - 7s 94ms/step\n",
      "Shape of prediction data: (2371, 1)\n"
     ]
    }
   ],
   "source": [
    "#Reinitialize the test dataset (necessary to start at beginning)\n",
    "test_dataset = init_test_dataset()\n",
    "#Retrieve predictions\n",
    "predictions = model.predict(test_dataset, steps = nb_test_batches)\n",
    "#Put predictionsin a numpy array\n",
    "y_pred = np.array([round(i) for i  in predictions.flatten()])\n",
    "print(\"Shape of prediction data:\", predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#Import results from file\\nimported_test_results = pd.read_csv(testResPath)\\ny_test = np.array(imported_test_results[\"y_test\"])\\ny_pred = np.array(imported_test_results[\"y_pred\"])\\ndel imported_test_results\\n'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Save test results to file\n",
    "pd.DataFrame({\"y_test\": y_test, \"y_pred\": y_pred}).to_csv(testResPath, index=False)\n",
    "\n",
    "\"\"\"\n",
    "#Import results from file\n",
    "imported_test_results = pd.read_csv(testResPath)\n",
    "y_test = np.array(imported_test_results[\"y_test\"])\n",
    "y_pred = np.array(imported_test_results[\"y_pred\"])\n",
    "del imported_test_results\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the loss\n",
    "loss = sum(abs(y_test - y_pred))/len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Determine true/false positives and negatives\n",
    "pos_indices = y_test == 1\n",
    "neg_indices = y_test == 0\n",
    "\n",
    "#True positives\n",
    "true_pos = sum(abs(y_test[pos_indices] == y_pred[pos_indices]))\n",
    "\n",
    "#False negatives\n",
    "false_neg = sum(abs(y_test[pos_indices] != y_pred[pos_indices]))\n",
    "\n",
    "#True negatives\n",
    "true_neg = sum(abs(y_test[neg_indices] == y_pred[neg_indices]))\n",
    "\n",
    "#False positives\n",
    "false_pos = sum(abs(y_test[neg_indices] != y_pred[neg_indices]))\n",
    "\n",
    "#Precision\n",
    "try:\n",
    "    precision = true_pos / (true_pos + false_pos)\n",
    "except:\n",
    "    precision = np.nan\n",
    "\n",
    "#Recall (sensitivity)\n",
    "try:\n",
    "    recall = true_pos / (true_pos + false_neg)\n",
    "except:\n",
    "    recall = np.nan\n",
    "\n",
    "#Specificity\n",
    "try:\n",
    "    specificity = true_neg / (true_neg + false_pos)\n",
    "except:\n",
    "    specificity = np.nan\n",
    "\n",
    "#F1 Score\n",
    "try:\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "except:\n",
    "    f1_score = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---TEST RESULTS---\n",
      "True positives: 5\n",
      "False positives: 638\n",
      "True negatives: 1728\n",
      "False negatives: 0\n",
      "\n",
      "Sensitivity: 1.0\n",
      "Specificity: 0.7303465765004227\n",
      "\n",
      "Precision: 0.007776049766718507\n",
      "Recall: 1.0\n",
      "\n",
      "F1 Score: 0.0154320987654321\n",
      "Loss on test data: 0.2690847743568115\n"
     ]
    }
   ],
   "source": [
    "print(\"---TEST RESULTS---\")\n",
    "print(\"True positives:\", true_pos)\n",
    "print(\"False positives:\", false_pos)\n",
    "print(\"True negatives:\", true_neg)\n",
    "print(\"False negatives:\", false_neg)\n",
    "print()\n",
    "print(\"Sensitivity:\", recall)\n",
    "print(\"Specificity:\", specificity)\n",
    "print()\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print()\n",
    "print(\"F1 Score:\", f1_score)\n",
    "print(\"Loss on test data:\", loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p310",
   "language": "python",
   "name": "conda_tensorflow2_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL - IMAGE LOADING & NEURAL NETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "import csv\n",
    "import os\n",
    "import io\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) GENERAL FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to show image\n",
    "def show_img(image):\n",
    "    plt.imshow(image, interpolation=None)\n",
    "    plt.grid(None)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Image cropping\n",
    "def crop_image(images_list, nbPix = 100):\n",
    "    output_images = []\n",
    "    for image in images_list:\n",
    "        #Height adjustments\n",
    "        h = len(image)\n",
    "        adj = len(image) - nbPix\n",
    "        h1 = round(adj / 2) #Top\n",
    "        h2 = h - (adj - h1) #Bottom\n",
    "\n",
    "        #Width adjustments\n",
    "        w = len(image[0])\n",
    "        w_adj = w - nbPix\n",
    "        w1 = round(w_adj / 2) #Left\n",
    "        w2 = w - (w_adj - w1) #Right\n",
    "\n",
    "        img = image[h1:h2,w1:w2]\n",
    "        output_images.append(img)\n",
    "        \n",
    "    return np.array(output_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) IMPORT DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 - Declare file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#General file paths\n",
    "projectDir = os.getcwd() + \"/\"\n",
    "parentDir = os.path.abspath(os.path.join(projectDir, os.pardir)) + \"/\"\n",
    "dataPath = os.path.abspath(os.path.join(projectDir, os.pardir)) + \"/isic-2024-challenge/\"\n",
    "\n",
    "#Metadata file paths\n",
    "#metaPath = dataPath + \"train-metadata.csv\"\n",
    "metaPath = dataPath + \"sample-metadata.csv\"\n",
    "\n",
    "#Image file path\n",
    "#hdf5_file = dataPath + \"train-image.hdf5\"\n",
    "hdf5_file = dataPath + \"sample-image.hdf5\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 - Load metadata from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import metadata\n",
    "metadata = pd.read_csv(metaPath, sep=\",\")\n",
    "\n",
    "#METADATA: color and size features having no NAs\n",
    "metadata = metadata[[\"isic_id\",\n",
    "                     \"target\",\n",
    "                     \"clin_size_long_diam_mm\",\n",
    "                     \"tbp_lv_areaMM2\",\n",
    "                     \"tbp_lv_area_perim_ratio\",\n",
    "                     \"tbp_lv_eccentricity\",\n",
    "                     \"tbp_lv_minorAxisMM\",\n",
    "                     \"tbp_lv_color_std_mean\",\n",
    "                     \"tbp_lv_deltaLBnorm\",\n",
    "                     \"tbp_lv_radial_color_std_max\"]]\n",
    "\n",
    "#Verify that there are no NAs\n",
    "print(\"-- X_meta NA counts --\")\n",
    "print(metadata.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 - Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 - Train, Validate, Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to perform train-validate or train-test-validate split on a list of isic_ids\n",
    "def ttv_split(isic_ids, test_frac=0.2, validate_frac=0.2, random_state=88, shuffle=True, stratify=None):\n",
    "    if test_frac < 0 or validate_frac < 0:\n",
    "        print(\"ERROR: Test of validate fraction is negative\")\n",
    "        return None\n",
    "    if test_frac > 1 or validate_frac > 1:\n",
    "        print(\"ERROR: Test of validate fraction is above 0\")\n",
    "        return None\n",
    "    if test_frac + validate_frac >= 1:\n",
    "        print(\"ERROR: Test and validate fractions sum to 1 or more.\")\n",
    "        return None\n",
    "\n",
    "    #Split training from the rest\n",
    "    test_size = test_frac + validate_frac\n",
    "    train, temp = train_test_split(isic_ids, test_size = test_size, random_state=random_state, shuffle=shuffle, stratify=stratify)\n",
    "    #Split test and validate\n",
    "    if test_frac == 0 or validate_frac == 0:\n",
    "        return train.tolist(), temp.tolist()\n",
    "    else:\n",
    "        test_size = test_frac / (test_frac + validate_frac)\n",
    "        test, validate = train_test_split(temp, test_size = test_size, random_state=random_state, shuffle=shuffle, stratify=stratify)\n",
    "        return train.tolist(), test.tolist(), validate.tolist()\n",
    "\n",
    "#Generate the splits of the isic_ids\n",
    "train_ids, test_ids, val_ids = ttv_split(metadata[\"isic_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split train-test-validate portions of metadata into X and y\n",
    "def Xy_split(metadata, ids):\n",
    "    #Generate X\n",
    "    X = metadata[metadata[\"isic_id\"].isin(ids)]\n",
    "    X.loc[:, ~X.columns.isin(['isic_id', 'target'])]\n",
    "    #Generate y\n",
    "    y = metadata[metadata[\"isic_id\"].isin(ids)][\"target\"]\n",
    "    return X, y\n",
    "\n",
    "X_train, y_train = Xy_split(metadata, train_ids)\n",
    "X_test, y_test = Xy_split(metadata, test_ids)\n",
    "X_validate, y_validate = Xy_split(metadata, val_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 - Load images and create hybrid tensorflow dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OLD WORKING VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GENERATOR FOR HDF5\n",
    "#Generates the image (standardized). Avoids multiple file open/read/close operations.\n",
    "#file: full path for file\n",
    "#imgs: list of images to load\n",
    "#imgSize: number of pixels for size/resolution adjustment in square form\n",
    "class hdf5_generator:\n",
    "    def __init__(self, file, img_names, imgSize):\n",
    "        self.file = file\n",
    "        self.img_names = img_names\n",
    "        self.imgSize = imgSize\n",
    "    def __call__(self):\n",
    "        with h5py.File(self.file, 'r') as h5file:\n",
    "            for img in self.img_names:\n",
    "                img = np.array(Image.open(io.BytesIO(h5file[img][()])))\n",
    "                img = tf.image.resize(img, [self.imgSize, self.imgSize])\n",
    "                img = tf.constant(np.reshape(img/255,(1,100,100,3)), dtype=tf.float32) #standardized here\n",
    "                yield img\n",
    "\n",
    "#DATASET FUNCTION\n",
    "def make_dataset(hdf5_file, meta, target, img_names, imgSize=100):\n",
    "    #Generate image dataset\n",
    "    img_dataset = tf.data.Dataset.from_generator(\n",
    "        hdf5_generator(hdf5_file, img_names, imgSize),\n",
    "        output_types=tf.float32,\n",
    "        output_shapes = tf.TensorShape([1, imgSize,imgSize,3])\n",
    "        )\n",
    "\n",
    "    #Generate target dataset\n",
    "    target = [np.reshape(element, (1,1)) for element in target]\n",
    "    target = tf.cast(target, dtype=tf.int32)\n",
    "    target = tf.data.Dataset.from_tensor_slices(target, name = \"target\")\n",
    "\n",
    "    #Generate metadata set\n",
    "    meta = tf.cast(meta, dtype=tf.float32)\n",
    "    meta = tf.reshape(meta, shape=(9,1,8))\n",
    "    meta = tf.data.Dataset.from_tensor_slices(meta, name = \"metadata\")\n",
    "\n",
    "    #Combine datasets into one\n",
    "    dataset = tf.data.Dataset.zip((img_dataset, meta))\n",
    "    dataset = tf.data.Dataset.zip((dataset, target))\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make datasets\n",
    "train_dataset = make_dataset(hdf5_file, X_train, y_train, train_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Delete duplicate data, keeping only the dataset\n",
    "#del metadata\n",
    "#del y_train\n",
    "#del X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Examine the dataset objects\n",
    "print(train_dataset, \"\\n\")\n",
    "\n",
    "iterator = iter(train_dataset)\n",
    "#print(iterator.get_next())\n",
    "print(next(iterator))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MARTIAL'S VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HDF5 Image Data Generator with Augmentation and Standardization\n",
    "class hdf5_generator:\n",
    "    def __init__(self, file, img_names, imgSize):\n",
    "        self.file = file\n",
    "        self.img_names = img_names\n",
    "        self.imgSize = imgSize\n",
    "\n",
    "    def __call__(self):\n",
    "        with h5py.File(self.file, 'r') as h5file:\n",
    "            for img_name in self.img_names:\n",
    "                try:\n",
    "                    # Load image data from HDF5\n",
    "                    img = np.array(Image.open(io.BytesIO(h5file[img_name][()])))\n",
    "                    \n",
    "                    # Resize the image\n",
    "                    img = tf.image.resize(img, [self.imgSize, self.imgSize])\n",
    "                    \n",
    "                    # Data Augmentation \n",
    "                    img = tf.image.random_flip_left_right(img)\n",
    "                    img = tf.image.random_flip_up_down(img)\n",
    "                    img = tf.image.random_brightness(img, max_delta=0.2)\n",
    "\n",
    "                    # Standardize and return as TensorFlow constant\n",
    "                    img = tf.constant(img / 255, dtype=tf.float32)  # Standardize here\n",
    "               \n",
    "                    \n",
    "                    yield img\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading image {img_name}: {e}\")\n",
    "                    # log the error to a file for later analysis\n",
    "                    with open('image_errors.log', 'a') as f:\n",
    "                        f.write(f\"Error loading image {img_name}: {e}\\n\")\n",
    "                    continue\n",
    "\n",
    "#Generate the dataset with batch size and prefetching\n",
    "def make_dataset(hdf5_file, meta, target, img_names, imgSize=100, batch_size=5):\n",
    "    # Generate image dataset\n",
    "    img_dataset = tf.data.Dataset.from_generator(\n",
    "        hdf5_generator(hdf5_file, img_names, imgSize),\n",
    "        output_types=tf.float32,\n",
    "        output_shapes=tf.TensorShape([imgSize, imgSize, 3])\n",
    "    )\n",
    "\n",
    "    # Generate target dataset\n",
    "    target = [np.reshape(element, (1, 1)) for element in target]\n",
    "    target = tf.cast(target, dtype=tf.int32)\n",
    "    target = tf.data.Dataset.from_tensor_slices(target, name=\"target\")\n",
    "\n",
    "    # Generate metadata set\n",
    "    meta = tf.cast(meta, dtype=tf.float32)\n",
    "    meta = tf.reshape(meta, shape=(9,1,8))\n",
    "    meta = tf.data.Dataset.from_tensor_slices(meta, name=\"metadata\")\n",
    "\n",
    "    # Combine datasets into one\n",
    "    dataset = tf.data.Dataset.zip((img_dataset, meta))\n",
    "    dataset = tf.data.Dataset.zip((dataset, target))\n",
    "\n",
    "    # Add shuffling, batching, and prefetching\n",
    "    dataset = dataset.shuffle(buffer_size=min(len(img_names), 10000)).batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make datasets\n",
    "train_dataset = make_dataset(hdf5_file, X_train, y_train, train_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) CNN MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 - Model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simple CNN model using only images and target\n",
    "class CNN_model(tf.keras.Model):\n",
    "    def __init__(self, neurons = 8, activ = 'tanh'):\n",
    "        super().__init__()\n",
    "        self.conv1 = tf.keras.layers.Conv2D(filters=16, kernel_size=5, strides=(1, 1), activation='relu', padding='same', input_shape=(100, 100, 3))\n",
    "        self.pool1 = tf.keras.layers.MaxPool2D(pool_size=(2,2))\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.dense1 = tf.keras.layers.Dense(neurons, activation = activ)\n",
    "        self.dense2 = tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x_image, x_meta = inputs\n",
    "\n",
    "        # Convolutions\n",
    "        x1 = self.conv1(x_image)\n",
    "        x1 = self.pool1(x1)\n",
    "\n",
    "        # Flattening of images for input layer\n",
    "        x1 = self.flatten(x1)\n",
    "\n",
    "        # Hidden layers of neural network\n",
    "        x1 = self.dense1(x1)\n",
    "\n",
    "        # Output layer of neural network\n",
    "        output = self.dense2(x1)\n",
    "\n",
    "        return output\n",
    "\n",
    "#Hybrid CNN model taking metadata\n",
    "class Hybrid_model(tf.keras.Model):\n",
    "    def __init__(self, neurons = 8, activ = 'tanh'):\n",
    "        super().__init__()\n",
    "        self.conv1 = tf.keras.layers.Conv2D(filters=16, kernel_size=5, strides=(1, 1), activation='relu', padding='same')\n",
    "        self.conv2 = tf.keras.layers.Conv2D(32, 5, activation='relu')\n",
    "        self.pool = tf.keras.layers.MaxPool2D(pool_size=(2,2))\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.dense1 = tf.keras.layers.Dense(neurons, activation = activ)\n",
    "        self.dense2 = tf.keras.layers.Dense(neurons, activation = activ)\n",
    "        self.dense3 = tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "        #self.dropout = tf.keras.layers.dropout(0.25)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        x_image, x_meta = inputs\n",
    "        # Convolutions\n",
    "        x = self.conv1(x_image)\n",
    "        x = self.pool(x)\n",
    "        #x = self.conv2(x)\n",
    "        #x = self.pool(x)\n",
    "        # Flattening of images and concatenation with other data\n",
    "        x = self.flatten(x)\n",
    "        #x_all = tf.concat([x,x_meta], axis=1)\n",
    "        x_all = keras.layers.Concatenate(axis=1)([x, x_meta])\n",
    "        # Neural Network\n",
    "        x_all = self.dense1(x_all)\n",
    "        #x_all = self.dense2(x_all)\n",
    "        #if training:\n",
    "        #    x_all = self.dropout(x_all, training=training)\n",
    "        output = self.dense3(x_all)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 - Model compiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set seed\n",
    "tf.random.set_seed(71)\n",
    "\n",
    "#Initialize model\n",
    "model = CNN_model(neurons=8, activ='tanh')\n",
    "#model = Hybrid_model(neurons=8, activ='tanh')\n",
    "\n",
    "#Define optimizer and loss function\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "loss = tf.keras.losses.BinaryCrossentropy(from_logits=True,\n",
    "                                          label_smoothing=0.0,\n",
    "                                          axis=-1,\n",
    "                                          reduction='sum_over_batch_size',\n",
    "                                          name='binary_crossentropy')\n",
    "\n",
    "#Compile the model with loss, optimizer, and metrics\n",
    "model.compile(loss = loss,\n",
    "              optimizer = optimizer,\n",
    "              metrics = [\n",
    "                  tf.keras.metrics.BinaryAccuracy(),\n",
    "                  tf.keras.metrics.FalseNegatives(),\n",
    "                  tf.keras.metrics.FalsePositives(),\n",
    "                  tf.keras.metrics.TrueNegatives(),\n",
    "                  tf.keras.metrics.TruePositives()\n",
    "                  ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 - Model fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit the model\n",
    "mod = model.fit(train_dataset, epochs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod.history"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ISIC24_skin_cancer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL - IMAGE LOADING & NEURAL NETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: absl-py==2.1.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 1)) (2.1.0)\n",
      "Collecting alembic==1.13.3 (from -r requirements.txt (line 2))\n",
      "  Using cached alembic-1.13.3-py3-none-any.whl.metadata (7.4 kB)\n",
      "Requirement already satisfied: annotated-types==0.7.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 3)) (0.7.0)\n",
      "Collecting ansible==10.5.0 (from -r requirements.txt (line 4))\n",
      "  Using cached ansible-10.5.0-py3-none-any.whl.metadata (8.0 kB)\n",
      "Collecting ansible-core==2.17.5 (from -r requirements.txt (line 5))\n",
      "  Using cached ansible_core-2.17.5-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: asttokens==2.4.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 6)) (2.4.1)\n",
      "Requirement already satisfied: astunparse==1.6.3 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 7)) (1.6.3)\n",
      "Requirement already satisfied: attrs==23.2.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 8)) (23.2.0)\n",
      "Requirement already satisfied: beautifulsoup4==4.12.3 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 9)) (4.12.3)\n",
      "Requirement already satisfied: promise==2.3 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 10)) (2.3)\n",
      "Requirement already satisfied: setuptools==58.0.4 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 11)) (58.0.4)\n",
      "Requirement already satisfied: bleach==6.1.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 12)) (6.1.0)\n",
      "Requirement already satisfied: blinker==1.8.2 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 13)) (1.8.2)\n",
      "Collecting boto3==1.35.44 (from -r requirements.txt (line 14))\n",
      "  Using cached boto3-1.35.44-py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting botocore==1.35.44 (from -r requirements.txt (line 15))\n",
      "  Using cached botocore-1.35.44-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting cachetools==5.5.0 (from -r requirements.txt (line 16))\n",
      "  Using cached cachetools-5.5.0-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting certifi==2024.8.30 (from -r requirements.txt (line 17))\n",
      "  Using cached certifi-2024.8.30-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting cffi==1.17.1 (from -r requirements.txt (line 18))\n",
      "  Using cached cffi-1.17.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: charset-normalizer==3.3.2 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 19)) (3.3.2)\n",
      "Requirement already satisfied: click==8.1.7 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 20)) (8.1.7)\n",
      "Requirement already satisfied: cloudpickle==2.2.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 21)) (2.2.1)\n",
      "Requirement already satisfied: colorama==0.4.6 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 22)) (0.4.6)\n",
      "Requirement already satisfied: comm==0.2.2 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 23)) (0.2.2)\n",
      "Collecting contourpy==1.3.0 (from -r requirements.txt (line 24))\n",
      "  Using cached contourpy-1.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.4 kB)\n",
      "Collecting cryptography==43.0.3 (from -r requirements.txt (line 25))\n",
      "  Using cached cryptography-43.0.3-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: cycler==0.12.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 26)) (0.12.1)\n",
      "Collecting databricks-sdk==0.35.0 (from -r requirements.txt (line 27))\n",
      "  Using cached databricks_sdk-0.35.0-py3-none-any.whl.metadata (38 kB)\n",
      "Collecting debugpy==1.8.5 (from -r requirements.txt (line 28))\n",
      "  Using cached debugpy-1.8.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: decorator==5.1.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 29)) (5.1.1)\n",
      "Requirement already satisfied: defusedxml==0.7.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 30)) (0.7.1)\n",
      "Collecting Deprecated==1.2.14 (from -r requirements.txt (line 31))\n",
      "  Using cached Deprecated-1.2.14-py2.py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting dill==0.3.9 (from -r requirements.txt (line 32))\n",
      "  Using cached dill-0.3.9-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: dm-tree==0.1.8 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 33)) (0.1.8)\n",
      "Requirement already satisfied: docker==7.1.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 34)) (7.1.0)\n",
      "Requirement already satisfied: docstring_parser==0.16 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 35)) (0.16)\n",
      "Requirement already satisfied: etils==1.10.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 36)) (1.10.0)\n",
      "Collecting executing==2.1.0 (from -r requirements.txt (line 37))\n",
      "  Using cached executing-2.1.0-py2.py3-none-any.whl.metadata (8.9 kB)\n",
      "Requirement already satisfied: fastjsonschema==2.20.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 38)) (2.20.0)\n",
      "Requirement already satisfied: Flask==3.0.3 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 39)) (3.0.3)\n",
      "Requirement already satisfied: flatbuffers==24.3.25 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 40)) (24.3.25)\n",
      "Requirement already satisfied: fonttools==4.53.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 41)) (4.53.1)\n",
      "Collecting fsspec==2024.9.0 (from -r requirements.txt (line 42))\n",
      "  Using cached fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: gast==0.6.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 43)) (0.6.0)\n",
      "Collecting gitdb==4.0.11 (from -r requirements.txt (line 44))\n",
      "  Using cached gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting GitPython==3.1.43 (from -r requirements.txt (line 45))\n",
      "  Using cached GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting google-auth==2.35.0 (from -r requirements.txt (line 46))\n",
      "  Using cached google_auth-2.35.0-py2.py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: google-pasta==0.2.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 47)) (0.2.0)\n",
      "Collecting googleapis-common-protos==1.65.0 (from -r requirements.txt (line 48))\n",
      "  Using cached googleapis_common_protos-1.65.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting graphene==3.4 (from -r requirements.txt (line 49))\n",
      "  Using cached graphene-3.4-py2.py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting graphql-core==3.2.5 (from -r requirements.txt (line 50))\n",
      "  Using cached graphql_core-3.2.5-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting graphql-relay==3.2.0 (from -r requirements.txt (line 51))\n",
      "  Using cached graphql_relay-3.2.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting greenlet==3.1.1 (from -r requirements.txt (line 52))\n",
      "  Using cached greenlet-3.1.1-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting grpcio==1.66.1 (from -r requirements.txt (line 53))\n",
      "  Using cached grpcio-1.66.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: h5py==3.11.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 54)) (3.11.0)\n",
      "Collecting idna==3.10 (from -r requirements.txt (line 55))\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: immutabledict==4.2.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 56)) (4.2.0)\n",
      "Requirement already satisfied: importlib-metadata==6.11.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 57)) (6.11.0)\n",
      "Collecting importlib_resources==6.4.5 (from -r requirements.txt (line 58))\n",
      "  Using cached importlib_resources-6.4.5-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: ipykernel==6.29.5 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 59)) (6.29.5)\n",
      "Collecting ipython==8.27.0 (from -r requirements.txt (line 60))\n",
      "  Using cached ipython-8.27.0-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: itsdangerous==2.2.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 61)) (2.2.0)\n",
      "Collecting jedi==0.19.1 (from -r requirements.txt (line 62))\n",
      "  Using cached jedi-0.19.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Requirement already satisfied: Jinja2==3.1.4 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 63)) (3.1.4)\n",
      "Requirement already satisfied: jmespath==1.0.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 64)) (1.0.1)\n",
      "Requirement already satisfied: joblib==1.4.2 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 65)) (1.4.2)\n",
      "Requirement already satisfied: jsonschema==4.23.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 66)) (4.23.0)\n",
      "Requirement already satisfied: jsonschema-specifications==2023.12.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 67)) (2023.12.1)\n",
      "Collecting jupyter_client==8.6.3 (from -r requirements.txt (line 68))\n",
      "  Using cached jupyter_client-8.6.3-py3-none-any.whl.metadata (8.3 kB)\n",
      "Requirement already satisfied: jupyter_core==5.7.2 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 69)) (5.7.2)\n",
      "Requirement already satisfied: jupyterlab_pygments==0.3.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 70)) (0.3.0)\n",
      "Requirement already satisfied: kagglehub==0.3.3 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 71)) (0.3.3)\n",
      "Collecting keras==3.5.0 (from -r requirements.txt (line 72))\n",
      "  Using cached keras-3.5.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: keras-core==0.1.7 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 73)) (0.1.7)\n",
      "Requirement already satisfied: keras-cv==0.9.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 74)) (0.9.0)\n",
      "Collecting kiwisolver==1.4.7 (from -r requirements.txt (line 75))\n",
      "  Using cached kiwisolver-1.4.7-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: libclang==18.1.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 76)) (18.1.1)\n",
      "Collecting Mako==1.3.5 (from -r requirements.txt (line 77))\n",
      "  Using cached Mako-1.3.5-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting Markdown==3.7 (from -r requirements.txt (line 78))\n",
      "  Using cached Markdown-3.7-py3-none-any.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: markdown-it-py==3.0.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 79)) (3.0.0)\n",
      "Requirement already satisfied: MarkupSafe==2.1.5 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 80)) (2.1.5)\n",
      "Collecting matplotlib==3.8.4 (from -r requirements.txt (line 81))\n",
      "  Using cached matplotlib-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: matplotlib-inline==0.1.7 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 82)) (0.1.7)\n",
      "Requirement already satisfied: mdurl==0.1.2 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 83)) (0.1.2)\n",
      "Requirement already satisfied: mistune==3.0.2 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 84)) (3.0.2)\n",
      "Collecting ml-dtypes==0.4.1 (from -r requirements.txt (line 85))\n",
      "  Using cached ml_dtypes-0.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Collecting mlflow==2.17.0 (from -r requirements.txt (line 86))\n",
      "  Using cached mlflow-2.17.0-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting mlflow-skinny==2.17.0 (from -r requirements.txt (line 87))\n",
      "  Using cached mlflow_skinny-2.17.0-py3-none-any.whl.metadata (30 kB)\n",
      "Requirement already satisfied: mock==4.0.3 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 88)) (4.0.3)\n",
      "Collecting multiprocess==0.70.17 (from -r requirements.txt (line 89))\n",
      "  Using cached multiprocess-0.70.17-py310-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: namex==0.0.8 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 90)) (0.0.8)\n",
      "Requirement already satisfied: nbclient==0.10.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 91)) (0.10.0)\n",
      "Requirement already satisfied: nbconvert==7.16.4 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 92)) (7.16.4)\n",
      "Requirement already satisfied: nbformat==5.10.4 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 93)) (5.10.4)\n",
      "Requirement already satisfied: nest-asyncio==1.6.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 94)) (1.6.0)\n",
      "Requirement already satisfied: numpy==1.26.4 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 95)) (1.26.4)\n",
      "Requirement already satisfied: opencv-python==4.10.0.84 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 96)) (4.10.0.84)\n",
      "Collecting opentelemetry-api==1.27.0 (from -r requirements.txt (line 97))\n",
      "  Using cached opentelemetry_api-1.27.0-py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting opentelemetry-sdk==1.27.0 (from -r requirements.txt (line 98))\n",
      "  Using cached opentelemetry_sdk-1.27.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.48b0 (from -r requirements.txt (line 99))\n",
      "  Using cached opentelemetry_semantic_conventions-0.48b0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: opt-einsum==3.3.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 100)) (3.3.0)\n",
      "Requirement already satisfied: optree==0.12.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 101)) (0.12.1)\n",
      "Collecting packaging==24.1 (from -r requirements.txt (line 102))\n",
      "  Using cached packaging-24.1-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting pandas==2.2.2 (from -r requirements.txt (line 103))\n",
      "  Using cached pandas-2.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
      "Collecting pandocfilters==1.5.1 (from -r requirements.txt (line 104))\n",
      "  Using cached pandocfilters-1.5.1-py2.py3-none-any.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: parso==0.8.4 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 105)) (0.8.4)\n",
      "Collecting pathos==0.3.3 (from -r requirements.txt (line 106))\n",
      "  Using cached pathos-0.3.3-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: pillow==10.4.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 107)) (10.4.0)\n",
      "Collecting platformdirs==4.3.6 (from -r requirements.txt (line 108))\n",
      "  Using cached platformdirs-4.3.6-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting pox==0.3.5 (from -r requirements.txt (line 109))\n",
      "  Using cached pox-0.3.5-py3-none-any.whl.metadata (8.0 kB)\n",
      "Collecting ppft==1.7.6.9 (from -r requirements.txt (line 110))\n",
      "  Using cached ppft-1.7.6.9-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: prompt_toolkit==3.0.47 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 111)) (3.0.47)\n",
      "Collecting protobuf==4.25.5 (from -r requirements.txt (line 112))\n",
      "  Using cached protobuf-4.25.5-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
      "Requirement already satisfied: psutil==6.0.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 113)) (6.0.0)\n",
      "Requirement already satisfied: pure_eval==0.2.3 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 114)) (0.2.3)\n",
      "Requirement already satisfied: pyarrow==17.0.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 115)) (17.0.0)\n",
      "Collecting pyasn1==0.6.1 (from -r requirements.txt (line 116))\n",
      "  Using cached pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting pyasn1_modules==0.4.1 (from -r requirements.txt (line 117))\n",
      "  Using cached pyasn1_modules-0.4.1-py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: pycparser==2.22 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 118)) (2.22)\n",
      "Collecting pydantic==2.9.2 (from -r requirements.txt (line 119))\n",
      "  Using cached pydantic-2.9.2-py3-none-any.whl.metadata (149 kB)\n",
      "Collecting pydantic_core==2.23.4 (from -r requirements.txt (line 120))\n",
      "  Using cached pydantic_core-2.23.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: Pygments==2.18.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages (from -r requirements.txt (line 121)) (2.18.0)\n",
      "Collecting pyparsing==3.1.4 (from -r requirements.txt (line 122))\n",
      "  Using cached pyparsing-3.1.4-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting python-dateutil==2.9.0.post0 (from -r requirements.txt (line 123))\n",
      "  Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting pytz==2024.2 (from -r requirements.txt (line 124))\n",
      "  Using cached pytz-2024.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "\u001b[31mERROR: Ignored the following versions that require a different python version: 11.0.0a1 Requires-Python >=3.11; 11.0.0a2 Requires-Python >=3.11; 2.18.0b1 Requires-Python >=3.11; 2.18.0rc1 Requires-Python >=3.11\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement pywin32==306 (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for pywin32==306\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-22 20:31:08.436625: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-22 20:31:08.459671: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-22 20:31:08.459706: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-22 20:31:08.473847: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-22 20:31:09.529776: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend\n"
     ]
    }
   ],
   "source": [
    "#Import libraries\n",
    "import boto3\n",
    "import sagemaker\n",
    "import gc\n",
    "import csv\n",
    "import keras_cv\n",
    "import os\n",
    "import io\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import random\n",
    "from collections.abc import Generator\n",
    "import itertools\n",
    "import datetime\n",
    "\n",
    "TF_ENABLE_ONEDNN_OPTS=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) HYPERPARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fraction of data to use (CEDRIC can reduce to speed up tests... go to 0.02!)\n",
    "train_frac_to_use = 0.03   #Reduce training data to this fraction. Set to \"1\" to use all data.\n",
    "val_frac_to_use = 0.03     #Reduce validation data to this fraction. Set to \"1\" to use all data.\n",
    "test_frac_to_use = 0.03     #Reduce validation data to this fraction. Set to \"1\" to use all data.\n",
    "\n",
    "#Memory management (Speeds up model and currently works with a machine with 16GB of memory)\n",
    "save_val_in_memory = False  #Place all validation directly in memory to accelerate the validation step in the model\n",
    "\n",
    "#Splitting of train-validate-test\n",
    "split_seed = 88                 #Seed to use for all train-validate-test splits, including the split of reserved Target=1 data\n",
    "reserve_frac = 0.1              #Fraction of total original data of Target = 1 (reserved for use in validation data)\n",
    "test_frac = 0.2                 #Fraction of total original data, excluding the reserved fraction, to use as the test data\n",
    "nb_of_augments = 100            #Number of augments to perform on Target = 1 images in train-validate sets\n",
    "val_frac = 0.33                 #Fraction of augmented train-validate list to use as the validation data. The rest becomes the training data.\n",
    "nb_of_augments_reserved = 15    #Number of augmentations to perform on reserved validation fraction (Target = 1). Note: this is added to the validation data.\n",
    "reduce_frac = 0.8               #Fraction of Target = 0 samples to remove from the validation data (improves balance)\n",
    "\n",
    "#Image resizing: all images are adjusted to this size so the the CNN receives same number of data points each time\n",
    "imgSize = 100\n",
    "\n",
    "#Hair removal (applies to all images)\n",
    "apply_hair_removal = False\n",
    "\n",
    "#Batch sizes\n",
    "train_batch_size = 32\n",
    "val_batch_size = 32\n",
    "test_batch_size = 32\n",
    "\n",
    "#Neural Network Parameters\n",
    "nb_neurons_hidden_layers = 36  #Number of neurons in each hidden layer\n",
    "dropout = 0.1                  #Fraction of neurons to drop\n",
    "\n",
    "#Optimizer Parameters\n",
    "learning_rate = 0.01           #Initial learning rate for the Adam optimizer\n",
    "\n",
    "#Epoch management (CEDRIC... reduce number of epochs if needed)\n",
    "nb_epochs = 5\n",
    "early_break = False #End early in case of increasing validation loss\n",
    "wt_save_freq = 1 #Frequency of weights saving to file (epochs/save)\n",
    "\n",
    "#Debugging\n",
    "cheat = False #add the target to the metadata so the model can precisely learn the correct response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) IMPORT DATA & DECLARE SAVES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 - Declare file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORT FILES\n",
    "#Directory for data files - FULL DATA\n",
    "dataPath = \"s3://images-projet-deep-learning/\" #slash required at end\n",
    "#Metadata file paths\n",
    "metaPath = dataPath + \"cleaned-metadata.csv\"\n",
    "#Image file path\n",
    "hdf5_file = \"train-image.hdf5\"\n",
    "\n",
    "#SAVE FILES\n",
    "#Directory for saved files \n",
    "savePath = \"s3://dsti-a23-deep-learning-outputs/\" #slash required at end\n",
    "#Model results\n",
    "modelResPath = savePath + \"model_results_save.csv\"\n",
    "#Test results (y_test, y_pred)\n",
    "testResPath = savePath + \"test_results_save.csv\"\n",
    "\n",
    "\n",
    "#ALTERNATIVE 1 QUI MARCHE PARFOIS\n",
    "#base_path = \"C:/Users/admin/Documents/DSTI/DeepLearning/Project/Computer_Vision/isic-2024-challenge\"\n",
    "#hdf5_file = os.path.join(base_path, \"sampleclaire-image.hdf5\")\n",
    "#metadata = pd.read_csv(metaPath, sep=\",\")\n",
    "\n",
    "#ALTERNATIVE 2 QUI MARCHE PARFOIS\n",
    "#base_path = \"C:/Users/admin/Documents/DSTI/DeepLearning/Project/Computer_Vision/isic-2024-challenge\"\n",
    "#metadata = pd.read_csv(os.path.join(base_path, \"train-metadata.csv\"))\n",
    "#hdf5_file = os.path.join(base_path, \"train-image.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "savePath = \"s3://dsti-a23-deep-learning-outputs/\" #slash required at end\n",
    "#Model results\n",
    "modelResPath = savePath + \"model_results_save.csv\"\n",
    "#Test results (y_test, y_pred)\n",
    "testResPath = savePath + \"test_results_save.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 - Load metadata from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/fsspec/registry.py:279: UserWarning: Your installed version of s3fs is very old and known to cause\n",
      "severe performance issues, see also https://github.com/dask/dask/issues/10276\n",
      "\n",
      "To fix, you should specify a lower version bound on s3fs, or\n",
      "update the current installation.\n",
      "\n",
      "  warnings.warn(s3_msg)\n",
      "/tmp/ipykernel_21847/327758908.py:2: DtypeWarning: Columns (51,52) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  metadata = pd.read_csv(metaPath, sep=\",\")\n"
     ]
    }
   ],
   "source": [
    "#Import metadata from file\n",
    "metadata = pd.read_csv(metaPath, sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- X_meta NA counts --\n",
      "isic_id                           0\n",
      "age_approx                     2798\n",
      "target                            0\n",
      "clin_size_long_diam_mm            0\n",
      "tbp_lv_areaMM2                    0\n",
      "tbp_lv_area_perim_ratio           0\n",
      "tbp_lv_eccentricity               0\n",
      "tbp_lv_minorAxisMM                0\n",
      "tbp_lv_color_std_mean             0\n",
      "tbp_lv_deltaLBnorm                0\n",
      "tbp_lv_radial_color_std_max       0\n",
      "tbp_lv_location                   0\n",
      "dtype: int64\n",
      "Number of unknown for tbp_lv_location 5756\n"
     ]
    }
   ],
   "source": [
    "#METADATA: color and size features having no NAs\n",
    "metadata = metadata[[\"isic_id\",\n",
    "                     \"age_approx\",\n",
    "                     \"target\",\n",
    "                     \"clin_size_long_diam_mm\",\n",
    "                     \"tbp_lv_areaMM2\",\n",
    "                     \"tbp_lv_area_perim_ratio\",\n",
    "                     \"tbp_lv_eccentricity\",\n",
    "                     \"tbp_lv_minorAxisMM\",\n",
    "                     \"tbp_lv_color_std_mean\",\n",
    "                     \"tbp_lv_deltaLBnorm\",\n",
    "                     \"tbp_lv_radial_color_std_max\",\n",
    "                     \"tbp_lv_location\"]]\n",
    "\n",
    "#Verify that there are no NAs\n",
    "print(\"-- X_meta NA counts --\")\n",
    "print(metadata.isna().sum())\n",
    "\n",
    "#Check number of Unknoxn for tbp_lv_location\n",
    "loc_unknown=metadata[metadata[\"tbp_lv_location\"]==\"Unknown\"]\n",
    "print(\"Number of unknown for tbp_lv_location\", len(loc_unknown))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Activate for debugging of the predict function\n",
    "if cheat:\n",
    "    metadata[\"target_cheat\"] = metadata[\"target\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 - Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata=metadata[metadata[\"tbp_lv_location\"]!=\"Unknown\"]\n",
    "\n",
    "loc_unknown2=metadata[metadata[\"tbp_lv_location\"]==\"Unknown\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply One-hot encoding for location\n",
    "location=pd.get_dummies(metadata[\"tbp_lv_location\"],prefix='category')\n",
    "location = location.astype(int)\n",
    "metadata = pd.concat([metadata, location], axis=1)\n",
    "metadata=metadata.drop(\"tbp_lv_location\",axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- X_meta NA counts --\n",
      "isic_id                             0\n",
      "age_approx                          0\n",
      "target                              0\n",
      "clin_size_long_diam_mm              0\n",
      "tbp_lv_areaMM2                      0\n",
      "tbp_lv_area_perim_ratio             0\n",
      "tbp_lv_eccentricity                 0\n",
      "tbp_lv_minorAxisMM                  0\n",
      "tbp_lv_color_std_mean               0\n",
      "tbp_lv_deltaLBnorm                  0\n",
      "tbp_lv_radial_color_std_max         0\n",
      "category_Head & Neck                0\n",
      "category_Left Arm                   0\n",
      "category_Left Arm - Lower           0\n",
      "category_Left Arm - Upper           0\n",
      "category_Left Leg                   0\n",
      "category_Left Leg - Lower           0\n",
      "category_Left Leg - Upper           0\n",
      "category_Right Arm                  0\n",
      "category_Right Arm - Lower          0\n",
      "category_Right Arm - Upper          0\n",
      "category_Right Leg                  0\n",
      "category_Right Leg - Lower          0\n",
      "category_Right Leg - Upper          0\n",
      "category_Torso Back                 0\n",
      "category_Torso Back Bottom Third    0\n",
      "category_Torso Back Middle Third    0\n",
      "category_Torso Back Top Third       0\n",
      "category_Torso Front                0\n",
      "category_Torso Front Bottom Half    0\n",
      "category_Torso Front Top Half       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Calculate the mean of age_approx for each target group\n",
    "mean_age_malign = metadata.loc[metadata[\"target\"] == 1, \"age_approx\"].mean()\n",
    "mean_age_benign = metadata.loc[metadata[\"target\"] == 0, \"age_approx\"].mean()\n",
    "\n",
    "# Define a function to fill NA based on the target value\n",
    "def fill_na_by_target(row):\n",
    "    if pd.isna(row['age_approx']):\n",
    "        if row['target'] == 1:\n",
    "            return mean_age_malign\n",
    "        elif row['target'] == 0:\n",
    "            return mean_age_benign\n",
    "    return row['age_approx']\n",
    "\n",
    "# Apply the function to the age_approx column\n",
    "metadata['age_approx'] = metadata.apply(fill_na_by_target, axis=1)\n",
    "\n",
    "#Verify that there are no NAs\n",
    "print(\"-- X_meta NA counts --\")\n",
    "print(metadata.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Train, Validate, Test Split + Preparation of Data Augmentation\n",
    "1. Make two separate lists of isic_ids for target=0 and target=1. Transform into tuples (isic_id, target, mod toggle). Base data has mod toggle = 0, meaning no adjustment will be made.\n",
    "2. Reserve 10% of target = 1 for validate\n",
    "3. Split into train-validate & test on both lists (0 and 1). Take the test lists (0 and 1), concatenate and shuffle them\n",
    "4. Create augmentation preparation function for target = 1: mod toggle = strictly positive integer (this adds more isic_ids to the list, with mod toggle non zero))\n",
    "5. Run augmentation preparation function on train-validate and the reserved validation data: mod toggle = strictly positive integer\n",
    "6. Split train-validate on both lists (0 and 1)\n",
    "7. Reduce the validation data on target = 0 by value specified in reduce_frac\n",
    "8. Concatenate and shuffle the train lists and validation lists\n",
    "9. Limit training and validation data to speed up training (take only fraction of prepared lists)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 - Make two separate lists of isic_ids for target=0 (mod_toggle = -1) and target=1 (mod_toggle = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total ids with target = 0: 394910\n",
      "Total ids with target = 1: 393\n"
     ]
    }
   ],
   "source": [
    "#Make a list of isic_ids for each target value (0 and 1)\n",
    "isic_id_target_0 = metadata[metadata['target'] == 0]['isic_id'].tolist()\n",
    "isic_id_target_1 = metadata[metadata['target'] == 1]['isic_id'].tolist()\n",
    "\n",
    "#Retrieve dataframe with isic id and target\n",
    "temp_0 = metadata[metadata[\"isic_id\"].isin(isic_id_target_0)].loc[:,[\"isic_id\",\"target\"]]\n",
    "temp_1 = metadata[metadata[\"isic_id\"].isin(isic_id_target_1)].loc[:,[\"isic_id\",\"target\"]]\n",
    "\n",
    "#Convert into list of tuples... this makes it compatible with data augmentations\n",
    "#Form: (isic_id, target, mod toggle)\n",
    "isic_id_target_0 = list(zip(temp_0.iloc[:,0], temp_0.iloc[:,1], [-1]*len(temp_0)))\n",
    "isic_id_target_1 = list(zip(temp_1.iloc[:,0], temp_1.iloc[:,1], [0]*len(temp_1)))\n",
    "\n",
    "#Delete temporary dataframes (the original metadata dataframe is untouched)\n",
    "del temp_0\n",
    "del temp_1\n",
    "\n",
    "#Count the number of occurrences for each target value\n",
    "print(\"Total ids with target = 0:\", len(isic_id_target_0))\n",
    "print(\"Total ids with target = 1:\", len(isic_id_target_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 - Reserve 10% of target = 1 for validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total ids with target = 0: 394910\n",
      "Total ids with target = 1: 353\n",
      "Total reserved target = 1: 40\n"
     ]
    }
   ],
   "source": [
    "#Keep 10% of isic_Id of target=1 without duplication\n",
    "isic_id_target_1, isic_id_target_1_reserved = train_test_split(isic_id_target_1, test_size = reserve_frac, random_state=split_seed, shuffle=False)\n",
    "\n",
    "#Count the number of occurrences for each target value (AFTER RESERVATION)\n",
    "print(\"Total ids with target = 0:\", len(isic_id_target_0))\n",
    "print(\"Total ids with target = 1:\", len(isic_id_target_1))\n",
    "print(\"Total reserved target = 1:\", len(isic_id_target_1_reserved))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 - Split into train-validate & test on both lists (0 and 1). Take the test lists (0 and 1), concatenate and shuffle them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split out the test ids\n",
    "trainval_0, test_0 = train_test_split(isic_id_target_0, test_size = test_frac, random_state=split_seed, shuffle=True)\n",
    "trainval_1, test_1 = train_test_split(isic_id_target_1, test_size = test_frac, random_state=split_seed, shuffle=True)\n",
    "\n",
    "test_ids = test_0 + test_1\n",
    "np.random.shuffle(test_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 - Create augmentation preparation function for target = 1: mod toggle = strictly positive integer\n",
    "(this adds more isic_ids to the list, with mod toggle non zero))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate a list containing augmentation toggles. Apply only to training and validation sets.\n",
    "def augment_prep(tuple_list, nb_of_augments = 30, shuffle_seed=None):\n",
    "    augment_list = []\n",
    "    \n",
    "    #If augmentation is desired, then the mod toggle is a strictly positive integer\n",
    "    augment_list = [(item[0], item[1], i) for item in tuple_list for i in range(1, nb_of_augments + 1)]\n",
    "    \n",
    "    #Shuffle the list\n",
    "    augment_list.extend(tuple_list)\n",
    "    np.random.seed(shuffle_seed)\n",
    "    np.random.shuffle(augment_list)\n",
    "\n",
    "    return augment_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 - Run augmentation preparation function on train-validate and the reserved validation data: mod toggle = strictly positive integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Augment the training and validation list\n",
    "trainval_1 = augment_prep(trainval_1, nb_of_augments=nb_of_augments, shuffle_seed=50)\n",
    "\n",
    "#Duplicate the reserved training data\n",
    "reserved_1 = augment_prep(isic_id_target_1_reserved, nb_of_augments=nb_of_augments_reserved, shuffle_seed=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 - Split train-validate on both lists (0 and 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split training and validation lists\n",
    "train_0, val_0 = train_test_split(trainval_0, test_size = val_frac, random_state=split_seed, shuffle=True)\n",
    "train_1, val_1 = train_test_split(trainval_1, test_size = val_frac, random_state=split_seed, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7 - Reduce the validation data on target = 0 by value specified in reduce_frac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reduce the validation data of type Target = 0\n",
    "nb_samples = int((1 - reduce_frac) * len(val_0))\n",
    "val_0 = random.sample(val_0, nb_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.8 - Concatenate and shuffle the train and validation lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concatenate\n",
    "train_ids=train_0.copy()\n",
    "train_ids.extend(train_1)\n",
    "val_ids=list(itertools.chain(val_0, val_1, reserved_1))\n",
    "\n",
    "#Shuffle\n",
    "np.random.seed(60)\n",
    "np.random.shuffle(train_ids)\n",
    "np.random.shuffle(val_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Validate/Test Counts: 230753 / 30891 / 79053\n",
      "Train/Validate/Test Fractions: 0.68 / 0.09 / 0.23\n",
      "Proportion of Target = 1 in training data: 0.08269448284529345\n",
      "Proportion of Target = 1 in validation data: 0.3250137580525072\n",
      "Proportion of Target = 1 in test data: 0.0008981316332081009\n"
     ]
    }
   ],
   "source": [
    "#Calculate the proportaion of Target=1 in each set (training, validation, test)\n",
    "def calc_frac_target1(ids):\n",
    "    return sum([item[1] for item in ids]) / len(ids)\n",
    "\n",
    "tot_samples = len(train_ids) + len(val_ids) + len(test_ids)\n",
    "\n",
    "print(\"Train/Validate/Test Counts:\", len(train_ids), \"/\", len(val_ids), \"/\", len(test_ids))\n",
    "print(\"Train/Validate/Test Fractions:\", round(len(train_ids)/tot_samples,2), \"/\", round(len(val_ids)/tot_samples,2), \"/\", round(len(test_ids)/tot_samples,2))\n",
    "print(\"Proportion of Target = 1 in training data:\", calc_frac_target1(train_ids))\n",
    "print(\"Proportion of Target = 1 in validation data:\", calc_frac_target1(val_ids))\n",
    "print(\"Proportion of Target = 1 in test data:\", calc_frac_target1(test_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.9 - Limit training and validation data to speed up training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train ids length before: 230753\n",
      "Train ids length after: 6922\n",
      "Validate ids length before: 30891\n",
      "Validate ids length after: 926\n",
      "Test ids length before: 79053\n",
      "Test ids length after: 2371\n"
     ]
    }
   ],
   "source": [
    "#Choose a portion of TRAINING ids to load into memory\n",
    "def take_fewer_samples(ids, frac_to_use, seed):\n",
    "    if frac_to_use < 1 and frac_to_use > 0:\n",
    "        random.seed(seed)\n",
    "        k = int(frac_to_use * len(ids))\n",
    "        ids_short = random.choices(ids, k=k)\n",
    "        return ids_short\n",
    "    return ids\n",
    "\n",
    "print(\"Train ids length before:\", len(train_ids))\n",
    "train_ids = take_fewer_samples(train_ids, train_frac_to_use, seed=12)\n",
    "print(\"Train ids length after:\", len(train_ids))\n",
    "\n",
    "print(\"Validate ids length before:\", len(val_ids))\n",
    "val_ids = take_fewer_samples(val_ids, val_frac_to_use, seed=12)\n",
    "print(\"Validate ids length after:\", len(val_ids))\n",
    "\n",
    "print(\"Test ids length before:\", len(test_ids))\n",
    "test_ids = take_fewer_samples(test_ids, test_frac_to_use, seed=12)\n",
    "print(\"Test ids length after:\", len(test_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Augmentation functions (used during dataset creation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hair_removal(image, crop_pixels=10):\n",
    "    height_pixels = len(image)  # Image rows\n",
    "    width_pixels = len(image[0])  # Image columns\n",
    "\n",
    "    # Image cropping\n",
    "    height = [crop_pixels, height_pixels - crop_pixels]\n",
    "    width = [crop_pixels, width_pixels - crop_pixels]\n",
    "    img = image[height[0]:height[1], width[0]:width[1]]\n",
    "\n",
    "    # Gray scale\n",
    "    grayScale = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    # Black hat filter\n",
    "    kernel = cv2.getStructuringElement(1, (9, 9))\n",
    "    blackhat = cv2.morphologyEx(grayScale, cv2.MORPH_BLACKHAT, kernel)\n",
    "    # Gaussian filter\n",
    "    bhg = cv2.GaussianBlur(blackhat, (3, 3), cv2.BORDER_DEFAULT)\n",
    "    # Binary thresholding (MASK)\n",
    "    ret, mask = cv2.threshold(bhg, 10, 255, cv2.THRESH_BINARY)\n",
    "    # Replace pixels of the mask\n",
    "    dst = cv2.inpaint(img, mask, 6, cv2.INPAINT_TELEA)\n",
    "\n",
    "    return dst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Define the augmentation function\\ndef augment_image(image, is_training=False):\\n    #Apply a series of augmentations to create diverse variations of the input image.\\n    #Includes random flips, rotations, brightness adjustments, and other transformations.\\n    # Apply various augmentations\\n\\n    # parameter\\n    if is_training==True:\\n        height_factor_cut=(0.02, 0.06),\\n        width_factor_cut=(0.02, 0.06),\\n        max_delta_brigth=0.25,\\n        lower_sat=0.7, \\n        upper_sat=1.8,\\n        lower_cont=0.7, \\n        upper_cont=1.8,\\n        minval_rot=0,\\n        maxvalue_rot=4\\n    else:\\n        height_factor_cut=(0, 0),\\n        width_factor_cut=(0, 0),\\n        max_delta_brigth=0.15,\\n        lower_sat=0.8, \\n        upper_sat=1.2,\\n        lower_cont=0.8, \\n        upper_cont=1.2,\\n        minval_rot=0,\\n        maxvalue_rot=0\\n        \\n    # RandomCutout initialization\\n    cutout_layer = keras_cv.layers.RandomCutout(height_factor=height_factor_cut, width_factor=width_factor_cut)\\n    \\n    # List of augmentations\\n    augmentations = [\\n        tf.image.random_flip_left_right,  \\n        tf.image.random_flip_up_down,   \\n        tf.image.random_brightness,      \\n        tf.image.random_contrast,         \\n        tf.image.random_saturation,\\n        lambda img: tf.image.rot90(img, tf.random.uniform(shape=[], minval=minval_rot, maxval=maxvalue_rot, dtype=tf.int32)),\\n        lambda img: cutout_layer(img) \\n    ]\\n\\n    # Shuffle and pick one augmentation\\n    augmentation = augmentations[tf.random.uniform(shape=[], minval=0, maxval=len(augmentations), dtype=tf.int32)]\\n    \\n    # Apply augmentation with 95% probability\\n    #if tf.random.uniform([]) < 0.95:\\n    # Apply augmentation \\n    if augmentation in [tf.image.random_flip_left_right, tf.image.random_flip_up_down]:\\n        image = augmentation(image) \\n    elif augmentation == tf.image.random_brightness:\\n        image = augmentation(image, max_delta=max_delta_brigth)  \\n    elif augmentation == tf.image.random_contrast:\\n        image = augmentation(image, lower_cont, upper_cont)\\n    elif augmentation == tf.image.random_saturation:\\n        image = augmentation(image, lower_sat, upper_sat)  \\n    else:\\n        image = augmentation(image)\\n    \\n    return image\\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Define the augmentation function\n",
    "def augment_image(image, is_training=False):\n",
    "    #Apply a series of augmentations to create diverse variations of the input image.\n",
    "    #Includes random flips, rotations, brightness adjustments, and other transformations.\n",
    "    # Apply various augmentations\n",
    "\n",
    "    # parameter\n",
    "    if is_training==True:\n",
    "        height_factor_cut=(0.02, 0.06),\n",
    "        width_factor_cut=(0.02, 0.06),\n",
    "        max_delta_brigth=0.25,\n",
    "        lower_sat=0.7, \n",
    "        upper_sat=1.8,\n",
    "        lower_cont=0.7, \n",
    "        upper_cont=1.8,\n",
    "        minval_rot=0,\n",
    "        maxvalue_rot=4\n",
    "    else:\n",
    "        height_factor_cut=(0, 0),\n",
    "        width_factor_cut=(0, 0),\n",
    "        max_delta_brigth=0.15,\n",
    "        lower_sat=0.8, \n",
    "        upper_sat=1.2,\n",
    "        lower_cont=0.8, \n",
    "        upper_cont=1.2,\n",
    "        minval_rot=0,\n",
    "        maxvalue_rot=0\n",
    "        \n",
    "    # RandomCutout initialization\n",
    "    cutout_layer = keras_cv.layers.RandomCutout(height_factor=height_factor_cut, width_factor=width_factor_cut)\n",
    "    \n",
    "    # List of augmentations\n",
    "    augmentations = [\n",
    "        tf.image.random_flip_left_right,  \n",
    "        tf.image.random_flip_up_down,   \n",
    "        tf.image.random_brightness,      \n",
    "        tf.image.random_contrast,         \n",
    "        tf.image.random_saturation,\n",
    "        lambda img: tf.image.rot90(img, tf.random.uniform(shape=[], minval=minval_rot, maxval=maxvalue_rot, dtype=tf.int32)),\n",
    "        lambda img: cutout_layer(img) \n",
    "    ]\n",
    "\n",
    "    # Shuffle and pick one augmentation\n",
    "    augmentation = augmentations[tf.random.uniform(shape=[], minval=0, maxval=len(augmentations), dtype=tf.int32)]\n",
    "    \n",
    "    # Apply augmentation with 95% probability\n",
    "    #if tf.random.uniform([]) < 0.95:\n",
    "    # Apply augmentation \n",
    "    if augmentation in [tf.image.random_flip_left_right, tf.image.random_flip_up_down]:\n",
    "        image = augmentation(image) \n",
    "    elif augmentation == tf.image.random_brightness:\n",
    "        image = augmentation(image, max_delta=max_delta_brigth)  \n",
    "    elif augmentation == tf.image.random_contrast:\n",
    "        image = augmentation(image, lower_cont, upper_cont)\n",
    "    elif augmentation == tf.image.random_saturation:\n",
    "        image = augmentation(image, lower_sat, upper_sat)  \n",
    "    else:\n",
    "        image = augmentation(image)\n",
    "    \n",
    "    return image\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the augmentation function\n",
    "def augment_image(image, mod_toggle, is_training=False):\n",
    "    \"\"\"\n",
    "    Apply a series of augmentations to create diverse variations of the input image.\n",
    "    Includes random flips, rotations, brightness adjustments, and other transformations.\n",
    "    \"\"\"\n",
    "\n",
    "    # Parameters\n",
    "    if is_training==True:\n",
    "        #Cutout values\n",
    "        height_factor_cut=(0.02, 0.06)\n",
    "        width_factor_cut=(0.02, 0.06)\n",
    "        random_cutout = keras_cv.layers.RandomCutout(height_factor=height_factor_cut, width_factor=width_factor_cut)\n",
    "        #Brightness values\n",
    "        min_delta_bright=0.05\n",
    "        max_delta_bright=0.25\n",
    "        #Saturation values\n",
    "        weak_sat_low=0.7\n",
    "        weak_sat_high=0.95\n",
    "        strong_sat_low=1.05\n",
    "        strong_sat_high=1.8\n",
    "        #Contrast values\n",
    "        weak_cont_low=0.7\n",
    "        weak_cont_high=0.95\n",
    "        strong_cont_low=1.05\n",
    "        strong_cont_high=1.8\n",
    "\n",
    "    else:\n",
    "        #Brightness values\n",
    "        min_delta_bright=0.05\n",
    "        max_delta_bright=0.15\n",
    "        #Saturation values\n",
    "        weak_sat_low=0.8\n",
    "        weak_sat_high=0.95\n",
    "        strong_sat_low=1.05\n",
    "        strong_sat_high=1.2\n",
    "        #Contrast values\n",
    "        weak_cont_low=0.8\n",
    "        weak_cont_high=0.95\n",
    "        strong_cont_low=1.05\n",
    "        strong_cont_high=1.2\n",
    "\n",
    "    #List of base augmentations\n",
    "    base_augments = [\n",
    "        lambda img: tf.image.flip_left_right(img),\n",
    "        lambda img: tf.image.flip_up_down(img),\n",
    "        lambda img: tf.image.rot90(img, k=1),\n",
    "        lambda img: tf.image.rot90(img, k=2),\n",
    "        lambda img: tf.image.rot90(img, k=3),\n",
    "        lambda img: random_cutout(img)\n",
    "    ]\n",
    "\n",
    "    #List of other augmentations that can be performed multiple times\n",
    "    other_augments = [\n",
    "        lambda img: tf.image.adjust_brightness(img, -random.uniform(min_delta_bright, max_delta_bright)),\n",
    "        lambda img: tf.image.adjust_brightness(img, random.uniform(min_delta_bright, max_delta_bright)),\n",
    "        lambda img: tf.image.random_contrast(img, lower=weak_cont_low, upper=weak_cont_high),\n",
    "        lambda img: tf.image.random_contrast(img, lower=strong_cont_low, upper=strong_cont_high),\n",
    "        lambda img: tf.image.random_saturation(img, lower=weak_sat_low, upper=weak_sat_high),\n",
    "        lambda img: tf.image.random_saturation(img, lower=strong_sat_low, upper=strong_sat_high)\n",
    "    ]\n",
    "\n",
    "    #Select augmentations to use based on whether it is training data or not\n",
    "    if is_training:\n",
    "        base_augments = base_augments\n",
    "        other_augments = other_augments \n",
    "    else:\n",
    "        base_selection = [0,1]  #Positions of the augmentations to use in the base_augments list\n",
    "        base_augments = [base_augments[i] for i in base_selection]\n",
    "        other_augments = other_augments\n",
    "\n",
    "    #Engage the augment based on the mod_toggle. The base_augments are always done first.\n",
    "    nb_base_aug = len(base_augments)\n",
    "    \n",
    "    if mod_toggle <= nb_base_aug and mod_toggle > 0:\n",
    "        augmentation = base_augments[mod_toggle - 1] #Mod toggles start at 1\n",
    "        image = augmentation(image)\n",
    "    else:\n",
    "        augmentation = random.choice(other_augments)\n",
    "        image = augmentation(image)\n",
    "        #Apply a second transformation with a certain probability\n",
    "        if tf.random.uniform([]) < 0.85:\n",
    "            augmentation2 = random.choice(base_augments)\n",
    "            image = augmentation2(image)\n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nid_try = \"ISIC_5675460\"\\nwith h5py.File(hdf5_file, \\'r\\') as h5file:\\n    img_try = np.array(Image.open(io.BytesIO(h5file[id_try][()])))\\n\\nfor mod_toggle in range(500):\\n    img = hair_removal(img_try)\\n    img = cv2.resize(img, (100, 100), interpolation= cv2.INTER_AREA)\\n    augment_image(img, mod_toggle, is_training=True)\\n'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TESTING OF AUGMENT_IMAGE FUNCTION\n",
    "\"\"\"\n",
    "id_try = \"ISIC_5675460\"\n",
    "with h5py.File(hdf5_file, 'r') as h5file:\n",
    "    img_try = np.array(Image.open(io.BytesIO(h5file[id_try][()])))\n",
    "\n",
    "for mod_toggle in range(500):\n",
    "    img = hair_removal(img_try)\n",
    "    img = cv2.resize(img, (100, 100), interpolation= cv2.INTER_AREA)\n",
    "    augment_image(img, mod_toggle, is_training=True)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nid_try = \"ISIC_5675460\"\\nwith h5py.File(hdf5_file, \\'r\\') as h5file:\\n    img_try = np.array(Image.open(io.BytesIO(h5file[id_try][()])))\\n\\nheight_factor_cut=(0.02, 0.06)\\nwidth_factor_cut=(0.02, 0.06)\\nrandom_cutout = keras_cv.layers.RandomCutout(height_factor=height_factor_cut, width_factor=width_factor_cut)\\n\\nbase_augments = [\\n    lambda img: img,\\n    lambda img: tf.image.flip_left_right(img),\\n    lambda img: tf.image.flip_up_down(img),\\n    lambda img: tf.image.rot90(img, k=1),\\n    lambda img: tf.image.rot90(img, k=2),\\n    lambda img: tf.image.rot90(img, k=3),\\n    lambda img: random_cutout(img).numpy().astype(int),\\n]\\n\\nimages = []\\nfor augmentation in base_augments:\\n    image = augmentation(img_try)\\n    images.append(image)\\n\\n#Show the images\\nfor image in images:\\n    plt.imshow(image, interpolation=None)\\n    plt.grid(None)\\n    plt.show()\\n'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TESTING OF BASE AUGMENTS\n",
    "\"\"\"\n",
    "id_try = \"ISIC_5675460\"\n",
    "with h5py.File(hdf5_file, 'r') as h5file:\n",
    "    img_try = np.array(Image.open(io.BytesIO(h5file[id_try][()])))\n",
    "\n",
    "height_factor_cut=(0.02, 0.06)\n",
    "width_factor_cut=(0.02, 0.06)\n",
    "random_cutout = keras_cv.layers.RandomCutout(height_factor=height_factor_cut, width_factor=width_factor_cut)\n",
    "\n",
    "base_augments = [\n",
    "    lambda img: img,\n",
    "    lambda img: tf.image.flip_left_right(img),\n",
    "    lambda img: tf.image.flip_up_down(img),\n",
    "    lambda img: tf.image.rot90(img, k=1),\n",
    "    lambda img: tf.image.rot90(img, k=2),\n",
    "    lambda img: tf.image.rot90(img, k=3),\n",
    "    lambda img: random_cutout(img).numpy().astype(int),\n",
    "]\n",
    "\n",
    "images = []\n",
    "for augmentation in base_augments:\n",
    "    image = augmentation(img_try)\n",
    "    images.append(image)\n",
    "\n",
    "#Show the images\n",
    "for image in images:\n",
    "    plt.imshow(image, interpolation=None)\n",
    "    plt.grid(None)\n",
    "    plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nid_try = \"ISIC_5675460\"\\nwith h5py.File(hdf5_file, \\'r\\') as h5file:\\n    img_try = np.array(Image.open(io.BytesIO(h5file[id_try][()])))\\n\\n#Brightness values\\nmin_delta_bright=0.05\\nmax_delta_bright=0.25\\n#Saturation values\\nweak_sat_low=0.7\\nweak_sat_high=0.95\\nstrong_sat_low=1.05\\nstrong_sat_high=1.8\\n#Contrast values\\nweak_cont_low=0.7\\nweak_cont_high=0.95\\nstrong_cont_low=1.05\\nstrong_cont_high=1.8\\n\\n#List of other augmentations that can be performed multiple times\\nother_augments = [\\n    lambda img: tf.image.adjust_brightness(img, -random.uniform(min_delta_bright, max_delta_bright)),\\n    lambda img: tf.image.adjust_brightness(img, random.uniform(min_delta_bright, max_delta_bright)),\\n    lambda img: tf.image.random_contrast(img, lower=weak_cont_low, upper=weak_cont_high),\\n    lambda img: tf.image.random_contrast(img, lower=strong_cont_low, upper=strong_cont_high),\\n    lambda img: tf.image.random_saturation(img, lower=weak_sat_low, upper=weak_sat_high),\\n    lambda img: tf.image.random_saturation(img, lower=strong_sat_low, upper=strong_sat_high)\\n]\\n\\nimages = []\\nfor augmentation in other_augments:\\n    image = augmentation(img_try)\\n    if tf.random.uniform([]) < 1:\\n        augmentation2 = random.choice(base_augments)\\n        image = augmentation2(image)\\n    images.append(image)\\n\\n#Show the images\\nfor image in images:\\n    plt.imshow(image, interpolation=None)\\n    plt.grid(None)\\n    plt.show()\\n'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TESTING OF OTHER AUGMENTS\n",
    "\"\"\"\n",
    "id_try = \"ISIC_5675460\"\n",
    "with h5py.File(hdf5_file, 'r') as h5file:\n",
    "    img_try = np.array(Image.open(io.BytesIO(h5file[id_try][()])))\n",
    "\n",
    "#Brightness values\n",
    "min_delta_bright=0.05\n",
    "max_delta_bright=0.25\n",
    "#Saturation values\n",
    "weak_sat_low=0.7\n",
    "weak_sat_high=0.95\n",
    "strong_sat_low=1.05\n",
    "strong_sat_high=1.8\n",
    "#Contrast values\n",
    "weak_cont_low=0.7\n",
    "weak_cont_high=0.95\n",
    "strong_cont_low=1.05\n",
    "strong_cont_high=1.8\n",
    "\n",
    "#List of other augmentations that can be performed multiple times\n",
    "other_augments = [\n",
    "    lambda img: tf.image.adjust_brightness(img, -random.uniform(min_delta_bright, max_delta_bright)),\n",
    "    lambda img: tf.image.adjust_brightness(img, random.uniform(min_delta_bright, max_delta_bright)),\n",
    "    lambda img: tf.image.random_contrast(img, lower=weak_cont_low, upper=weak_cont_high),\n",
    "    lambda img: tf.image.random_contrast(img, lower=strong_cont_low, upper=strong_cont_high),\n",
    "    lambda img: tf.image.random_saturation(img, lower=weak_sat_low, upper=weak_sat_high),\n",
    "    lambda img: tf.image.random_saturation(img, lower=strong_sat_low, upper=strong_sat_high)\n",
    "]\n",
    "\n",
    "images = []\n",
    "for augmentation in other_augments:\n",
    "    image = augmentation(img_try)\n",
    "    if tf.random.uniform([]) < 1:\n",
    "        augmentation2 = random.choice(base_augments)\n",
    "        image = augmentation2(image)\n",
    "    images.append(image)\n",
    "\n",
    "#Show the images\n",
    "for image in images:\n",
    "    plt.imshow(image, interpolation=None)\n",
    "    plt.grid(None)\n",
    "    plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Metadata Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a metadata dictionary for efficient lookup\n",
    "#Structure: {index: value} where value is (mod_toggle, metadata array)\n",
    "\n",
    "#Objective: We need \"train_ids\" to efficiently retrieve and augment images from the HDF5 file. This already exists.\n",
    "#           We need to accesss metadata through a dictionary to improve speed. A common reference is needed for both.\n",
    "\n",
    "#Idea:      Since train_ids is a LIST of tuples (isic_id, target, mod toggle), it is accessed via indices (ex. train_ids[10]).\n",
    "#           We need to make a dictionary that, for each train_ids index, lists all the metadata associated to the isic_id.\n",
    "#           Thus, when train_ids[10] is called, we call the dictionary and request key=10 to get the metadata.\n",
    "#           Result: very fast data retrieval\n",
    "\n",
    "def make_meta_dict(metadata, isic_ids_tuple):\n",
    "    #Reindex. The metadata must be contiguously indexed. Holes in index numbering will not work.\n",
    "    metadata = metadata.reset_index(drop=True)\n",
    "\n",
    "    #Get the column number for \"isic_id\" and \"target\" in the metadata dataframe.\n",
    "    #This allows us to know where these items are located in the metadata retrieved for each item.\n",
    "    #This is used later to filter out these items from the metadata.\n",
    "    col_num_id = metadata.columns.get_loc(\"isic_id\")\n",
    "    col_num_target = metadata.columns.get_loc(\"target\")\n",
    "\n",
    "    #The metadata contains all unique isic_ids. Since the dataframe is reindexed, it is possible to make\n",
    "    #a dictionary of (isic_id: row number) for fast retrieval of all metadata associated to an isic_id.\n",
    "\n",
    "    #Take the metadata dataframe and create a dictionary that stores (isic_id, row number).\n",
    "    metadata_index_dict = metadata[\"isic_id\"].to_dict()\n",
    "    metadata_index_dict = dict((v, k) for k, v in metadata_index_dict.items())\n",
    "\n",
    "    #Make a dictionary of (index: (mod toggle, metadata)), where index is the position of a sample in train_ids and metadata is the metadata\n",
    "    #associated to the sample's isic_id. We thus create a dict_of_meta that is a mirror image of the \"isic_ids_tuple\" list.\n",
    "    #We must be careful to not shuffle \"isic_ids_tuple\".\n",
    "    dict_of_meta = {}\n",
    "    for pos, tup in enumerate(isic_ids_tuple):\n",
    "        # Use the lookup table to directly find the index\n",
    "        index = metadata_index_dict.get(tup[0], -1)  # -1 if not found\n",
    "\n",
    "        if index != -1:\n",
    "            # Access the row directly without masking\n",
    "            dict_of_meta.update({pos: (tup[2], np.array(metadata.iloc[index].values))})\n",
    "            # Process the row as needed\n",
    "        else:\n",
    "            raise Exception(\"isic_id values are not all unique\")\n",
    "    return dict_of_meta, col_num_id, col_num_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the metadata dictionaries for train-validate-test\n",
    "train_meta_dict, train_pos_isic_id, train_pos_target = make_meta_dict(metadata, train_ids)\n",
    "val_meta_dict, val_pos_isic_id, val_pos_target = make_meta_dict(metadata, val_ids)\n",
    "test_meta_dict, test_pos_isic_id, test_pos_target = make_meta_dict(metadata, test_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAMPLE OF ITEMS FROM VALIDATION META DICTIONARY\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: (-1,\n",
       "  array(['ISIC_7165460', 55.0, 0, 6.69, 25.82098, 25.89203, 0.6025009,\n",
       "         5.812954, 1.021409, 5.621105, 1.045389, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0], dtype=object)),\n",
       " 1: (-1,\n",
       "  array(['ISIC_7121780', 50.0, 0, 3.11, 4.31600675548883, 15.7481987482575,\n",
       "         0.811870937116803, 1.86919395237753, 0.654966439645182,\n",
       "         8.84540004559486, 0.953580157645684, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=object)),\n",
       " 2: (-1,\n",
       "  array(['ISIC_6293671', 55.0, 0, 6.4, 29.4802, 17.91898, 0.3042147,\n",
       "         6.115376, 2.852407, 16.25082, 1.430599, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], dtype=object))}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Look some dictionary elements to understand the structure {index, (mod_toggle, metadata)}\n",
    "print(\"SAMPLE OF ITEMS FROM VALIDATION META DICTIONARY\")\n",
    "dict(filter(lambda item: item[0] in {0, 1, 2}, val_meta_dict.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Dataset generation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMAGE generator in a class\n",
    "class hdf5_generator_all_included(Generator):\n",
    "    def __init__(self, file, meta_dict, dict_pos_isic_id, dict_pos_target, num_features, imgSize, is_training=False, shuffle_seed=None, apply_hair_removal=False):\n",
    "        self.file = file\n",
    "        self.meta_dict = meta_dict\n",
    "        self.pos_mod_toggle = 0                     #Position of 'mod_toggle' within dictionary: structure {key: value} where value is (mod_toggle, metadata array)\n",
    "        self.pos_metadata_array = 1                 #Position of 'metadata array' within dictionary: structure {key: value} where value is (mod_toggle, metadata array)\n",
    "        self.dict_pos_isic_id = dict_pos_isic_id    #Set position of 'isic_id\" within the metadata array: structure [var0, var1, var2, var3, ...]\n",
    "        self.dict_pos_target = dict_pos_target      #Set position of 'target\" within the metadata array: structure [var0, var1, var2, var3, ...]\n",
    "        self.num_features = num_features\n",
    "        self.imgSize = imgSize\n",
    "        self.is_training = is_training\n",
    "        self.shuffle_seed = shuffle_seed\n",
    "        self.apply_hair_removal = apply_hair_removal\n",
    "        self.len = len(meta_dict)\n",
    "        self.start = 0\n",
    "        self.stop = self.len\n",
    "        self.i = self.start\n",
    "        self.error_check()\n",
    "        self.open_hdf5()\n",
    "        self.order_and_shuffle()\n",
    "        \n",
    "    def send(self, value):\n",
    "        if self.i < self.stop:\n",
    "            if self.i == self.start:\n",
    "                self.open_hdf5()\n",
    "\n",
    "            #Retrieve index of isic_id according to the shuffled order\n",
    "            index = self.order[self.i]\n",
    "\n",
    "            #Retrieve target... remember that each item of the the meta_dict is a tuple of (mod toggle, metadata)\n",
    "            target = self.meta_dict[index][self.pos_metadata_array][self.dict_pos_target]\n",
    "            target = np.reshape(target, (1,1))\n",
    "            target = tf.cast(target, dtype=tf.int32)\n",
    "\n",
    "            #Retrieve metadata... remember that each item of the the meta_dict is a tuple of (mod toggle, metadata)\n",
    "            meta = np.delete(self.meta_dict[index][self.pos_metadata_array], [self.dict_pos_isic_id, self.dict_pos_target], 0)\n",
    "            meta = meta.astype(dtype=float)\n",
    "            meta = tf.cast(meta, dtype=tf.float32)\n",
    "            meta = tf.reshape(meta, shape=(1, self.num_features))\n",
    "\n",
    "            try:\n",
    "                #Retrieve isic_id\n",
    "                img_name = self.meta_dict[index][self.pos_metadata_array][self.dict_pos_isic_id]\n",
    "                \n",
    "                # Load image data from HDF5\n",
    "                img = np.array(Image.open(io.BytesIO(self.h5file[img_name][()])))\n",
    "\n",
    "                # Clean image\n",
    "                if self.apply_hair_removal:\n",
    "                    img = hair_removal(img)\n",
    "\n",
    "                # Resize the image\n",
    "                img = cv2.resize(img, (self.imgSize, self.imgSize), interpolation= cv2.INTER_AREA)\n",
    "\n",
    "                # Apply augmentations if needed\n",
    "                mod_toggle = self.meta_dict[index][self.pos_mod_toggle]\n",
    "                if mod_toggle > 0:\n",
    "                    img=augment_image(img, mod_toggle, self.is_training)\n",
    "                    \n",
    "                # Standardize and return as TensorFlow constant\n",
    "                img = tf.constant(img / 255, dtype=tf.float32)\n",
    "                \n",
    "                #Augment counter\n",
    "                self.i = self.i + 1\n",
    "                return (img, meta), target\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error loading image {img_name}: {e}\")\n",
    "                # log the error to a file for later analysis\n",
    "                with open('image_errors.log', 'a') as f:\n",
    "                    f.write(f\"Error loading image {img_name}: {e}\\n\")\n",
    "\n",
    "            if self.i == self.stop:\n",
    "                self.h5file.close()\n",
    "        raise StopIteration\n",
    "\n",
    "    def throw(self, typ, val=None, tb=None):\n",
    "        #Close HDF5 file and terminate generator\n",
    "        try:\n",
    "            self.h5file.close()\n",
    "            super().throw(typ, val, tb)\n",
    "        except:\n",
    "            super().throw(typ, val, tb)\n",
    "\n",
    "    def error_check(self):\n",
    "        #Seed type check\n",
    "        try:\n",
    "            int(self.shuffle_seed) == self.shuffle_seed\n",
    "        except:\n",
    "            if self.shuffle_seed != None:\n",
    "                raise Exception(\"Seed must either be an integer or None\")\n",
    "\n",
    "    def order_and_shuffle(self):\n",
    "        np.random.seed(self.shuffle_seed)\n",
    "        self.order = np.array(list(self.meta_dict.keys()), dtype=int)\n",
    "        if self.shuffle_seed != None:\n",
    "            np.random.shuffle(self.order)\n",
    "\n",
    "    def open_hdf5(self):\n",
    "        self.h5file = h5py.File(self.file, 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(hdf5_file, meta_dict, dict_pos_isic_id, dict_pos_target, imgSize=100, batch_size=32, is_training=False, shuffle_seed = None, apply_hair_removal=False):\n",
    "    num_features = len(val_meta_dict[0][1]) - 2 #Subtract isic_id and target\n",
    "\n",
    "    combined_generator = hdf5_generator_all_included(hdf5_file, meta_dict, dict_pos_isic_id, dict_pos_target, num_features, imgSize, is_training, shuffle_seed, apply_hair_removal)\n",
    "\n",
    "    # Generate image dataset\n",
    "    element_spec = ((tf.TensorSpec(shape=(imgSize, imgSize, 3), dtype=tf.float32),\n",
    "                    tf.TensorSpec(shape=(1, num_features), dtype=tf.float32)),\n",
    "                    tf.TensorSpec(shape=(1, 1), dtype=tf.int32))\n",
    "\n",
    "    img_dataset = tf.data.Dataset.from_generator(\n",
    "        lambda: combined_generator,\n",
    "        output_signature=element_spec\n",
    "    )\n",
    "\n",
    "    return img_dataset.batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) Alternative dataset function to load all in memory (not a generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_in_memory_dataset(hdf5_file, meta_dict, dict_pos_isic_id, dict_pos_target, imgSize=100, batch_size=32, is_training=False, shuffle_seed=None, apply_hair_removal=False):\n",
    "    #Position of elements in the value part of the dictionary (it is a tuple with multiple elements)\n",
    "    pos_mod_toggle = 0\n",
    "    pos_metadata_array = 1\n",
    "\n",
    "    num_features = len(meta_dict[0][pos_metadata_array]) - 2  # Subtract isic_id and target columns\n",
    "\n",
    "    # Initialize lists to hold images, metadata, and targets\n",
    "    images = []\n",
    "    metas = []\n",
    "    targets = []\n",
    "\n",
    "    np.random.seed(shuffle_seed)\n",
    "    order = np.array(list(meta_dict.keys()), dtype=int)\n",
    "    if shuffle_seed is not None:\n",
    "        np.random.shuffle(order)\n",
    "\n",
    "    with h5py.File(hdf5_file, 'r') as h5file:\n",
    "        for i in range(len(meta_dict)):\n",
    "            index = order[i]\n",
    "            \n",
    "            # Retrieve target\n",
    "            target = meta_dict[index][pos_metadata_array][dict_pos_target]\n",
    "            target = np.reshape(target, (1, 1))\n",
    "            target = tf.cast(target, dtype=tf.int32)\n",
    "\n",
    "            # Retrieve metadata\n",
    "            meta = np.delete(meta_dict[index][pos_metadata_array], [dict_pos_isic_id, dict_pos_target], 0)\n",
    "            meta = meta.astype(dtype=float)\n",
    "            meta = tf.cast(meta, dtype=tf.float32)\n",
    "            meta = tf.reshape(meta, shape=(1, num_features))\n",
    "\n",
    "            try:\n",
    "                # Retrieve isic_id and load image\n",
    "                img_name = meta_dict[index][pos_metadata_array][dict_pos_isic_id]\n",
    "                \n",
    "                # Load image data from HDF5\n",
    "                img = np.array(Image.open(io.BytesIO(h5file[img_name][()])))\n",
    "\n",
    "                # Clean image\n",
    "                if apply_hair_removal:\n",
    "                    img = hair_removal(img)\n",
    "\n",
    "                # Resize the image\n",
    "                img = cv2.resize(img, (imgSize, imgSize), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "                # Apply augmentations if needed\n",
    "                mod_toggle = meta_dict[index][pos_mod_toggle]\n",
    "                if mod_toggle > 0:\n",
    "                    img=augment_image(img, mod_toggle, is_training)\n",
    "\n",
    "                # Normalize the image\n",
    "                img = tf.constant(img / 255, dtype=tf.float32)\n",
    "\n",
    "                # Add processed data to lists\n",
    "                images.append(img)\n",
    "                metas.append(meta)\n",
    "                targets.append(target)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading image {img_name}: {e}\")\n",
    "                with open('image_errors.log', 'a') as f:\n",
    "                    f.write(f\"Error loading image {img_name}: {e}\\n\")\n",
    "                continue\n",
    "\n",
    "    # Convert lists to tensors\n",
    "    images_tensor = tf.stack(images, axis=0)\n",
    "    metas_tensor = tf.stack(metas, axis=0)\n",
    "    targets_tensor = tf.stack(targets, axis=0)\n",
    "\n",
    "    # Combine the images, metadata, and targets into a tf.data.Dataset\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(((images_tensor, metas_tensor), targets_tensor))\n",
    "\n",
    "    # Batch and prefetch the dataset\n",
    "    return dataset.batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8) CNN MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 - Model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simple CNN model using only images and target (FOR TESTING)\n",
    "class CNN_model(tf.keras.Model):\n",
    "    def __init__(self, neurons = 8, activ = 'leaky_relu', img_size = 100, img_channels=3):\n",
    "        #Run the constructor of the parent class\n",
    "        super().__init__()\n",
    "\n",
    "        #Weight and bias initializers\n",
    "        kernel_initializer = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.05, seed=None)\n",
    "        bias_initializer = tf.keras.initializers.RandomUniform(minval=-0.05, maxval=0.05, seed=None)\n",
    "        \n",
    "        #Image size declaration\n",
    "        self.img_size = img_size\n",
    "        self.img_channels = img_channels\n",
    "\n",
    "        #Layers\n",
    "        self.conv1 = tf.keras.layers.Conv2D(filters=16, kernel_size=5, strides=(1, 1), activation='relu', padding='same', input_shape=(img_size, img_size, img_channels),\n",
    "                                            kernel_initializer=kernel_initializer, bias_initializer=bias_initializer)\n",
    "        self.pool1 = tf.keras.layers.MaxPool2D(pool_size=(2,2))\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.dense1 = tf.keras.layers.Dense(neurons, activation = activ, kernel_initializer=kernel_initializer, bias_initializer=bias_initializer)\n",
    "        self.dense2 = tf.keras.layers.Dense(1, activation='sigmoid', kernel_initializer=kernel_initializer, bias_initializer=bias_initializer)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x_image, x_meta = inputs\n",
    "\n",
    "        # Convolutions\n",
    "        x1 = self.conv1(x_image)\n",
    "        x1 = self.pool1(x1)\n",
    "\n",
    "        # Flattening of images for input layer\n",
    "        x1 = self.flatten(x1)\n",
    "\n",
    "        # Hidden layers of neural network\n",
    "        x1 = self.dense1(x1)\n",
    "\n",
    "        # Output layer of neural network\n",
    "        output = self.dense2(x1)\n",
    "\n",
    "        return output \n",
    "\n",
    "#Metadata Neural Network (FOR TESTING)\n",
    "class Meta_model(tf.keras.Model):\n",
    "    def __init__(self, neurons = 8, activ = 'tanh',**kwargs):\n",
    "        if kwargs:  \n",
    "            self.name=kwargs['name'] \n",
    "            \n",
    "        #Run the constructor of the parent class\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        #Weight and bias initializers\n",
    "        kernel_initializer = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.05, seed=None)\n",
    "        bias_initializer = tf.keras.initializers.RandomUniform(minval=-0.05, maxval=0.05, seed=None)\n",
    "\n",
    "        #Layers\n",
    "        self.dense1 = tf.keras.layers.Dense(neurons, activation = activ, kernel_initializer=kernel_initializer, bias_initializer=bias_initializer)\n",
    "        self.dense2 = tf.keras.layers.Dense(neurons, activation = activ, kernel_initializer=kernel_initializer, bias_initializer=bias_initializer)\n",
    "        self.dense3 = tf.keras.layers.Dense(1, activation='sigmoid', kernel_initializer=kernel_initializer, bias_initializer=bias_initializer)\n",
    "        self.dropout = tf.keras.layers.Dropout(0.25)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        x_image, x_meta = inputs\n",
    "        x_all = tf.reshape(x_meta, (tf.shape(x_meta)[0], x_meta.shape[-1]))\n",
    "        # Neural Network\n",
    "        x_all = self.dense1(x_all)\n",
    "        x_all = self.dense2(x_all)\n",
    "        if training:\n",
    "            x_all = self.dropout(x_all, training=training)\n",
    "        output = self.dense3(x_all)\n",
    "        return output\n",
    "\n",
    "#Hybrid CNN model taking metadata (FULL MODEL)\n",
    "@tf.keras.utils.register_keras_serializable(package=\"MyLayers\", name=\"KernelMult\")\n",
    "class Hybrid_model(tf.keras.Model):\n",
    "    def __init__(self, neurons = 8, dropout = 0, activ = 'leaky_relu', img_size = 100, img_channels = 3, **kwargs):\n",
    "        #Run the constructor of the parent class\n",
    "        super(). __init__(**kwargs)\n",
    "        \n",
    "        #Save inputs\n",
    "        self.neurons = neurons\n",
    "        self.dropout = dropout\n",
    "        self.activ = activ\n",
    "        self.img_size = img_size\n",
    "        self.img_channels = img_channels\n",
    "\n",
    "        #Weight and bias initializers\n",
    "        self.kernel_initializer = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.05, seed=None)\n",
    "        self.bias_initializer = tf.keras.initializers.RandomUniform(minval=-0.05, maxval=0.05, seed=None)\n",
    "\n",
    "        #Layers\n",
    "        self.conv1 = tf.keras.layers.Conv2D(filters=32, kernel_size=5, strides=(1, 1), activation='relu', padding='same', input_shape=(self.img_size, self.img_size, self.img_channels),\n",
    "                                            kernel_initializer=self.kernel_initializer, bias_initializer=self.bias_initializer)\n",
    "        self.conv2 = tf.keras.layers.Conv2D(64, 5, activation='relu', kernel_initializer=self.kernel_initializer, bias_initializer=self.bias_initializer)\n",
    "        self.pool = tf.keras.layers.MaxPool2D(pool_size=(2,2))\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.dense1 = tf.keras.layers.Dense(self.neurons, activation = self.activ, kernel_initializer=self.kernel_initializer, bias_initializer=self.bias_initializer)\n",
    "        self.dropout1 = tf.keras.layers.Dropout(self.dropout)\n",
    "        self.dense2 = tf.keras.layers.Dense(self.neurons, activation = self.activ, kernel_initializer=self.kernel_initializer, bias_initializer=self.bias_initializer)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(self.dropout)\n",
    "        self.dense3 = tf.keras.layers.Dense(1, activation='sigmoid', kernel_initializer=self.kernel_initializer, bias_initializer=self.bias_initializer)\n",
    "        self.concatenate = keras.layers.Concatenate(axis=1)\n",
    "        \n",
    "    def call(self, inputs, training=False):\n",
    "        flattened_inputs = tf.nest.flatten(inputs)\n",
    "        x_image, x_meta = flattened_inputs\n",
    "        # Convolutions\n",
    "        x = self.conv1(x_image)\n",
    "        x = self.pool(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.pool(x)\n",
    "        # Flattening of images and concatenation with other data\n",
    "        x = self.flatten(x)\n",
    "        # Reshape metadata to match dimensions\n",
    "        x_meta = tf.reshape(x_meta, (tf.shape(x_meta)[0], x_meta.shape[-1]))\n",
    "        x_all = self.concatenate([x, x_meta])\n",
    "        # Neural Network\n",
    "        x_all = self.dense1(x_all)\n",
    "        if training:\n",
    "            x_all = self.dropout1(x_all, training=training)\n",
    "        x_all = self.dense2(x_all)\n",
    "        if training:\n",
    "            x_all = self.dropout2(x_all, training=training)\n",
    "        output = self.dense3(x_all)\n",
    "        return output\n",
    "           \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            'conv1' : self.conv1,\n",
    "            'conv2' : self.conv2,\n",
    "            'pool' : self.pool,\n",
    "            'flatten' : self.flatten,\n",
    "            'dense1' : self.dense1,\n",
    "            'dropout1' : self.dropout1,\n",
    "            'dense2' : self.dense2,\n",
    "            'dropout2' : self.dropout2,\n",
    "            'dense3' : self.dense3,\n",
    "            'concatenate' : self.concatenate,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        config[\"conv1\"] = keras.layers.deserialize(config[\"conv1\"])\n",
    "        config[\"conv2\"] = keras.layers.deserialize(config[\"conv2\"])\n",
    "        config[\"pool\"] = keras.layers.deserialize(config[\"pool\"])\n",
    "        config[\"flatten\"] = keras.layers.deserialize(config[\"flatten\"])\n",
    "        config[\"dense1\"] = keras.layers.deserialize(config[\"dense1\"])\n",
    "        config[\"dropout1\"] = keras.layers.deserialize(config[\"dropout1\"])\n",
    "        config[\"dense2\"] = keras.layers.deserialize(config[\"dense2\"])\n",
    "        config[\"dropout2\"] = keras.layers.deserialize(config[\"dropout2\"])\n",
    "        config[\"dense3\"] = keras.layers.deserialize(config[\"dense3\"])\n",
    "        config[\"concatenate\"] = keras.layers.deserialize(config[\"concatenate\"])\n",
    "        return cls(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n#MANUAL DEFINITION OF LAYERS - ATTEMPT TO BE ABLE TO EXPORT MODEL - SLOWER!!!\\nfrom keras import Model\\nkernel_initializer = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.05, seed=None)\\nbias_initializer = tf.keras.initializers.RandomUniform(minval=-0.05, maxval=0.05, seed=None)\\n\\ninput_img = keras.layers.Input(shape=(100,100,3))\\nx = keras.layers.Conv2D(filters=32, kernel_size=5, strides=(1, 1), activation='relu', padding='same', input_shape=(100, 100, 3), kernel_initializer=kernel_initializer, bias_initializer=bias_initializer)(input_img)\\nx = keras.layers.Conv2D(64, 5, activation='relu', kernel_initializer=kernel_initializer, bias_initializer=bias_initializer)(x)\\nx = keras.layers.MaxPool2D(pool_size=(2,2))(x)\\nx = keras.layers.Flatten()(x)\\nx = Model(inputs=input_img, outputs=x)\\n\\ninput_meta = keras.layers.Input(shape=(1,29))\\nx_meta = keras.layers.Reshape(target_shape=([29]))(input_meta)\\n#x_meta = tf.reshape(input_meta, (tf.shape(input_meta)[0], input_meta.shape[-1]))\\nx_meta = Model(inputs=input_meta, outputs=x_meta)\\n\\ncombined = keras.layers.Concatenate(axis=1)([x.output, x_meta.output])\\n\\nx_all = keras.layers.Dense(8, activation = 'leaky_relu', kernel_initializer=kernel_initializer, bias_initializer=bias_initializer)(combined)\\nx_all = tf.keras.layers.Dropout(0.1)(x_all)\\nx_all = tf.keras.layers.Dense(8, activation = 'leaky_relu', kernel_initializer=kernel_initializer, bias_initializer=bias_initializer)(x_all)\\nx_all = tf.keras.layers.Dropout(0.1)(x_all)\\nx_all = tf.keras.layers.Dense(1, activation='sigmoid', kernel_initializer=kernel_initializer, bias_initializer=bias_initializer)(x_all)\\nmodel = Model(inputs=[input_img, input_meta], outputs=x_all)\\n\""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "#MANUAL DEFINITION OF LAYERS - ATTEMPT TO BE ABLE TO EXPORT MODEL - SLOWER!!!\n",
    "from keras import Model\n",
    "kernel_initializer = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.05, seed=None)\n",
    "bias_initializer = tf.keras.initializers.RandomUniform(minval=-0.05, maxval=0.05, seed=None)\n",
    "\n",
    "input_img = keras.layers.Input(shape=(100,100,3))\n",
    "x = keras.layers.Conv2D(filters=32, kernel_size=5, strides=(1, 1), activation='relu', padding='same', input_shape=(100, 100, 3), kernel_initializer=kernel_initializer, bias_initializer=bias_initializer)(input_img)\n",
    "x = keras.layers.Conv2D(64, 5, activation='relu', kernel_initializer=kernel_initializer, bias_initializer=bias_initializer)(x)\n",
    "x = keras.layers.MaxPool2D(pool_size=(2,2))(x)\n",
    "x = keras.layers.Flatten()(x)\n",
    "x = Model(inputs=input_img, outputs=x)\n",
    "\n",
    "input_meta = keras.layers.Input(shape=(1,29))\n",
    "x_meta = keras.layers.Reshape(target_shape=([29]))(input_meta)\n",
    "#x_meta = tf.reshape(input_meta, (tf.shape(input_meta)[0], input_meta.shape[-1]))\n",
    "x_meta = Model(inputs=input_meta, outputs=x_meta)\n",
    "\n",
    "combined = keras.layers.Concatenate(axis=1)([x.output, x_meta.output])\n",
    "\n",
    "x_all = keras.layers.Dense(8, activation = 'leaky_relu', kernel_initializer=kernel_initializer, bias_initializer=bias_initializer)(combined)\n",
    "x_all = tf.keras.layers.Dropout(0.1)(x_all)\n",
    "x_all = tf.keras.layers.Dense(8, activation = 'leaky_relu', kernel_initializer=kernel_initializer, bias_initializer=bias_initializer)(x_all)\n",
    "x_all = tf.keras.layers.Dropout(0.1)(x_all)\n",
    "x_all = tf.keras.layers.Dense(1, activation='sigmoid', kernel_initializer=kernel_initializer, bias_initializer=bias_initializer)(x_all)\n",
    "model = Model(inputs=[input_img, input_meta], outputs=x_all)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 - Model compiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-22 20:31:46.189161: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:282] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
     ]
    }
   ],
   "source": [
    "#Set seed\n",
    "tf.random.set_seed(71)\n",
    "\n",
    "#Initialize model - the full model is Hybrid_model. The others are only for testing\n",
    "#model = CNN_model(neurons=8, activ='tanh')\n",
    "model = Hybrid_model(neurons=nb_neurons_hidden_layers, dropout=dropout, activ='leaky_relu')\n",
    "#model = Meta_model(neurons=18, activ='tanh')\n",
    "\n",
    "#Define optimizer and loss function\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "loss = tf.keras.losses.BinaryCrossentropy(from_logits=False,\n",
    "                                          label_smoothing=0.0,\n",
    "                                          axis=-1,\n",
    "                                          reduction='sum_over_batch_size',\n",
    "                                          name='binary_crossentropy')\n",
    "\n",
    "#Compile the model with loss, optimizer, and metrics\n",
    "model.compile(loss = loss,\n",
    "              optimizer = optimizer,\n",
    "              metrics = [\n",
    "                  tf.keras.metrics.BinaryAccuracy(),\n",
    "                  tf.keras.metrics.FalseNegatives(),\n",
    "                  tf.keras.metrics.FalsePositives(),\n",
    "                  tf.keras.metrics.TrueNegatives(),\n",
    "                  tf.keras.metrics.TruePositives()\n",
    "                  ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3 - Model loss function weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define function to calculate weights to use in loss function\n",
    "def compute_class_weights(meta_dict, pos_target):\n",
    "    # Initialize counters for target=0 and target=1\n",
    "    target_0_count = 0\n",
    "    target_1_count = 0\n",
    "\n",
    "    # Calculate total number of images\n",
    "    total = len(meta_dict)\n",
    "    # Calculate number of target = 1 by summing the target value for each dict item\n",
    "    target_1_count = sum([meta_dict[key][1][pos_target] for key in range(total)])\n",
    "    # Calculate number of target = 1\n",
    "    target_0_count = total - target_1_count\n",
    "\n",
    "    # Calculate class weights based on the counts, avoid division by zero\n",
    "    if target_1_count > 0 :\n",
    "        if target_1_count < target_0_count:\n",
    "            weight_for_0 = 1\n",
    "            weight_for_1 = target_0_count/target_1_count\n",
    "        elif target_0_count > 0:\n",
    "            weight_for_0 = target_1_count/target_0_count\n",
    "            weight_for_1 = 1\n",
    "        else:\n",
    "            weight_for_0 = 0\n",
    "            weight_for_1=target_1_count\n",
    "    else:\n",
    "        weight_for_0 = target_0_count\n",
    "        weight_for_1 = 0\n",
    "        \n",
    "\n",
    "    return weight_for_0, weight_for_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get weights for training\n",
    "weight_for_0, weight_for_1 = compute_class_weights(train_meta_dict, train_pos_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.4 - Model Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training batches in dataset: 217\n",
      "Total validate batches in dataset: 29\n"
     ]
    }
   ],
   "source": [
    "#Determine the number of batches (includes last incomplete batch)\n",
    "nb_training_batches = int(np.ceil(len(train_meta_dict)/train_batch_size))\n",
    "nb_validate_batches = int(np.ceil(len(val_meta_dict)/val_batch_size))\n",
    "\n",
    "#Print results\n",
    "print(\"Total training batches in dataset:\", nb_training_batches)\n",
    "print(\"Total validate batches in dataset:\", nb_validate_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nimport boto3\\nimport h5py\\nimport io\\n\\n# Fonction pour charger le fichier HDF5 directement depuis S3 sans le télécharger localement\\ndef load_hdf5_from_s3(bucket_name, s3_key):\\n    s3 = boto3.client('s3')\\n    # Télécharger le fichier en mémoire\\n    obj = s3.get_object(Bucket=bucket_name, Key=s3_key)\\n    bytestream = io.BytesIO(obj['Body'].read())  # Lire le fichier en mémoire\\n    return bytestream  # Retourner le flux de mémoire\\n\\n# Utilisation des paramètres du bucket S3\\nbucket_name = 'images-projet-deep-learning'\\nhdf5_s3_key = 'train-image.hdf5'\\n\\n# Charger le fichier HDF5 directement depuis S3\\nhdf5_file_stream = load_hdf5_from_s3(bucket_name, hdf5_s3_key)\\n\\n# Charger le dataset de validation dans la mémoire (si nécessaire)\\nif save_val_in_memory:\\n    # Utiliser directement l'objet `h5file` dans `make_in_memory_dataset` sans le rouvrir\\n    val_in_memory = make_in_memory_dataset(hdf5_file_stream, val_meta_dict, val_pos_isic_id, val_pos_target, apply_hair_removal=apply_hair_removal)\\n    \\n\""
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import boto3\n",
    "import h5py\n",
    "import io\n",
    "\n",
    "# Fonction pour charger le fichier HDF5 directement depuis S3 sans le télécharger localement\n",
    "def load_hdf5_from_s3(bucket_name, s3_key):\n",
    "    s3 = boto3.client('s3')\n",
    "    # Télécharger le fichier en mémoire\n",
    "    obj = s3.get_object(Bucket=bucket_name, Key=s3_key)\n",
    "    bytestream = io.BytesIO(obj['Body'].read())  # Lire le fichier en mémoire\n",
    "    return bytestream  # Retourner le flux de mémoire\n",
    "\n",
    "# Utilisation des paramètres du bucket S3\n",
    "bucket_name = 'images-projet-deep-learning'\n",
    "hdf5_s3_key = 'train-image.hdf5'\n",
    "\n",
    "# Charger le fichier HDF5 directement depuis S3\n",
    "hdf5_file_stream = load_hdf5_from_s3(bucket_name, hdf5_s3_key)\n",
    "\n",
    "# Charger le dataset de validation dans la mémoire (si nécessaire)\n",
    "if save_val_in_memory:\n",
    "    # Utiliser directement l'objet `h5file` dans `make_in_memory_dataset` sans le rouvrir\n",
    "    val_in_memory = make_in_memory_dataset(hdf5_file_stream, val_meta_dict, val_pos_isic_id, val_pos_target, apply_hair_removal=apply_hair_removal)\n",
    "    \n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-22 20:31:48.462285: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 111120000 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "# Load validation dataset into memory to speed up the model val_loss calc\n",
    "if save_val_in_memory:\n",
    "    val_in_memory = make_in_memory_dataset(hdf5_file, val_meta_dict, val_pos_isic_id, val_pos_target, apply_hair_removal=apply_hair_removal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clear the memory leak in Keras\n",
    "class CustomCallback(tf.keras.callbacks.Callback):\n",
    "  def on_epoch_end(self, epoch, logs=None):\n",
    "    gc.collect()\n",
    "    #print(f\"Epoch {epoch+1} finished. Validation loss: {logs['val_loss']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/tf_keras/src/initializers/initializers.py:121: UserWarning: The initializer RandomNormal is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/tf_keras/src/initializers/initializers.py:121: UserWarning: The initializer RandomUniform is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "217/217 [==============================] - ETA: 0s - loss: 1.2042 - binary_accuracy: 0.7233 - false_negatives: 307.0000 - false_positives: 1608.0000 - true_negatives: 4726.0000 - true_positives: 281.0000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-22 20:33:12.992912: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 111120000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "217/217 [==============================] - 87s 279ms/step - loss: 1.2042 - binary_accuracy: 0.7233 - false_negatives: 307.0000 - false_positives: 1608.0000 - true_negatives: 4726.0000 - true_positives: 281.0000 - val_loss: 0.6041 - val_binary_accuracy: 0.7203 - val_false_negatives: 185.0000 - val_false_positives: 74.0000 - val_true_negatives: 511.0000 - val_true_positives: 156.0000\n",
      "EPOCH 2\n",
      "217/217 [==============================] - 60s 275ms/step - loss: 1.0782 - binary_accuracy: 0.7862 - false_negatives: 220.0000 - false_positives: 1260.0000 - true_negatives: 5074.0000 - true_positives: 368.0000 - val_loss: 0.4739 - val_binary_accuracy: 0.8078 - val_false_negatives: 121.0000 - val_false_positives: 57.0000 - val_true_negatives: 528.0000 - val_true_positives: 220.0000\n",
      "EPOCH 3\n",
      "217/217 [==============================] - 62s 282ms/step - loss: 0.9567 - binary_accuracy: 0.8222 - false_negatives: 189.0000 - false_positives: 1042.0000 - true_negatives: 5292.0000 - true_positives: 399.0000 - val_loss: 0.4862 - val_binary_accuracy: 0.8078 - val_false_negatives: 50.0000 - val_false_positives: 128.0000 - val_true_negatives: 457.0000 - val_true_positives: 291.0000\n",
      "EPOCH 4\n",
      "217/217 [==============================] - 60s 277ms/step - loss: 0.9209 - binary_accuracy: 0.8067 - false_negatives: 156.0000 - false_positives: 1182.0000 - true_negatives: 5152.0000 - true_positives: 432.0000 - val_loss: 0.4221 - val_binary_accuracy: 0.8218 - val_false_negatives: 106.0000 - val_false_positives: 59.0000 - val_true_negatives: 526.0000 - val_true_positives: 235.0000\n",
      "EPOCH 5\n",
      "217/217 [==============================] - 60s 277ms/step - loss: 0.9326 - binary_accuracy: 0.8071 - false_negatives: 166.0000 - false_positives: 1169.0000 - true_negatives: 5165.0000 - true_positives: 422.0000 - val_loss: 0.4944 - val_binary_accuracy: 0.8067 - val_false_negatives: 53.0000 - val_false_positives: 126.0000 - val_true_negatives: 459.0000 - val_true_positives: 288.0000\n"
     ]
    }
   ],
   "source": [
    "#Run the model through epochs\n",
    "for epoch in range(1, nb_epochs + 1):\n",
    "    #Make datasets\n",
    "    print(\"EPOCH\", epoch)\n",
    "\n",
    "    #Reinitialize the training dataset with a new shuffle each time\n",
    "    shuffle_seed = 8 + epoch #Next initialization of datasets will have a different shuffle\n",
    "    train_dataset = make_dataset(hdf5_file, train_meta_dict, train_pos_isic_id, train_pos_target, batch_size = train_batch_size, is_training=True, shuffle_seed=shuffle_seed, apply_hair_removal=apply_hair_removal)\n",
    "\n",
    "    #Fit the model, using validation data either stored in memory or on the hard disk\n",
    "    if save_val_in_memory:\n",
    "        mod = model.fit(train_dataset, epochs=1, steps_per_epoch = nb_training_batches, validation_data = val_in_memory, callbacks = [CustomCallback()],\n",
    "                        class_weight={0: weight_for_0, 1: weight_for_1})\n",
    "    else:\n",
    "        val_dataset = make_dataset(hdf5_file, val_meta_dict, val_pos_isic_id, val_pos_target, batch_size = val_batch_size, is_training=False, shuffle_seed=shuffle_seed, apply_hair_removal=apply_hair_removal)\n",
    "        mod = model.fit(train_dataset, epochs=1, steps_per_epoch = nb_training_batches, validation_data = val_dataset, validation_steps = nb_validate_batches, callbacks = [CustomCallback()],\n",
    "                        class_weight={0: weight_for_0, 1: weight_for_1})\n",
    "    \n",
    "    #Save results\n",
    "    if epoch == 1:\n",
    "        results = mod.history\n",
    "    else:\n",
    "        for key in mod.history:   \n",
    "            results[key] += mod.history[key]\n",
    "            \n",
    "    #Export model structure and weights (json and H5) - NOT WORKING\n",
    "    '''    \n",
    "    #Save occasionally\n",
    "    if epoch == 5:\n",
    "        model_json = model.to_json()\n",
    "        with open(savePath + \"model.json\", \"w\") as json_file:\n",
    "            json_file.write(model_json)\n",
    "\n",
    "    if (epoch % wt_save_freq == 0):\n",
    "        now = datetime.datetime.now()\n",
    "\n",
    "        if apply_hair_removal:\n",
    "            modifier = \"with_hair_removal_\"\n",
    "        else:\n",
    "            modifier = \"no_hair_removal_\"\n",
    "        #filename = \"Model_\" + modifier + \"Epoch_\" + str(epoch) + \"_\" + now.strftime(\"%Y-%m-%d_%Hh%Mm%Ss\") + \".weights.h5\"\n",
    "        filename = \"Model\" + \".weights.h5\"\n",
    "        model.save_weights(savePath + filename)\n",
    "    '''    \n",
    "\n",
    "    #Clean memory after use\n",
    "    del mod\n",
    "    del train_dataset\n",
    "    #If save_val_in_memory is not true, we have a generator. In this case, we want to delete the generator.\n",
    "    if save_val_in_memory != True:\n",
    "        del val_dataset\n",
    "    tf.keras.backend.clear_session()\n",
    "    gc.collect()\n",
    "\n",
    "    #Early termination (check after 15 epochs)\n",
    "    if epoch >= 15 and early_break == True:\n",
    "        #Calculate previous three changes, if positive, then loss is increasing\n",
    "        change1 = results[\"val_loss\"][-1] - results[\"val_loss\"][-2]\n",
    "        change2 = results[\"val_loss\"][-2] - results[\"val_loss\"][-3]\n",
    "        change3 = results[\"val_loss\"][-3] - results[\"val_loss\"][-4]\n",
    "\n",
    "        #Three consecutive increases in validation loss will stop the model\n",
    "        if change1 > 0 and change2 > 0 and change3 > 0:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"hybrid_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             multiple                  2432      \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           multiple                  51264     \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  multiple                  0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " flatten (Flatten)           multiple                  0         \n",
      "                                                                 \n",
      " dense (Dense)               multiple                  1219896   \n",
      "                                                                 \n",
      " dropout (Dropout)           multiple                  0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             multiple                  1332      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         multiple                  0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             multiple                  37        \n",
      "                                                                 \n",
      " concatenate (Concatenate)   multiple                  0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1274961 (4.86 MB)\n",
      "Trainable params: 1274961 (4.86 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_ = tf.keras.models.load_model(\"save1.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n#Import results from file\\nimported_results = pd.read_csv(modelResPath).to_dict(orient='list')\\n\""
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Save results to file\n",
    "pd.DataFrame.from_dict(results).to_csv(modelResPath, index=False)\n",
    "\n",
    "\"\"\"\n",
    "#Import results from file\n",
    "imported_results = pd.read_csv(modelResPath).to_dict(orient='list')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'hybrid_model/conv2d/kernel:0' shape=(5, 5, 3, 32) dtype=float32, numpy=\n",
       " array([[[[-0.09321486, -0.00034968, -0.07952068, ..., -0.10088284,\n",
       "            0.06613299,  0.02432221],\n",
       "          [ 0.01848743, -0.04244754, -0.1006982 , ..., -0.09881319,\n",
       "            0.0328213 , -0.12314083],\n",
       "          [-0.04138722, -0.01584755, -0.09878173, ..., -0.07682215,\n",
       "           -0.06895993, -0.07896694]],\n",
       " \n",
       "         [[-0.08856352, -0.03791987, -0.03189535, ..., -0.08230435,\n",
       "            0.02023908, -0.11186402],\n",
       "          [ 0.00023363, -0.01545649,  0.00907223, ..., -0.06130867,\n",
       "            0.02375645, -0.07746928],\n",
       "          [-0.136271  , -0.07163994, -0.11319935, ..., -0.00297736,\n",
       "           -0.03749488, -0.13206805]],\n",
       " \n",
       "         [[-0.03856353, -0.03666678, -0.0581926 , ..., -0.02112424,\n",
       "            0.01255411, -0.0329218 ],\n",
       "          [-0.08643143,  0.04930909, -0.02645641, ..., -0.14168586,\n",
       "           -0.02060019, -0.07004733],\n",
       "          [ 0.01624137,  0.03390079, -0.09006553, ..., -0.07111495,\n",
       "            0.00285913, -0.02738358]],\n",
       " \n",
       "         [[-0.03872238, -0.06875217, -0.08105146, ..., -0.0126169 ,\n",
       "            0.06348444, -0.06739412],\n",
       "          [-0.01207103, -0.1249525 , -0.04345057, ..., -0.04079764,\n",
       "           -0.07391923, -0.0918379 ],\n",
       "          [ 0.02990064, -0.03740746, -0.04415537, ..., -0.15978281,\n",
       "            0.03362556, -0.09821747]],\n",
       " \n",
       "         [[ 0.01567476, -0.08567075, -0.06926304, ..., -0.01934629,\n",
       "           -0.04495891, -0.12289274],\n",
       "          [-0.01039167, -0.03261534, -0.04475459, ..., -0.0370279 ,\n",
       "           -0.00287038, -0.07144633],\n",
       "          [-0.00045831,  0.04242313, -0.15674147, ..., -0.06924481,\n",
       "           -0.01180101, -0.02973428]]],\n",
       " \n",
       " \n",
       "        [[[-0.01848491, -0.02521936,  0.05570323, ...,  0.02240435,\n",
       "           -0.01466565, -0.01775484],\n",
       "          [-0.04845646, -0.04739226,  0.03151358, ..., -0.03273635,\n",
       "            0.02879384,  0.00156063],\n",
       "          [-0.14043756, -0.05662889, -0.12814228, ..., -0.11660378,\n",
       "            0.05040383,  0.0267129 ]],\n",
       " \n",
       "         [[ 0.01228679,  0.01510617, -0.03860662, ..., -0.03828534,\n",
       "           -0.01139285, -0.07292911],\n",
       "          [-0.00297174, -0.10031781, -0.10031357, ..., -0.1294327 ,\n",
       "            0.03246664, -0.04994739],\n",
       "          [ 0.01216069, -0.01082449,  0.06145772, ..., -0.06114992,\n",
       "            0.08406266, -0.088774  ]],\n",
       " \n",
       "         [[-0.01222998,  0.07299803, -0.10269613, ..., -0.04412556,\n",
       "           -0.04438965, -0.03504059],\n",
       "          [-0.11436607, -0.07968436, -0.09255187, ..., -0.1180525 ,\n",
       "            0.01103952, -0.02870069],\n",
       "          [-0.03576362, -0.03573918, -0.08690457, ..., -0.08779493,\n",
       "            0.12662108, -0.1278438 ]],\n",
       " \n",
       "         [[ 0.02706316, -0.0331596 , -0.05509054, ..., -0.00891449,\n",
       "           -0.0084031 , -0.07006446],\n",
       "          [-0.00055591, -0.02784924,  0.02528011, ..., -0.09493739,\n",
       "            0.00186446, -0.00508145],\n",
       "          [-0.07064749,  0.02759714, -0.08861276, ..., -0.03060698,\n",
       "            0.00903666, -0.01821001]],\n",
       " \n",
       "         [[-0.10471121, -0.07398367, -0.06404088, ...,  0.02629471,\n",
       "           -0.0364566 , -0.00243033],\n",
       "          [-0.1419593 , -0.04040652, -0.19627556, ...,  0.00852531,\n",
       "            0.14336936,  0.00691204],\n",
       "          [-0.12295476, -0.05189707, -0.07573101, ..., -0.05325058,\n",
       "           -0.03861953,  0.0027574 ]]],\n",
       " \n",
       " \n",
       "        [[[-0.06128588,  0.05335141, -0.02482759, ..., -0.07660298,\n",
       "            0.05813901, -0.07695147],\n",
       "          [-0.02094216, -0.04935397, -0.02820041, ..., -0.01865467,\n",
       "            0.06263095,  0.05499419],\n",
       "          [-0.07304068, -0.06177389, -0.05262186, ..., -0.03074658,\n",
       "            0.08031116, -0.05715033]],\n",
       " \n",
       "         [[-0.0337082 , -0.09176318, -0.04283063, ...,  0.06033621,\n",
       "            0.0110882 , -0.01428899],\n",
       "          [-0.08662159, -0.07253442, -0.17778768, ..., -0.08285487,\n",
       "            0.0267227 , -0.03734688],\n",
       "          [-0.07784656, -0.04704368, -0.0571991 , ..., -0.06582742,\n",
       "           -0.06938797, -0.04585763]],\n",
       " \n",
       "         [[-0.0419921 , -0.00251128, -0.03148537, ..., -0.02754306,\n",
       "            0.00695544, -0.0088493 ],\n",
       "          [-0.04979654, -0.07038273, -0.02641108, ..., -0.03969836,\n",
       "           -0.02592253, -0.08929478],\n",
       "          [-0.10409086, -0.0460747 , -0.08440785, ..., -0.04536396,\n",
       "           -0.01672773, -0.06579191]],\n",
       " \n",
       "         [[-0.05226201, -0.02852288, -0.12339484, ..., -0.1283826 ,\n",
       "            0.07147516, -0.07496228],\n",
       "          [-0.06778186, -0.03733448, -0.09820928, ..., -0.08368029,\n",
       "           -0.005907  , -0.1193068 ],\n",
       "          [-0.08197229, -0.03740506, -0.13106352, ..., -0.08107336,\n",
       "           -0.04663159, -0.09537451]],\n",
       " \n",
       "         [[-0.07265633,  0.0082662 , -0.11334512, ..., -0.13012767,\n",
       "           -0.01435457, -0.08953645],\n",
       "          [-0.11286674, -0.03380504, -0.03303602, ..., -0.18948197,\n",
       "            0.03101607, -0.08601636],\n",
       "          [-0.0489536 ,  0.00586716, -0.06781742, ..., -0.10659759,\n",
       "            0.03704732, -0.08860377]]],\n",
       " \n",
       " \n",
       "        [[[-0.10175445, -0.03206935,  0.02619272, ...,  0.02148443,\n",
       "           -0.04564514, -0.13878855],\n",
       "          [-0.07130015, -0.00055167, -0.05625999, ..., -0.1284224 ,\n",
       "           -0.07803308, -0.06618164],\n",
       "          [ 0.04678517, -0.05178633, -0.13195796, ..., -0.15743573,\n",
       "           -0.04143466, -0.09022722]],\n",
       " \n",
       "         [[-0.13393775, -0.01229578,  0.03943266, ..., -0.00949811,\n",
       "            0.00063955, -0.03196502],\n",
       "          [-0.11559084,  0.03672206,  0.01880724, ..., -0.05864229,\n",
       "            0.06229686, -0.0977443 ],\n",
       "          [-0.080009  , -0.0517336 , -0.07780792, ..., -0.0009089 ,\n",
       "            0.03037948, -0.02747716]],\n",
       " \n",
       "         [[-0.02264584,  0.02322073, -0.06929091, ..., -0.07552884,\n",
       "            0.02042277, -0.15198897],\n",
       "          [-0.06129893, -0.19160499, -0.11841755, ..., -0.07620266,\n",
       "            0.07685275, -0.0946323 ],\n",
       "          [-0.05710949, -0.08844626, -0.05453369, ..., -0.09666777,\n",
       "           -0.03470716, -0.16636324]],\n",
       " \n",
       "         [[-0.00114729, -0.10051791, -0.04640162, ..., -0.12849501,\n",
       "           -0.03418173,  0.00334213],\n",
       "          [-0.04955853, -0.05164994, -0.11698138, ..., -0.07784384,\n",
       "           -0.04988722, -0.05238752],\n",
       "          [-0.11547884, -0.05558036,  0.01411294, ..., -0.08535211,\n",
       "            0.08236508, -0.07699573]],\n",
       " \n",
       "         [[-0.11360598, -0.00793931, -0.09987953, ..., -0.01343725,\n",
       "            0.03914804, -0.12229563],\n",
       "          [-0.01963776, -0.00072686, -0.08365809, ..., -0.04646415,\n",
       "            0.00892518, -0.118323  ],\n",
       "          [-0.10342972, -0.12339917, -0.14116387, ..., -0.09368751,\n",
       "            0.06147412,  0.05815216]]],\n",
       " \n",
       " \n",
       "        [[[ 0.00636669, -0.03673116, -0.05804155, ..., -0.06589285,\n",
       "            0.06962471, -0.0609805 ],\n",
       "          [-0.14605415, -0.03211014, -0.10846074, ...,  0.07002594,\n",
       "           -0.1108335 , -0.04272438],\n",
       "          [-0.01507975, -0.02229691,  0.01463269, ..., -0.04188497,\n",
       "           -0.01925747, -0.01886372]],\n",
       " \n",
       "         [[-0.06047499, -0.04575133, -0.02961485, ..., -0.04255265,\n",
       "            0.02458052, -0.03121564],\n",
       "          [-0.10305566, -0.00942547, -0.00371739, ..., -0.04320353,\n",
       "            0.04431093,  0.0458597 ],\n",
       "          [-0.06073394, -0.04933804, -0.08457471, ...,  0.08484252,\n",
       "           -0.14061864, -0.0601958 ]],\n",
       " \n",
       "         [[-0.13412757, -0.06470002, -0.16855013, ..., -0.1075036 ,\n",
       "            0.09522994, -0.10127322],\n",
       "          [-0.00061729, -0.08884432, -0.10308764, ..., -0.0789722 ,\n",
       "            0.04613379, -0.0590893 ],\n",
       "          [-0.13401635,  0.03202497, -0.05190421, ..., -0.13232483,\n",
       "           -0.05131936, -0.10920512]],\n",
       " \n",
       "         [[-0.04939718,  0.0004822 , -0.0153499 , ..., -0.10296288,\n",
       "           -0.01270245, -0.10974329],\n",
       "          [-0.06687042, -0.04361623, -0.11699083, ..., -0.06301656,\n",
       "            0.03006903, -0.02246518],\n",
       "          [-0.05862167, -0.0277964 , -0.03294239, ..., -0.059121  ,\n",
       "            0.00954535, -0.01232462]],\n",
       " \n",
       "         [[-0.06706648,  0.00877655, -0.11315966, ..., -0.17868404,\n",
       "            0.0726485 ,  0.03328641],\n",
       "          [ 0.00187688, -0.00888979, -0.11782774, ..., -0.03266082,\n",
       "            0.04323217, -0.00896484],\n",
       "          [-0.08053633,  0.00889371, -0.11475489, ..., -0.02747455,\n",
       "           -0.020975  , -0.12450656]]]], dtype=float32)>,\n",
       " <tf.Variable 'hybrid_model/conv2d/bias:0' shape=(32,) dtype=float32, numpy=\n",
       " array([-0.09045123, -0.08184609, -0.06036381, -0.12180154, -0.07071811,\n",
       "        -0.00600219, -0.04313768, -0.07250369, -0.09042642, -0.10708867,\n",
       "        -0.04090076, -0.03957737, -0.06288949, -0.08460402, -0.01966247,\n",
       "        -0.07776889, -0.04431134, -0.05536081, -0.03591889, -0.04036257,\n",
       "        -0.05228951, -0.10766986, -0.06855224, -0.06133678, -0.05847547,\n",
       "         0.0067485 , -0.05548044, -0.04397595, -0.04361735, -0.10624598,\n",
       "         0.04774065, -0.05767563], dtype=float32)>,\n",
       " <tf.Variable 'hybrid_model/conv2d_1/kernel:0' shape=(5, 5, 32, 64) dtype=float32, numpy=\n",
       " array([[[[ 2.66422313e-02, -2.32228395e-02,  4.06804234e-02, ...,\n",
       "           -9.90255550e-02, -1.27182901e-02, -3.07062780e-03],\n",
       "          [-4.76529039e-02, -4.34331074e-02, -9.54891443e-02, ...,\n",
       "           -8.82169977e-02, -7.73941493e-03, -1.15219072e-01],\n",
       "          [ 1.17297389e-01,  7.10545629e-02,  1.76073033e-02, ...,\n",
       "            1.09726861e-01, -8.20429251e-02, -2.80208532e-02],\n",
       "          ...,\n",
       "          [ 9.07009169e-02,  7.39965737e-02, -7.97155872e-02, ...,\n",
       "           -1.03900343e-01,  1.82919316e-02,  1.73859969e-01],\n",
       "          [ 1.05279759e-02, -9.05042663e-02, -3.36344093e-02, ...,\n",
       "            5.71685359e-02, -1.69886231e-01, -3.21435146e-02],\n",
       "          [ 1.04953572e-01, -5.35229035e-02,  2.54201591e-02, ...,\n",
       "           -4.32661623e-02, -2.77554505e-02,  8.88374597e-02]],\n",
       " \n",
       "         [[ 1.70042142e-02, -3.68042774e-02,  1.06873438e-01, ...,\n",
       "            8.38468969e-02, -1.28148705e-01,  5.98654784e-02],\n",
       "          [-1.40410259e-01, -8.22542831e-02, -1.32619992e-01, ...,\n",
       "           -8.49271044e-02,  4.35988903e-02, -6.22185245e-02],\n",
       "          [-2.63870265e-02,  4.14934102e-03,  6.85027614e-02, ...,\n",
       "            1.01835085e-02, -4.09910232e-02, -1.07799619e-01],\n",
       "          ...,\n",
       "          [-7.27772489e-02,  6.75839782e-02, -5.57272173e-02, ...,\n",
       "           -1.38653610e-02,  1.27457455e-02, -1.37201041e-01],\n",
       "          [-7.68939927e-02, -9.77397785e-02, -6.17027245e-02, ...,\n",
       "           -6.25780001e-02, -6.17978685e-02, -2.10562646e-02],\n",
       "          [ 8.19231644e-02, -8.36695507e-02,  8.84390771e-02, ...,\n",
       "           -8.14936683e-02, -3.33330035e-02,  6.91823959e-02]],\n",
       " \n",
       "         [[ 6.95944997e-03, -1.51454300e-01, -7.12540671e-02, ...,\n",
       "            7.27386028e-02, -4.46261372e-03, -1.24960504e-02],\n",
       "          [ 6.25815913e-02, -1.21061020e-01, -2.61194017e-02, ...,\n",
       "           -3.75154056e-02, -3.23822014e-02, -2.59858631e-02],\n",
       "          [-7.48295784e-02, -1.10123254e-01, -5.09496368e-02, ...,\n",
       "            6.23721965e-02, -7.25454418e-03,  1.37162969e-01],\n",
       "          ...,\n",
       "          [-3.83943170e-02, -4.64213453e-02, -8.94399881e-02, ...,\n",
       "           -5.83703965e-02,  2.95460168e-02,  1.26662701e-02],\n",
       "          [ 5.17670810e-03, -1.32482335e-01, -1.25401109e-01, ...,\n",
       "           -7.28582293e-02,  1.13690114e-02, -2.92489380e-02],\n",
       "          [-4.26712781e-02, -1.00746136e-02,  3.61797139e-02, ...,\n",
       "           -3.64093371e-02, -4.32896279e-02,  8.32516029e-02]],\n",
       " \n",
       "         [[ 5.17918095e-02, -1.25370607e-01,  2.83150859e-02, ...,\n",
       "           -6.65352941e-02, -4.44312505e-02,  3.33586670e-02],\n",
       "          [-5.58636114e-02, -7.91246668e-02, -1.04003601e-01, ...,\n",
       "           -1.08741894e-01, -7.69391656e-02, -4.48712148e-02],\n",
       "          [ 2.93561071e-03, -7.41811469e-02, -4.16516401e-02, ...,\n",
       "            2.13813595e-02,  3.72318961e-02, -1.81927923e-02],\n",
       "          ...,\n",
       "          [ 3.89985032e-02, -4.83085178e-02,  2.59036925e-02, ...,\n",
       "            5.75959794e-02,  1.46179767e-02,  4.42628674e-02],\n",
       "          [-2.21015215e-01, -4.51703109e-02, -7.75696263e-02, ...,\n",
       "           -4.95875478e-02, -7.17186481e-02,  8.72915685e-02],\n",
       "          [-4.05173786e-02, -3.36081199e-02, -1.66589506e-02, ...,\n",
       "           -7.23159164e-02,  2.71290503e-02,  2.44960397e-01]],\n",
       " \n",
       "         [[ 4.56356816e-02, -5.79832494e-02, -1.20225539e-02, ...,\n",
       "           -6.21332265e-02,  8.98248237e-03,  1.07206866e-01],\n",
       "          [-1.27244189e-01, -1.05608597e-01, -9.66274440e-02, ...,\n",
       "           -4.72226785e-03,  1.93851255e-02, -1.09861523e-01],\n",
       "          [ 7.72471949e-02, -4.76287790e-02, -3.74721475e-02, ...,\n",
       "           -1.37973160e-01, -4.67848554e-02,  1.30142987e-01],\n",
       "          ...,\n",
       "          [ 1.44220084e-01, -1.72239810e-01,  2.22073663e-02, ...,\n",
       "           -8.58795829e-03,  2.99179237e-02,  2.58356109e-02],\n",
       "          [-5.09994105e-02, -1.21267676e-01, -3.39428708e-02, ...,\n",
       "           -9.74655300e-02, -6.13359287e-02, -3.15411687e-02],\n",
       "          [ 1.22949295e-02, -2.00824440e-02, -6.02717623e-02, ...,\n",
       "           -5.18570244e-02, -3.06547340e-02,  1.49158880e-01]]],\n",
       " \n",
       " \n",
       "        [[[ 1.52179927e-01, -7.90050998e-02, -1.89888701e-02, ...,\n",
       "           -7.34501034e-02, -5.12603596e-02, -1.92049947e-02],\n",
       "          [ 1.16060385e-02,  4.56548445e-02, -8.00946206e-02, ...,\n",
       "           -3.13487425e-02,  1.84844099e-02, -8.13026428e-02],\n",
       "          [ 6.34165406e-02,  7.28329420e-02, -1.41365945e-01, ...,\n",
       "            9.67516527e-02, -7.86097124e-02,  1.46699443e-01],\n",
       "          ...,\n",
       "          [ 4.78465715e-03,  9.90636796e-02, -3.44745144e-02, ...,\n",
       "           -6.92364275e-02, -4.17295436e-04,  4.77253795e-02],\n",
       "          [-2.79600080e-02, -8.62249583e-02, -5.28513268e-03, ...,\n",
       "           -4.75171779e-04, -3.88245508e-02, -7.26573318e-02],\n",
       "          [ 4.09555957e-02, -3.47995535e-02, -8.18777364e-03, ...,\n",
       "           -3.05364598e-02, -3.19335498e-02,  4.85116653e-02]],\n",
       " \n",
       "         [[ 3.26196104e-02, -6.56917468e-02,  1.01125754e-01, ...,\n",
       "           -3.87836546e-02, -5.29150926e-02,  9.93454903e-02],\n",
       "          [-1.09196648e-01,  6.37659803e-02, -5.91873331e-03, ...,\n",
       "           -8.70483816e-02,  6.33720085e-02, -6.17528930e-02],\n",
       "          [ 5.54234125e-02, -9.58577245e-02,  1.30631939e-01, ...,\n",
       "            3.49433832e-02, -6.27136901e-02, -3.59134786e-02],\n",
       "          ...,\n",
       "          [-3.92564833e-02,  4.04368639e-02,  5.01611903e-02, ...,\n",
       "           -6.69787303e-02, -5.95843792e-02, -1.24713928e-02],\n",
       "          [ 1.80203153e-03, -2.57201027e-02, -2.13312823e-02, ...,\n",
       "           -6.68598861e-02,  3.95748205e-02, -1.41884815e-02],\n",
       "          [ 4.22563218e-02, -1.37242511e-01,  4.08574641e-02, ...,\n",
       "           -2.24765223e-02, -4.95647267e-02, -6.75000250e-03]],\n",
       " \n",
       "         [[ 1.14697404e-01, -7.58184120e-02, -3.18312757e-02, ...,\n",
       "           -2.05936469e-02,  7.45061487e-02,  5.78691065e-02],\n",
       "          [-9.90219489e-02,  9.18347985e-02,  8.24762799e-04, ...,\n",
       "           -9.02770907e-02,  4.62263376e-02,  3.00982804e-03],\n",
       "          [ 1.24246903e-01, -5.36970459e-02, -4.00908552e-02, ...,\n",
       "           -3.87852378e-02,  1.36094615e-02,  5.17682545e-02],\n",
       "          ...,\n",
       "          [-1.08478531e-01, -4.08287439e-03,  7.85550382e-03, ...,\n",
       "           -1.57035485e-01, -8.78848061e-02, -5.83841503e-02],\n",
       "          [-8.71982202e-02, -1.08945169e-01, -1.47880927e-01, ...,\n",
       "            6.92498498e-03, -7.88399577e-02, -1.44930735e-01],\n",
       "          [ 4.59732711e-02, -1.14989489e-01,  4.00184765e-02, ...,\n",
       "            5.10618137e-03,  2.64974982e-02,  6.65830523e-02]],\n",
       " \n",
       "         [[ 6.11879937e-02, -1.28019109e-01,  1.27730994e-02, ...,\n",
       "           -6.08473122e-02, -1.06461560e-02,  9.33889598e-02],\n",
       "          [-1.34942025e-01, -5.59698492e-02, -7.65721425e-02, ...,\n",
       "            2.16826936e-03,  6.40189573e-02, -9.91431177e-02],\n",
       "          [ 5.75572215e-02, -8.03964958e-02,  5.01465946e-02, ...,\n",
       "            3.23858745e-02, -1.18913710e-01,  1.25366613e-01],\n",
       "          ...,\n",
       "          [ 1.16436258e-02, -7.03427801e-03, -9.75323934e-03, ...,\n",
       "            1.08158939e-01, -4.79227789e-02,  7.47469366e-02],\n",
       "          [-2.35613957e-02, -4.19044457e-02, -6.13912307e-02, ...,\n",
       "           -9.98945674e-04,  1.60951596e-02, -1.31121174e-01],\n",
       "          [-4.21580784e-02, -8.55880082e-02,  7.65466015e-04, ...,\n",
       "           -8.81269574e-02, -8.29726905e-02,  1.01659156e-01]],\n",
       " \n",
       "         [[ 7.23217055e-02,  2.31562871e-02, -4.68926802e-02, ...,\n",
       "           -9.59501192e-02, -6.77430257e-02,  2.55130213e-02],\n",
       "          [-5.36075756e-02, -5.90840466e-02, -2.51922514e-02, ...,\n",
       "           -1.03607895e-02,  1.52478907e-02, -1.20402128e-01],\n",
       "          [ 5.18712848e-02,  1.10185444e-02, -1.43854162e-02, ...,\n",
       "           -3.50675359e-02, -1.99125465e-02,  3.40457857e-02],\n",
       "          ...,\n",
       "          [ 6.87189102e-02, -5.41667156e-02, -4.62221839e-02, ...,\n",
       "           -7.55921304e-02, -5.11693060e-02,  1.04988940e-01],\n",
       "          [-1.18527606e-01, -8.80963057e-02, -9.11784247e-02, ...,\n",
       "           -1.02271885e-01, -8.89957100e-02, -8.05926174e-02],\n",
       "          [ 1.38136178e-01, -8.56118202e-02, -7.08352551e-02, ...,\n",
       "            7.16374209e-03, -3.27200070e-02,  8.58011544e-02]]],\n",
       " \n",
       " \n",
       "        [[[-7.84837321e-05, -9.83391404e-02,  1.01464219e-01, ...,\n",
       "           -3.12854238e-02, -7.75188208e-02,  2.90037785e-02],\n",
       "          [-1.69180240e-02,  2.95369197e-02, -6.31519215e-05, ...,\n",
       "           -2.76448131e-02, -2.60367477e-03, -7.91136548e-02],\n",
       "          [ 3.08585051e-03,  7.92073384e-02, -1.22795165e-01, ...,\n",
       "            5.89213222e-02,  6.63051829e-02,  1.06156126e-01],\n",
       "          ...,\n",
       "          [-2.85723694e-02,  1.05158150e-01, -5.90186417e-02, ...,\n",
       "           -8.81714672e-02, -6.61431327e-02,  5.81286773e-02],\n",
       "          [-1.04099095e-01,  2.96785105e-02, -8.39279667e-02, ...,\n",
       "           -6.95107207e-02, -1.16504475e-01, -1.00728638e-01],\n",
       "          [ 8.71652216e-02, -1.00650087e-01, -6.65733367e-02, ...,\n",
       "           -7.40003809e-02, -2.77740266e-02,  1.80637464e-02]],\n",
       " \n",
       "         [[ 1.07575454e-01, -9.56549868e-02,  4.46975380e-02, ...,\n",
       "            3.21741663e-02, -1.06087690e-02,  1.22226261e-01],\n",
       "          [-6.32364079e-02,  4.51026559e-02, -1.79934464e-02, ...,\n",
       "           -5.26519604e-02,  3.07446457e-02, -1.10431211e-02],\n",
       "          [ 9.17200670e-02, -8.00496247e-03,  1.12682516e-02, ...,\n",
       "           -1.85414497e-02,  2.79806573e-02, -4.86534871e-02],\n",
       "          ...,\n",
       "          [-6.62685037e-02,  9.99492779e-02,  5.26065528e-02, ...,\n",
       "           -3.59195508e-02,  1.08937584e-01, -1.12498924e-01],\n",
       "          [ 1.14005562e-02, -9.04751346e-02, -5.09051606e-02, ...,\n",
       "           -1.12165257e-01,  3.40983295e-03, -5.87392412e-02],\n",
       "          [ 1.67193089e-03, -6.68199658e-02,  4.30417545e-02, ...,\n",
       "            4.75426577e-02, -1.98038910e-02,  1.62360758e-01]],\n",
       " \n",
       "         [[ 2.33371742e-02, -2.56072599e-02, -5.12284925e-03, ...,\n",
       "           -7.25965947e-02,  5.95457219e-02,  3.76831964e-02],\n",
       "          [ 7.91697279e-02,  7.36773387e-02,  1.59467105e-02, ...,\n",
       "           -3.17013785e-02, -3.15614417e-02, -5.70476204e-02],\n",
       "          [ 9.65695530e-02, -1.05371274e-01,  3.49330232e-02, ...,\n",
       "            7.81492442e-02,  3.26141864e-02,  5.15141524e-02],\n",
       "          ...,\n",
       "          [-1.33579984e-01, -2.89073326e-02,  3.52432877e-02, ...,\n",
       "           -2.87843421e-02, -7.49790072e-02, -6.72335476e-02],\n",
       "          [-4.84282225e-02,  4.77026450e-03, -1.01852491e-01, ...,\n",
       "           -2.05030553e-02, -5.16798757e-02, -1.06610447e-01],\n",
       "          [ 1.19031481e-01, -4.84557450e-02,  8.93384442e-02, ...,\n",
       "           -7.13464916e-02,  2.10978463e-02,  3.04463245e-02]],\n",
       " \n",
       "         [[ 6.90723136e-02,  5.07363002e-04, -4.99105379e-02, ...,\n",
       "           -5.24549074e-02,  3.95259000e-02,  5.20983897e-02],\n",
       "          [-5.01782186e-02,  6.31399229e-02, -1.86496489e-02, ...,\n",
       "           -1.57693818e-01, -1.07722562e-02,  4.62831557e-03],\n",
       "          [ 2.62988433e-02, -8.18422511e-02, -7.15362430e-02, ...,\n",
       "            1.07029090e-02, -8.56529251e-02,  3.91301550e-02],\n",
       "          ...,\n",
       "          [-3.62593159e-02, -1.87816881e-02, -1.30091412e-02, ...,\n",
       "            9.08256844e-02, -6.99926633e-04,  4.72812690e-02],\n",
       "          [ 1.65137090e-02, -1.15672790e-01, -1.30676612e-01, ...,\n",
       "           -2.92014927e-02, -3.61240492e-03, -5.08797467e-02],\n",
       "          [ 1.12593785e-01, -4.87308502e-02,  2.51898281e-02, ...,\n",
       "           -2.25545745e-02, -1.58747207e-04,  4.04029991e-03]],\n",
       " \n",
       "         [[ 3.74489613e-02, -9.57224816e-02, -2.46190578e-02, ...,\n",
       "           -4.15797010e-02, -1.24746189e-02,  5.84879071e-02],\n",
       "          [-4.72655632e-02, -6.50100484e-02, -4.72998880e-02, ...,\n",
       "           -3.50516699e-02, -8.06140900e-02,  3.04533746e-02],\n",
       "          [ 7.07964301e-02, -5.06225713e-02, -2.85569634e-02, ...,\n",
       "           -1.11884125e-01, -5.49393408e-02,  8.66298303e-02],\n",
       "          ...,\n",
       "          [ 4.32342030e-02, -2.62156054e-02, -3.55021581e-02, ...,\n",
       "           -1.30839258e-01, -6.30065203e-02, -7.68603235e-02],\n",
       "          [-7.73828477e-02, -1.65039152e-02, -1.31772965e-01, ...,\n",
       "           -7.42844567e-02, -9.07888636e-02, -1.23639025e-01],\n",
       "          [-2.62485798e-02, -1.38497343e-02,  2.25387444e-03, ...,\n",
       "           -1.21207476e-01, -2.53092535e-02,  8.50545764e-02]]],\n",
       " \n",
       " \n",
       "        [[[ 2.59346124e-02, -7.82852024e-02,  8.47460106e-02, ...,\n",
       "           -6.40469790e-02, -1.03752069e-01,  1.07003227e-01],\n",
       "          [ 3.06179151e-02, -1.86423343e-02, -1.22725420e-01, ...,\n",
       "           -1.58961564e-01, -4.97780517e-02,  4.98289661e-03],\n",
       "          [ 2.12859530e-02,  8.15262049e-02, -1.21044606e-01, ...,\n",
       "           -1.15195483e-01,  5.26215844e-02,  7.00580925e-02],\n",
       "          ...,\n",
       "          [ 5.67069575e-02,  5.47566451e-02, -2.36808099e-02, ...,\n",
       "           -4.97921295e-02, -9.88793224e-02,  8.35905299e-02],\n",
       "          [-2.91439239e-02, -1.52435839e-01, -1.30821392e-01, ...,\n",
       "           -9.09556597e-02, -5.77779040e-02, -1.16417194e-02],\n",
       "          [ 2.27231160e-02, -2.28476617e-02, -3.05820927e-02, ...,\n",
       "            1.69619434e-02, -5.37472554e-02,  6.65625706e-02]],\n",
       " \n",
       "         [[ 5.76779284e-02,  2.42919512e-02, -1.06214667e-02, ...,\n",
       "           -1.06199704e-01,  8.07672143e-02,  7.58384541e-02],\n",
       "          [-4.59570158e-03,  9.99378487e-02,  2.98329275e-02, ...,\n",
       "           -2.82089673e-02,  7.47552589e-02,  8.19281302e-03],\n",
       "          [-1.60399433e-02,  3.05530746e-02,  1.15548015e-01, ...,\n",
       "            9.00395364e-02, -4.69601862e-02, -3.22624967e-02],\n",
       "          ...,\n",
       "          [-3.32717597e-02, -7.16017978e-03,  8.50979090e-02, ...,\n",
       "           -4.73279618e-02, -2.46677343e-02,  1.84073451e-03],\n",
       "          [-9.90804657e-03, -4.64593172e-02,  1.31437145e-02, ...,\n",
       "           -7.50944912e-02, -5.17692603e-02, -3.62547077e-02],\n",
       "          [ 5.89388683e-02, -1.13830380e-01,  7.14894608e-02, ...,\n",
       "           -7.92383403e-02, -5.35607412e-02,  7.97751769e-02]],\n",
       " \n",
       "         [[ 8.43849257e-02, -1.77866071e-02,  8.56387839e-02, ...,\n",
       "           -7.63674155e-02, -6.81038126e-02,  3.43882591e-02],\n",
       "          [-1.02228843e-01,  4.55468744e-02, -7.66468570e-02, ...,\n",
       "           -6.08250946e-02,  2.06902791e-02, -1.09536633e-01],\n",
       "          [ 5.65779954e-02, -2.55945809e-02, -5.47833145e-02, ...,\n",
       "           -4.96188365e-02,  7.45624825e-02, -8.57080985e-03],\n",
       "          ...,\n",
       "          [-6.73724487e-02,  7.76440799e-02,  5.53945974e-02, ...,\n",
       "           -1.13413401e-01,  2.30493899e-02, -1.39340252e-01],\n",
       "          [-3.13021266e-03, -3.80531885e-02, -1.12942576e-01, ...,\n",
       "           -1.56460747e-01, -9.43366960e-02, -1.48152947e-01],\n",
       "          [ 3.23653035e-02, -2.02164367e-01, -7.11877868e-02, ...,\n",
       "           -1.19588092e-01,  5.16168289e-02,  4.75740656e-02]],\n",
       " \n",
       "         [[-8.73209629e-03,  9.31029572e-05,  7.94530660e-02, ...,\n",
       "           -1.16910003e-01,  3.70416008e-02, -2.33734921e-02],\n",
       "          [ 2.58062612e-02,  3.28468941e-02,  5.60038760e-02, ...,\n",
       "           -1.84833892e-02,  2.36084089e-02, -3.44931558e-02],\n",
       "          [ 7.15762004e-02, -1.24047041e-01,  3.45798284e-02, ...,\n",
       "           -8.33064318e-03,  3.48517671e-02,  2.74890698e-02],\n",
       "          ...,\n",
       "          [ 1.29279926e-01,  1.18072247e-02,  5.96041195e-02, ...,\n",
       "            8.93462300e-02, -1.22632802e-01,  8.05469379e-02],\n",
       "          [-5.65066896e-02, -5.14535867e-02, -1.33557141e-01, ...,\n",
       "           -6.85871318e-02, -1.14710063e-01, -1.01192527e-01],\n",
       "          [ 2.26997472e-02, -8.57433230e-02,  1.74759980e-02, ...,\n",
       "           -4.74098437e-02, -4.52813320e-03,  2.41725873e-02]],\n",
       " \n",
       "         [[ 8.62229243e-02, -6.72075301e-02,  2.88324151e-02, ...,\n",
       "           -1.07703328e-01,  1.17808998e-01,  7.55032012e-03],\n",
       "          [-2.36586332e-02,  3.32525037e-02, -3.09949946e-02, ...,\n",
       "           -1.36031527e-02,  1.63366680e-03, -9.99832451e-02],\n",
       "          [ 1.18110785e-02, -1.26234606e-01, -9.56870392e-02, ...,\n",
       "           -9.51529443e-02, -3.39982025e-02,  9.76873841e-03],\n",
       "          ...,\n",
       "          [ 5.44641353e-02, -4.15546820e-02, -2.61044651e-02, ...,\n",
       "            3.64819244e-02,  2.92785764e-02,  9.64388698e-02],\n",
       "          [-6.77611008e-02, -8.27826783e-02, -3.53691019e-02, ...,\n",
       "           -1.37042431e-02, -4.85327654e-02, -3.57294418e-02],\n",
       "          [ 5.24961762e-02, -2.76335166e-03, -3.17887925e-02, ...,\n",
       "           -1.43599093e-01, -1.09091364e-01,  6.83296397e-02]]],\n",
       " \n",
       " \n",
       "        [[[ 1.93624217e-02, -3.71355861e-02,  5.35738133e-02, ...,\n",
       "           -1.15586653e-01, -7.37790484e-03,  1.44580960e-01],\n",
       "          [-1.93819795e-02,  5.99979684e-02, -1.70699164e-01, ...,\n",
       "           -3.33073139e-02,  4.23780419e-02, -9.45687480e-03],\n",
       "          [ 7.12776557e-02,  5.26800752e-02, -1.10726990e-02, ...,\n",
       "           -4.74427938e-02, -9.07665715e-02,  2.77186558e-02],\n",
       "          ...,\n",
       "          [ 6.44306839e-02,  1.46512417e-02, -4.65182401e-03, ...,\n",
       "           -6.47845343e-02, -6.90863878e-02,  7.65199661e-02],\n",
       "          [-2.44508628e-02, -9.21750665e-02, -5.74933141e-02, ...,\n",
       "           -7.01702684e-02, -1.23475589e-01, -2.27237493e-02],\n",
       "          [ 5.07241823e-02, -1.01008438e-01, -2.06854753e-02, ...,\n",
       "            2.01445073e-02, -1.44535173e-02,  7.19637498e-02]],\n",
       " \n",
       "         [[ 3.48244794e-02, -3.98151241e-02,  6.01295941e-02, ...,\n",
       "           -4.73025069e-02,  1.67497341e-02,  1.33160725e-01],\n",
       "          [-6.41218349e-02,  7.96024054e-02, -1.86775699e-02, ...,\n",
       "           -4.51293401e-02, -7.46703371e-02, -2.12151837e-02],\n",
       "          [ 4.53706421e-02,  4.18242402e-02,  1.04005851e-01, ...,\n",
       "            4.11116816e-02,  2.68062744e-02,  3.45356129e-02],\n",
       "          ...,\n",
       "          [-7.61752725e-02, -6.12916648e-02,  7.91104436e-02, ...,\n",
       "           -1.47692040e-02,  1.75015330e-02, -1.13980912e-01],\n",
       "          [-9.50116515e-02, -1.32461950e-01,  6.59278706e-02, ...,\n",
       "           -4.52227034e-02, -2.44572703e-02, -8.26863013e-03],\n",
       "          [ 8.22877586e-02, -7.61339217e-02,  1.16099767e-01, ...,\n",
       "            3.78023740e-03,  6.31560525e-03,  1.03177745e-02]],\n",
       " \n",
       "         [[ 5.60939498e-02, -5.50747216e-02, -5.76902702e-02, ...,\n",
       "           -9.27687511e-02, -8.35177973e-02,  1.48013085e-01],\n",
       "          [-5.42734340e-02,  1.49403229e-01, -5.75127080e-03, ...,\n",
       "           -1.93554640e-01,  2.54186485e-02, -1.75391678e-02],\n",
       "          [ 8.98223147e-02,  3.26697268e-02, -9.55161229e-02, ...,\n",
       "            4.47249077e-02,  9.38690379e-02,  1.66305184e-01],\n",
       "          ...,\n",
       "          [-7.38433003e-02, -6.84585571e-02,  1.22060500e-01, ...,\n",
       "            3.01174540e-02, -1.07231557e-01,  3.18378187e-03],\n",
       "          [-6.51917933e-03, -1.12374157e-01, -9.11705345e-02, ...,\n",
       "           -1.65423160e-04, -8.26942027e-02, -6.78792410e-03],\n",
       "          [ 5.76338768e-02, -4.55946177e-02,  1.13660777e-02, ...,\n",
       "           -6.13112561e-02, -4.23551165e-03,  1.24842182e-01]],\n",
       " \n",
       "         [[ 1.39711604e-01, -6.88154101e-02, -3.72282527e-02, ...,\n",
       "           -7.62329996e-02, -6.74762204e-02,  1.01713724e-01],\n",
       "          [ 3.51170525e-02,  5.21856360e-02,  4.43521813e-02, ...,\n",
       "           -1.23029269e-01, -4.31980118e-02, -2.56331824e-02],\n",
       "          [-2.01114709e-03,  1.02725416e-01, -6.52038082e-02, ...,\n",
       "            1.45530567e-01, -1.78455971e-02,  7.58433789e-02],\n",
       "          ...,\n",
       "          [ 9.46440641e-03, -9.59479064e-02,  1.94025021e-02, ...,\n",
       "            5.30271195e-02,  9.75357555e-03,  3.83197032e-02],\n",
       "          [-6.93338141e-02, -1.34829879e-01, -1.14288211e-01, ...,\n",
       "           -1.16445243e-01, -1.00819342e-01,  4.82682744e-03],\n",
       "          [ 5.85696520e-03, -6.04163408e-02, -6.76705176e-03, ...,\n",
       "           -5.36332801e-02,  5.08681200e-02, -1.67998895e-02]],\n",
       " \n",
       "         [[ 6.78351447e-02, -1.36058386e-02, -1.56054962e-02, ...,\n",
       "           -3.88448089e-02, -3.78431380e-02,  9.96098146e-02],\n",
       "          [-6.21786788e-02, -1.61584653e-02, -3.02540101e-02, ...,\n",
       "           -8.18961486e-02, -7.25685284e-02, -6.16352670e-02],\n",
       "          [ 2.63930615e-02, -2.13397443e-02, -2.67642681e-02, ...,\n",
       "           -1.54131707e-02, -5.29341772e-02,  8.41001794e-02],\n",
       "          ...,\n",
       "          [ 4.08630520e-02, -1.27830252e-01, -2.19834521e-02, ...,\n",
       "           -4.28111367e-02,  9.11058486e-02,  8.94769207e-02],\n",
       "          [-2.21860576e-02, -1.07418537e-01, -8.23201388e-02, ...,\n",
       "           -9.92318764e-02, -2.61501619e-03, -2.59864163e-02],\n",
       "          [ 1.47573218e-01, -4.06172834e-02, -5.27374595e-02, ...,\n",
       "           -1.30077824e-01, -7.54140764e-02,  1.31962761e-01]]]],\n",
       "       dtype=float32)>,\n",
       " <tf.Variable 'hybrid_model/conv2d_1/bias:0' shape=(64,) dtype=float32, numpy=\n",
       " array([-0.08281849, -0.13654475, -0.07387894, -0.02195302, -0.02672579,\n",
       "        -0.02201302, -0.0573445 , -0.0930867 , -0.09048551, -0.10150143,\n",
       "        -0.05222892, -0.0458001 , -0.06697796, -0.07089796, -0.06065741,\n",
       "        -0.02859874, -0.04113201, -0.05927314, -0.0323658 , -0.03738662,\n",
       "        -0.03081335, -0.12408637, -0.076478  , -0.05084065, -0.05820752,\n",
       "        -0.03488139, -0.02811062, -0.0381182 , -0.02085716, -0.12904094,\n",
       "        -0.01212958, -0.05751058, -0.06923634, -0.01526073, -0.05552556,\n",
       "        -0.06460754, -0.084466  , -0.06185791, -0.01286433, -0.11563814,\n",
       "        -0.03351772, -0.0790531 , -0.05581075, -0.09235026,  0.024655  ,\n",
       "        -0.05824992, -0.0560421 , -0.10562644, -0.05601954, -0.01228116,\n",
       "        -0.10381283, -0.07302515, -0.0617247 , -0.02234306, -0.06930977,\n",
       "        -0.02643015, -0.10172793, -0.10378913, -0.01592149, -0.011875  ,\n",
       "        -0.02062183, -0.04705323, -0.07955115, -0.02946644], dtype=float32)>,\n",
       " <tf.Variable 'hybrid_model/dense/kernel:0' shape=(33885, 36) dtype=float32, numpy=\n",
       " array([[-1.1074624e-02, -4.1608881e-02, -9.1312505e-02, ...,\n",
       "         -3.3745188e-02, -7.6793887e-02,  8.7570380e-03],\n",
       "        [-1.2989500e-01, -9.6730344e-02, -6.0182985e-02, ...,\n",
       "         -1.0107818e-02,  8.6363181e-02,  1.9389925e-03],\n",
       "        [-5.2506197e-02,  9.6489586e-02, -3.1130427e-02, ...,\n",
       "         -3.9168607e-02, -9.0295963e-02, -5.8157083e-02],\n",
       "        ...,\n",
       "        [-4.4107202e-01, -8.6392325e-01, -3.7295151e-01, ...,\n",
       "          6.7913008e-01, -4.0124199e-01, -3.9819798e-01],\n",
       "        [-9.8608917e-01, -1.8804027e+00, -2.4771707e+00, ...,\n",
       "          3.2110542e-01, -1.1941727e+00, -1.0374660e-01],\n",
       "        [-8.3557062e-02, -2.0223837e-01,  2.1544847e-01, ...,\n",
       "         -1.1823248e-02, -8.0447339e-02, -1.8204777e-03]], dtype=float32)>,\n",
       " <tf.Variable 'hybrid_model/dense/bias:0' shape=(36,) dtype=float32, numpy=\n",
       " array([-0.5089299 , -0.9977068 ,  0.7948738 ,  0.41429248, -0.0268608 ,\n",
       "        -0.17046283,  0.26843023,  0.16256846, -0.2451239 , -0.34756505,\n",
       "         0.17688183, -0.6604751 , -0.2170855 , -0.02498383,  0.08907021,\n",
       "        -0.7641542 ,  0.10137063,  0.6410424 ,  0.52331245,  0.42841268,\n",
       "         0.54903024,  0.55421853, -0.10378759, -0.21971573,  0.67780435,\n",
       "        -0.02171303,  0.04476112,  0.2912378 , -0.9152597 ,  0.1700478 ,\n",
       "        -0.15860154, -0.1919979 , -0.29296637, -0.04119523, -0.01695112,\n",
       "         0.12138514], dtype=float32)>,\n",
       " <tf.Variable 'hybrid_model/dense_1/kernel:0' shape=(36, 36) dtype=float32, numpy=\n",
       " array([[ 5.18236458e-02, -1.06214300e-01,  4.00338229e-03, ...,\n",
       "          5.02275050e-01,  4.48499583e-02, -1.00985534e-01],\n",
       "        [-1.12515584e-01, -6.52713656e-01, -6.56251162e-02, ...,\n",
       "         -5.34607589e-01,  2.87106697e-04, -3.74251381e-02],\n",
       "        [-4.61386703e-02, -7.19562545e-02,  8.05497169e-02, ...,\n",
       "          1.11862533e-02,  3.82855125e-02, -4.55510974e-01],\n",
       "        ...,\n",
       "        [ 6.76238835e-02,  2.74337083e-03,  1.26496568e-01, ...,\n",
       "          8.65460187e-02, -4.64125946e-02,  3.25118266e-02],\n",
       "        [ 1.17852809e-02,  2.44528323e-01, -1.66634321e-02, ...,\n",
       "          5.20958565e-02,  2.14283802e-02, -2.84539885e-03],\n",
       "        [ 5.27902097e-02,  7.82650262e-02, -4.63359691e-02, ...,\n",
       "          9.39445123e-02, -1.10409725e-02, -2.79807448e-02]], dtype=float32)>,\n",
       " <tf.Variable 'hybrid_model/dense_1/bias:0' shape=(36,) dtype=float32, numpy=\n",
       " array([-0.06147419,  0.33201504, -0.495243  , -0.3569523 ,  0.0469794 ,\n",
       "        -0.31724292,  0.292011  , -0.6669512 , -0.9254382 , -0.36095715,\n",
       "        -0.25289318, -0.19468687,  0.731265  , -0.12846814, -0.5434555 ,\n",
       "        -0.12426654, -0.4504563 , -0.13946787, -0.15937227, -0.05368416,\n",
       "        -0.1046337 , -0.09264367, -0.02971907, -0.12386534, -0.23788212,\n",
       "        -0.15831909,  0.30640626,  0.31944832, -0.0684476 , -0.2824049 ,\n",
       "         0.04625434, -0.27259612, -0.33852726, -0.37345478, -0.40636566,\n",
       "        -0.7677551 ], dtype=float32)>,\n",
       " <tf.Variable 'hybrid_model/dense_2/kernel:0' shape=(36, 1) dtype=float32, numpy=\n",
       " array([[-0.05368588],\n",
       "        [-0.09272792],\n",
       "        [ 0.02234269],\n",
       "        [-0.0004162 ],\n",
       "        [-0.04130239],\n",
       "        [-0.00871691],\n",
       "        [ 0.11383037],\n",
       "        [ 0.01040168],\n",
       "        [ 0.08819037],\n",
       "        [-0.04334035],\n",
       "        [ 0.0064861 ],\n",
       "        [-0.02469726],\n",
       "        [-0.21196891],\n",
       "        [ 0.06226403],\n",
       "        [ 0.0268675 ],\n",
       "        [-0.00793102],\n",
       "        [-0.0296137 ],\n",
       "        [-0.00683115],\n",
       "        [-0.03096012],\n",
       "        [-0.03118641],\n",
       "        [-0.04957122],\n",
       "        [-0.00573763],\n",
       "        [-0.01190941],\n",
       "        [ 0.02190767],\n",
       "        [ 0.00364109],\n",
       "        [ 0.09393103],\n",
       "        [-0.11360855],\n",
       "        [-0.04261293],\n",
       "        [ 0.08127906],\n",
       "        [-0.01781027],\n",
       "        [ 0.08836056],\n",
       "        [ 0.00560123],\n",
       "        [-0.01843539],\n",
       "        [-0.05689165],\n",
       "        [-0.00793195],\n",
       "        [ 0.0703616 ]], dtype=float32)>,\n",
       " <tf.Variable 'hybrid_model/dense_2/bias:0' shape=(1,) dtype=float32, numpy=array([-0.6654447], dtype=float32)>]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Examine the weights objects\n",
    "model.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"hybrid_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             multiple                  2432      \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           multiple                  51264     \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  multiple                  0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " flatten (Flatten)           multiple                  0         \n",
      "                                                                 \n",
      " dense (Dense)               multiple                  1219896   \n",
      "                                                                 \n",
      " dropout (Dropout)           multiple                  0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             multiple                  1332      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         multiple                  0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             multiple                  37        \n",
      "                                                                 \n",
      " concatenate (Concatenate)   multiple                  0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1274961 (4.86 MB)\n",
      "Trainable params: 1274961 (4.86 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try importing hdf5 and json for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimported_weights = model.load_weights(savePath + \"Model.weights.h5\", skip_mismatch=False)\\n\\n#Taken from https://machinelearningmastery.com/save-load-keras-deep-learning-models/\\n# load json and create model\\njson_file = open(savePath + \\'model.json\\', \\'r\\')\\nloaded_model_json = json_file.read()\\njson_file.close()\\n#loaded_model = tf.keras.models.model_from_json(loaded_model_json)\\n# load weights into new model\\n#loaded_model.load_weights(\"model.h5\")\\n#print(\"Loaded model from disk\")\\n'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "imported_weights = model.load_weights(savePath + \"Model.weights.h5\", skip_mismatch=False)\n",
    "\n",
    "#Taken from https://machinelearningmastery.com/save-load-keras-deep-learning-models/\n",
    "# load json and create model\n",
    "json_file = open(savePath + 'model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "#loaded_model = tf.keras.models.model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "#loaded_model.load_weights(\"model.h5\")\n",
    "#print(\"Loaded model from disk\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5 - Plot the losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 2.0)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plot the training and validation losses\n",
    "\n",
    "#Convert loss results into a dataframe\n",
    "result_preproc = pd.DataFrame({\n",
    "    'Epoch': [i+1 for i in range(len(results[\"loss\"]))], \n",
    "    'Train': results[\"loss\"],\n",
    "    'Validate': results[\"val_loss\"]\n",
    "    })\n",
    "\n",
    "# Convert dataframe from wide to long format\n",
    "df = pd.melt(result_preproc, ['Epoch'])\n",
    "\n",
    "#Make plot\n",
    "g = sns.lineplot(data=df, x='Epoch', y='value', hue='variable')\n",
    "g.set_title(\"Loss Curves\")\n",
    "g.legend_.set_title(\"Loss\")\n",
    "g.set_ylabel('Loss')\n",
    "g.set_ylim(0,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9) Predict Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simple simple function to ititialize the test dataset using global variables\n",
    "def init_test_dataset():\n",
    "    return make_dataset(hdf5_file, test_meta_dict, test_pos_isic_id, test_pos_target, batch_size = test_batch_size, apply_hair_removal=apply_hair_removal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total test batches in dataset: 75\n"
     ]
    }
   ],
   "source": [
    "#Test dataset basic size information: nb of samples and batch size\n",
    "nb_test_batches = int(np.ceil(len(test_meta_dict)/test_batch_size))\n",
    "print(\"Total test batches in dataset:\", nb_test_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-22 20:37:24.809830: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "#Retrieve real values for the target in the test dataset\n",
    "y_test = []\n",
    "test_dataset = init_test_dataset()\n",
    "for item in test_dataset.take(nb_test_batches):\n",
    "    img_meta, targ = item\n",
    "    y_test.extend(targ.numpy().flatten())\n",
    "#Convert to numpy array (mathematical operations are faster)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75/75 [==============================] - 7s 83ms/step\n",
      "Shape of prediction data: (2371, 1)\n"
     ]
    }
   ],
   "source": [
    "#Reinitialize the test dataset (necessary to start at beginning)\n",
    "test_dataset = init_test_dataset()\n",
    "#Retrieve predictions\n",
    "predictions = model.predict(test_dataset, steps = nb_test_batches)\n",
    "#Put predictionsin a numpy array\n",
    "y_pred = np.array([round(i) for i  in predictions.flatten()])\n",
    "print(\"Shape of prediction data:\", predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#Import results from file\\nimported_test_results = pd.read_csv(testResPath)\\ny_test = np.array(imported_test_results[\"y_test\"])\\ny_pred = np.array(imported_test_results[\"y_pred\"])\\ndel imported_test_results\\n'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Save test results to file\n",
    "pd.DataFrame({\"y_test\": y_test, \"y_pred\": y_pred}).to_csv(testResPath, index=False)\n",
    "\n",
    "\"\"\"\n",
    "#Import results from file\n",
    "imported_test_results = pd.read_csv(testResPath)\n",
    "y_test = np.array(imported_test_results[\"y_test\"])\n",
    "y_pred = np.array(imported_test_results[\"y_pred\"])\n",
    "del imported_test_results\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the loss\n",
    "loss = sum(abs(y_test - y_pred))/len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Determine true/false positives and negatives\n",
    "pos_indices = y_test == 1\n",
    "neg_indices = y_test == 0\n",
    "\n",
    "#True positives\n",
    "true_pos = sum(abs(y_test[pos_indices] == y_pred[pos_indices]))\n",
    "\n",
    "#False negatives\n",
    "false_neg = sum(abs(y_test[pos_indices] != y_pred[pos_indices]))\n",
    "\n",
    "#True negatives\n",
    "true_neg = sum(abs(y_test[neg_indices] == y_pred[neg_indices]))\n",
    "\n",
    "#False positives\n",
    "false_pos = sum(abs(y_test[neg_indices] != y_pred[neg_indices]))\n",
    "\n",
    "#Precision\n",
    "try:\n",
    "    precision = true_pos / (true_pos + false_pos)\n",
    "except:\n",
    "    precision = np.nan\n",
    "\n",
    "#Recall (sensitivity)\n",
    "try:\n",
    "    recall = true_pos / (true_pos + false_neg)\n",
    "except:\n",
    "    recall = np.nan\n",
    "\n",
    "#Specificity\n",
    "try:\n",
    "    specificity = true_neg / (true_neg + false_pos)\n",
    "except:\n",
    "    specificity = np.nan\n",
    "\n",
    "#F1 Score\n",
    "try:\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "except:\n",
    "    f1_score = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---TEST RESULTS---\n",
      "True positives: 1\n",
      "False positives: 459\n",
      "True negatives: 1911\n",
      "False negatives: 0\n",
      "\n",
      "Sensitivity: 1.0\n",
      "Specificity: 0.8063291139240506\n",
      "\n",
      "Precision: 0.002173913043478261\n",
      "Recall: 1.0\n",
      "\n",
      "F1 Score: 0.0043383947939262466\n",
      "Loss on test data: 0.1935892028679882\n"
     ]
    }
   ],
   "source": [
    "print(\"---TEST RESULTS---\")\n",
    "print(\"True positives:\", true_pos)\n",
    "print(\"False positives:\", false_pos)\n",
    "print(\"True negatives:\", true_neg)\n",
    "print(\"False negatives:\", false_neg)\n",
    "print()\n",
    "print(\"Sensitivity:\", recall)\n",
    "print(\"Specificity:\", specificity)\n",
    "print()\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print()\n",
    "print(\"F1 Score:\", f1_score)\n",
    "print(\"Loss on test data:\", loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p310",
   "language": "python",
   "name": "conda_tensorflow2_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
